{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary packages\n",
    "import os\n",
    "import h5py\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting all ligands and proteins from Grechishnikova BindingDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/home/iit/CDGCN/data/bindingdb/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysing the proteins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train datasets details:')\n",
    "\n",
    "d1_tr_prots = []\n",
    "d2_tr_prots = []\n",
    "d3_tr_prots = []\n",
    "d4_tr_prots = []\n",
    "d5_tr_prots = []\n",
    "\n",
    "with open(data_path+'train_dataset/train_4_org_1042_104/proteins_train_4_org_corrected') as f:\n",
    "    for line in f:\n",
    "        d1_tr_prots.append(line)\n",
    "print('Dataset 1 proteins count =', len(set(d1_tr_prots)))\n",
    "\n",
    "with open(data_path+'train_dataset/train_4_org_1000_112/proteins_train_4_org_corrected') as f:\n",
    "    for line in f:\n",
    "        d2_tr_prots.append(line)\n",
    "print('Dataset 2 proteins count =', len(set(d2_tr_prots)))\n",
    "\n",
    "with open(data_path+'train_dataset/train_4_org_1004_122/proteins_train_4_org_corrected') as f:\n",
    "    for line in f:\n",
    "        d3_tr_prots.append(line)\n",
    "print('Dataset 3 proteins count =', len(set(d3_tr_prots)))\n",
    "\n",
    "with open(data_path+'train_dataset/train_4_org_1002_103/proteins_train_4_org_corrected') as f:\n",
    "    for line in f:\n",
    "        d4_tr_prots.append(line)\n",
    "print('Dataset 4 proteins count =', len(set(d4_tr_prots)))\n",
    "\n",
    "with open(data_path+'train_dataset/train_4_org_1036_124/proteins_train_4_org_corrected') as f:\n",
    "    for line in f:\n",
    "        d5_tr_prots.append(line)\n",
    "print('Dataset 5 proteins count =', len(set(d5_tr_prots)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset 1 actually has 1514 proteins in train dataset, whereas in Grechishnikova paper it is given as 1042. All other datasets are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Test datasets details:')\n",
    "\n",
    "d1_te_prots = []\n",
    "d2_te_prots = []\n",
    "d3_te_prots = []\n",
    "d4_te_prots = []\n",
    "d5_te_prots = []\n",
    "\n",
    "with open(data_path+'test_dataset/test_4_org_1042_104/proteins_test_4_org_corrected') as f:\n",
    "    for line in f:\n",
    "        d1_te_prots.append(line)\n",
    "print('Dataset 1 proteins count =', len(set(d1_te_prots)))\n",
    "\n",
    "with open(data_path+'test_dataset/test_4_org_1000_112/proteins_test_4_org_corrected') as f:\n",
    "    for line in f:\n",
    "        d2_te_prots.append(line)\n",
    "print('Dataset 2 proteins count =', len(set(d2_te_prots)))\n",
    "\n",
    "with open(data_path+'test_dataset/test_4_org_1004_122/proteins_test_4_org_corrected') as f:\n",
    "    for line in f:\n",
    "        d3_te_prots.append(line)\n",
    "print('Dataset 3 proteins count =', len(set(d3_te_prots)))\n",
    "\n",
    "with open(data_path+'test_dataset/test_4_org_1002_103/proteins_test_4_org_corrected') as f:\n",
    "    for line in f:\n",
    "        d4_te_prots.append(line)\n",
    "print('Dataset 4 proteins count =', len(set(d4_te_prots)))\n",
    "\n",
    "with open(data_path+'test_dataset/test_4_org_1036_124/proteins_test_4_org_corrected') as f:\n",
    "    for line in f:\n",
    "        d5_te_prots.append(line)\n",
    "print('Dataset 5 proteins count =', len(set(d5_te_prots)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset 1 actually has 99 proteins in test dataset, whereas in Grechishnikova paper it is given as 104. All other datasets are correct.\n",
    "\n",
    "We need to fix Dataset 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(set(d1_tr_prots + d1_te_prots)))\n",
    "print(len(set(d2_tr_prots + d2_te_prots)))\n",
    "print(len(set(d3_tr_prots + d3_te_prots)))\n",
    "print(len(set(d4_tr_prots + d4_te_prots)))\n",
    "print(len(set(d5_tr_prots + d5_te_prots)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(set(d1_te_prots + d2_te_prots)))\n",
    "print(len(set(d1_te_prots + d3_te_prots)))\n",
    "print(len(set(d1_te_prots + d4_te_prots)))\n",
    "print(len(set(d1_te_prots + d5_te_prots)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2345_tr_prots = set(d2_tr_prots + d3_tr_prots + d4_tr_prots + d5_tr_prots)\n",
    "d1_tr_uprots_only = list(set(d1_tr_prots) - d2345_tr_prots)\n",
    "len(d1_tr_uprots_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1_tr_uprots_other = list(set(d1_tr_prots)-set(d1_tr_uprots_only))\n",
    "len(d1_tr_uprots_other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(17)\n",
    "\n",
    "d1_tr_uprots = d1_tr_uprots_only + random.sample(d1_tr_uprots_other, k=586)\n",
    "len(d1_tr_uprots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(set(d1_tr_uprots+d1_te_prots))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2345_te_prots = set(d2_te_prots + d3_te_prots + d4_te_prots + d5_te_prots + d2_tr_prots + d3_tr_prots + d4_tr_prots + d5_tr_prots) - set(d1_tr_uprots)\n",
    "d1_te_uprots_only = list(set(d1_te_prots) - d2345_te_prots)\n",
    "len(d1_te_uprots_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2345_te_prots_only = list(set(d2345_te_prots) - set(d1_te_uprots_only))\n",
    "len(d2345_te_prots_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1_te_uprots = d1_te_uprots_only + random.sample(d2345_te_prots_only, k=104-len(d1_te_uprots_only))\n",
    "len(d1_te_uprots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(d1_tr_uprots+d1_te_uprots))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fixed Dataset 1. Now we'll save the unique proteins for all 5 datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving train datasets\n",
    "\n",
    "with open(data_path+'train_dataset/train_4_org_1042_104/d1_tr_unique_proteins.txt', 'w+') as f:\n",
    "    for prot in d1_tr_uprots:\n",
    "        f.write(prot)\n",
    "print('Dataset 1 train saved.')\n",
    "\n",
    "with open(data_path+'train_dataset/train_4_org_1000_112/d2_tr_unique_proteins.txt', 'w+') as f:\n",
    "    for prot in set(d2_tr_prots):\n",
    "        f.write(prot)\n",
    "print('Dataset 2 train saved.')\n",
    "\n",
    "with open(data_path+'train_dataset/train_4_org_1004_122/d3_tr_unique_proteins.txt', 'w+') as f:\n",
    "    for prot in set(d3_tr_prots):\n",
    "        f.write(prot)\n",
    "print('Dataset 3 train saved.')\n",
    "\n",
    "with open(data_path+'train_dataset/train_4_org_1002_103/d4_tr_unique_proteins.txt', 'w+') as f:\n",
    "    for prot in set(d4_tr_prots):\n",
    "        f.write(prot)\n",
    "print('Dataset 4 train saved.')\n",
    "\n",
    "with open(data_path+'train_dataset/train_4_org_1036_124/d5_tr_unique_proteins.txt', 'w+') as f:\n",
    "    for prot in set(d5_tr_prots):\n",
    "        f.write(prot)\n",
    "print('Dataset 5 train saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving test datasets\n",
    "\n",
    "with open(data_path+'test_dataset/test_4_org_1042_104/d1_te_unique_proteins.txt', 'w+') as f:\n",
    "    for prot in d1_te_uprots:\n",
    "        f.write(prot)\n",
    "print('Dataset 1 test saved.')\n",
    "\n",
    "with open(data_path+'test_dataset/test_4_org_1000_112/d2_te_unique_proteins.txt', 'w+') as f:\n",
    "    for prot in set(d2_te_prots):\n",
    "        f.write(prot)\n",
    "print('Dataset 2 test saved.')\n",
    "\n",
    "with open(data_path+'test_dataset/test_4_org_1004_122/d3_te_unique_proteins.txt', 'w+') as f:\n",
    "    for prot in set(d3_te_prots):\n",
    "        f.write(prot)\n",
    "print('Dataset 3 test saved.')\n",
    "\n",
    "with open(data_path+'test_dataset/test_4_org_1002_103/d4_te_unique_proteins.txt', 'w+') as f:\n",
    "    for prot in set(d4_te_prots):\n",
    "        f.write(prot)\n",
    "print('Dataset 4 test saved.')\n",
    "\n",
    "with open(data_path+'test_dataset/test_4_org_1036_124/d5_te_unique_proteins.txt', 'w+') as f:\n",
    "    for prot in set(d5_te_prots):\n",
    "        f.write(prot)\n",
    "print('Dataset 5 test saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if everything is in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train datasets details:')\n",
    "\n",
    "d1_tr_uprots = []\n",
    "d2_tr_uprots = []\n",
    "d3_tr_uprots = []\n",
    "d4_tr_uprots = []\n",
    "d5_tr_uprots = []\n",
    "\n",
    "with open(data_path+'train_dataset/train_4_org_1042_104/d1_tr_unique_proteins.txt') as f:\n",
    "    for line in f:\n",
    "        d1_tr_uprots.append(line)\n",
    "print('Dataset 1 proteins count =', len(set(d1_tr_uprots)))\n",
    "\n",
    "with open(data_path+'train_dataset/train_4_org_1000_112/d2_tr_unique_proteins.txt') as f:\n",
    "    for line in f:\n",
    "        d2_tr_uprots.append(line)\n",
    "print('Dataset 2 proteins count =', len(set(d2_tr_uprots)))\n",
    "\n",
    "with open(data_path+'train_dataset/train_4_org_1004_122/d3_tr_unique_proteins.txt') as f:\n",
    "    for line in f:\n",
    "        d3_tr_uprots.append(line)\n",
    "print('Dataset 3 proteins count =', len(set(d3_tr_uprots)))\n",
    "\n",
    "with open(data_path+'train_dataset/train_4_org_1002_103/d4_tr_unique_proteins.txt') as f:\n",
    "    for line in f:\n",
    "        d4_tr_uprots.append(line)\n",
    "print('Dataset 4 proteins count =', len(set(d4_tr_uprots)))\n",
    "\n",
    "with open(data_path+'train_dataset/train_4_org_1036_124/d5_tr_unique_proteins.txt') as f:\n",
    "    for line in f:\n",
    "        d5_tr_uprots.append(line)\n",
    "print('Dataset 5 proteins count =', len(set(d5_tr_uprots)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test datasets details:')\n",
    "\n",
    "d1_te_uprots = []\n",
    "d2_te_uprots = []\n",
    "d3_te_uprots = []\n",
    "d4_te_uprots = []\n",
    "d5_te_uprots = []\n",
    "\n",
    "with open(data_path+'test_dataset/test_4_org_1042_104/d1_te_unique_proteins.txt') as f:\n",
    "    for line in f:\n",
    "        d1_te_uprots.append(line)\n",
    "print('Dataset 1 proteins count =', len(set(d1_te_uprots)))\n",
    "\n",
    "with open(data_path+'test_dataset/test_4_org_1000_112/d2_te_unique_proteins.txt') as f:\n",
    "    for line in f:\n",
    "        d2_te_uprots.append(line)\n",
    "print('Dataset 2 proteins count =', len(set(d2_te_uprots)))\n",
    "\n",
    "with open(data_path+'test_dataset/test_4_org_1004_122/d3_te_unique_proteins.txt') as f:\n",
    "    for line in f:\n",
    "        d3_te_uprots.append(line)\n",
    "print('Dataset 3 proteins count =', len(set(d3_te_uprots)))\n",
    "\n",
    "with open(data_path+'test_dataset/test_4_org_1002_103/d4_te_unique_proteins.txt') as f:\n",
    "    for line in f:\n",
    "        d4_te_uprots.append(line)\n",
    "print('Dataset 4 proteins count =', len(set(d4_te_uprots)))\n",
    "\n",
    "with open(data_path+'test_dataset/test_4_org_1036_124/d5_te_unique_proteins.txt') as f:\n",
    "    for line in f:\n",
    "        d5_te_uprots.append(line)\n",
    "print('Dataset 5 proteins count =', len(set(d5_te_uprots)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(set(d1_tr_uprots + d1_te_uprots)))\n",
    "print(len(set(d2_tr_uprots + d2_te_uprots)))\n",
    "print(len(set(d3_tr_uprots + d3_te_uprots)))\n",
    "print(len(set(d4_tr_uprots + d4_te_uprots)))\n",
    "print(len(set(d5_tr_uprots + d5_te_uprots)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking Needleman-Wunsch pairwise sequence similarity of proteins\n",
    "\n",
    "We will first store each protein as individual file to give as input to the emboss needle software."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset 1 proteins (train+test)\n",
    "for i, protein in enumerate(d1_tr_uprots):\n",
    "    with open(data_path+f'train_dataset/train_4_org_1042_104/protein{i+1}.fasta', 'w+') as fw:\n",
    "        fw.write(line.replace(' ', ''))\n",
    "        \n",
    "for i, protein in enumerate(d1_te_uprots):\n",
    "    with open(data_path+f'test_dataset/test_4_org_1042_104/protein{i+1}.fasta', 'w+') as fw:\n",
    "        fw.write(line.replace(' ', ''))\n",
    "        \n",
    "\n",
    "# Dataset 2 proteins (train+test)\n",
    "for i, protein in enumerate(d2_tr_uprots):\n",
    "    with open(data_path+f'train_dataset/train_4_org_1000_112/protein{i+1}.fasta', 'w+') as fw:\n",
    "        fw.write(line.replace(' ', ''))\n",
    "        \n",
    "for i, protein in enumerate(d2_te_uprots):\n",
    "    with open(data_path+f'test_dataset/test_4_org_1000_112/protein{i+1}.fasta', 'w+') as fw:\n",
    "        fw.write(line.replace(' ', ''))\n",
    "        \n",
    "\n",
    "# Dataset 3 proteins (train+test)\n",
    "for i, protein in enumerate(d3_tr_uprots):\n",
    "    with open(data_path+f'train_dataset/train_4_org_1004_122/protein{i+1}.fasta', 'w+') as fw:\n",
    "        fw.write(line.replace(' ', ''))\n",
    "        \n",
    "for i, protein in enumerate(d3_te_uprots):\n",
    "    with open(data_path+f'test_dataset/test_4_org_1004_122/protein{i+1}.fasta', 'w+') as fw:\n",
    "        fw.write(line.replace(' ', ''))\n",
    "        \n",
    "\n",
    "# Dataset 4 proteins (train+test)\n",
    "for i, protein in enumerate(d4_tr_uprots):\n",
    "    with open(data_path+f'train_dataset/train_4_org_1002_103/protein{i+1}.fasta', 'w+') as fw:\n",
    "        fw.write(line.replace(' ', ''))\n",
    "        \n",
    "for i, protein in enumerate(d4_te_uprots):\n",
    "    with open(data_path+f'test_dataset/test_4_org_1002_103/protein{i+1}.fasta', 'w+') as fw:\n",
    "        fw.write(line.replace(' ', ''))\n",
    "        \n",
    "\n",
    "# Dataset 5 proteins (train+test)\n",
    "for i, protein in enumerate(d5_tr_uprots):\n",
    "    with open(data_path+f'train_dataset/train_4_org_1036_124/protein{i+1}.fasta', 'w+') as fw:\n",
    "        fw.write(line.replace(' ', ''))\n",
    "        \n",
    "for i, protein in enumerate(d5_te_uprots):\n",
    "    with open(data_path+f'test_dataset/test_4_org_1036_124/protein{i+1}.fasta', 'w+') as fw:\n",
    "        fw.write(line.replace(' ', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_name = ['1042_104', '1000_112', '1004_122', '1002_103', '1036_124']\n",
    "ds_len = [(1042, 104), (1000, 112), (1004, 122), (1002, 103), (1036, 124)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we compute pairwise similarities between train and test proteins for each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(f'mkdir {data_path}needle_outputs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(f'mkdir {data_path}needle_outputs/train-test')\n",
    "\n",
    "for i in range(5):\n",
    "    for p1 in range(1, ds_len[i][0]+1):\n",
    "        for p2 in range(1, ds_len[i][1]+1):\n",
    "            os.system(f'needle {data_path}train_dataset/train_4_org_{ds_name[i]}/protein{p1}.fasta {data_path}test_dataset/test_4_org_{ds_name[i]}/protein{p2}.fasta stdout -gapopen 10.0 -gapextend 0.5 >> {data_path}needle_outputs/train-test/ds{i+1}_proteins_{p1}_{p2}.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we compute pairwise similarities within train proteins for each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(f'mkdir {data_path}needle_outputs/within_train')\n",
    "\n",
    "for i in range(5):\n",
    "    for p1 in range(1, ds_len[i][0]+1):\n",
    "        for p2 in range(1, ds_len[i][0]+1):\n",
    "            os.system(f'needle {data_path}train_dataset/train_4_org_{ds_name[i]}/protein{p1}.fasta {data_path}train_dataset/train_4_org_{ds_name[i]}/protein{p2}.fasta stdout -gapopen 10.0 -gapextend 0.5 >> {data_path}needle_outputs/within_train/ds{i+1}_proteins_{p1}_{p2}.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we compute pairwise similarities within test proteins for each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(f'mkdir {data_path}needle_outputs/within_test')\n",
    "\n",
    "for i in range(5):\n",
    "    for p1 in range(1, ds_len[i][1]+1):\n",
    "        for p2 in range(1, ds_len[i][1]+1):\n",
    "            os.system(f'needle {data_path}test_dataset/test_4_org_{ds_name[i]}/protein{p1}.fasta {data_path}test_dataset/test_4_org_{ds_name[i]}/protein{p2}.fasta stdout -gapopen 10.0 -gapextend 0.5 >> {data_path}needle_outputs/within_test/ds{i+1}_proteins_{p1}_{p2}.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll plot the similarity scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    needle_scores = []\n",
    "    for p1 in range(1, len(ds_len[i][0])+1):\n",
    "        for p2 in range(1, len(ds_len[i][1])+1):\n",
    "            with open(f'{data_path}needle_outputs/train-test/ds{i+1}_proteins_{p1}_{p2}.txt') as f:\n",
    "                line = f.readlines()\n",
    "                needle_scores.append(float(line[24].strip()[26:-2]))\n",
    "    sns.histplot(needle_scores, element=\"step\", stat=\"density\", common_norm=False)\n",
    "    plt.xlabel(\"Needleman-Wunsch similarity score\")\n",
    "    plt.ylabel(\"\")\n",
    "    plt.xlim([0,100])\n",
    "    plt.savefig(f'{data_path}needle_outputs/train-test/images/ds{i+1}.png', dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We have the correct datasets and we also finalised all the proteins. Now we will start filtering based on ligand properties we want.\n",
    "\n",
    "### Analysis of ligands (Salt removal)\n",
    "\n",
    "First we will remove all salts (disconnected components) from the ligands (salts = substrings connected with '.')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uprots = set(d1_tr_prots+d2_tr_prots+d3_tr_prots+d4_tr_prots+d5_tr_prots+d1_te_prots+d2_te_prots+d3_te_prots+d4_te_prots+d5_te_prots)\n",
    "len(uprots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train datasets details:')\n",
    "\n",
    "d1_tr_ligs = []\n",
    "d2_tr_ligs = []\n",
    "d3_tr_ligs = []\n",
    "d4_tr_ligs = []\n",
    "d5_tr_ligs = []\n",
    "\n",
    "with open(data_path+'train_dataset/train_4_org_1042_104/ligands_train_4_org_corrected') as f:\n",
    "    for line in f:\n",
    "        d1_tr_ligs.append(line.strip().replace(' ', ''))\n",
    "print('Dataset 1 ligands count =', len(d1_tr_ligs))\n",
    "\n",
    "with open(data_path+'train_dataset/train_4_org_1000_112/ligands_train_4_org_corrected') as f:\n",
    "    for line in f:\n",
    "        d2_tr_ligs.append(line.strip().replace(' ', ''))\n",
    "print('Dataset 2 ligands count =', len(d2_tr_ligs))\n",
    "\n",
    "with open(data_path+'train_dataset/train_4_org_1004_122/ligands_train_4_org_corrected') as f:\n",
    "    for line in f:\n",
    "        d3_tr_ligs.append(line.strip().replace(' ', ''))\n",
    "print('Dataset 3 ligands count =', len(d3_tr_ligs))\n",
    "\n",
    "with open(data_path+'train_dataset/train_4_org_1002_103/ligands_train_4_org_corrected') as f:\n",
    "    for line in f:\n",
    "        d4_tr_ligs.append(line.strip().replace(' ', ''))\n",
    "print('Dataset 4 ligands count =', len(d4_tr_ligs))\n",
    "\n",
    "with open(data_path+'train_dataset/train_4_org_1036_124/ligands_train_4_org_corrected') as f:\n",
    "    for line in f:\n",
    "        d5_tr_ligs.append(line.strip().replace(' ', ''))\n",
    "print('Dataset 5 ligands count =', len(d5_tr_ligs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Test datasets details:')\n",
    "\n",
    "d1_te_ligs = []\n",
    "d2_te_ligs = []\n",
    "d3_te_ligs = []\n",
    "d4_te_ligs = []\n",
    "d5_te_ligs = []\n",
    "\n",
    "with open(data_path+'test_dataset/test_4_org_1042_104/ligands_test_4_org_corrected') as f:\n",
    "    for line in f:\n",
    "        d1_te_ligs.append(line.strip().replace(' ', ''))\n",
    "print('Dataset 1 ligands count =', len(d1_te_ligs))\n",
    "\n",
    "with open(data_path+'test_dataset/test_4_org_1000_112/ligands_test_4_org_corrected') as f:\n",
    "    for line in f:\n",
    "        d2_te_ligs.append(line.strip().replace(' ', ''))\n",
    "print('Dataset 2 ligands count =', len(d2_te_ligs))\n",
    "\n",
    "with open(data_path+'test_dataset/test_4_org_1004_122/ligands_test_4_org_corrected') as f:\n",
    "    for line in f:\n",
    "        d3_te_ligs.append(line.strip().replace(' ', ''))\n",
    "print('Dataset 3 ligands count =', len(d3_te_ligs))\n",
    "\n",
    "with open(data_path+'test_dataset/test_4_org_1002_103/ligands_test_4_org_corrected') as f:\n",
    "    for line in f:\n",
    "        d4_te_ligs.append(line.strip().replace(' ', ''))\n",
    "print('Dataset 4 ligands count =', len(d4_te_ligs))\n",
    "\n",
    "with open(data_path+'test_dataset/test_4_org_1036_124/ligands_test_4_org_corrected') as f:\n",
    "    for line in f:\n",
    "        d5_te_ligs.append(line.strip().replace(' ', ''))\n",
    "print('Dataset 5 ligands count =', len(d5_te_ligs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving train datasets\n",
    "\n",
    "with open(data_path+'train_dataset/train_4_org_1042_104/d1_tr_ligands.txt', 'w+') as f:\n",
    "    for lig in d1_tr_ligs:\n",
    "        f.write([b for b in lig.split('.') if len(b) == max([len(bs) for bs in lig.split('.')])][0]+'\\n')\n",
    "print('Dataset 1 train saved.')\n",
    "\n",
    "with open(data_path+'train_dataset/train_4_org_1000_112/d2_tr_ligands.txt', 'w+') as f:\n",
    "    for lig in d2_tr_ligs:\n",
    "        f.write([b for b in lig.split('.') if len(b) == max([len(bs) for bs in lig.split('.')])][0]+'\\n')\n",
    "print('Dataset 2 train saved.')\n",
    "\n",
    "with open(data_path+'train_dataset/train_4_org_1004_122/d3_tr_ligands.txt', 'w+') as f:\n",
    "    for lig in d3_tr_ligs:\n",
    "        f.write([b for b in lig.split('.') if len(b) == max([len(bs) for bs in lig.split('.')])][0]+'\\n')\n",
    "print('Dataset 3 train saved.')\n",
    "\n",
    "with open(data_path+'train_dataset/train_4_org_1002_103/d4_tr_ligands.txt', 'w+') as f:\n",
    "    for lig in d4_tr_ligs:\n",
    "        f.write([b for b in lig.split('.') if len(b) == max([len(bs) for bs in lig.split('.')])][0]+'\\n')\n",
    "print('Dataset 4 train saved.')\n",
    "\n",
    "with open(data_path+'train_dataset/train_4_org_1036_124/d5_tr_ligands.txt', 'w+') as f:\n",
    "    for lig in d5_tr_ligs:\n",
    "        f.write([b for b in lig.split('.') if len(b) == max([len(bs) for bs in lig.split('.')])][0]+'\\n')\n",
    "print('Dataset 5 train saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving test datasets\n",
    "\n",
    "with open(data_path+'test_dataset/test_4_org_1042_104/d1_te_ligands.txt', 'w+') as f:\n",
    "    for lig in d1_te_ligs:\n",
    "        f.write([b for b in lig.split('.') if len(b) == max([len(bs) for bs in lig.split('.')])][0]+'\\n')\n",
    "print('Dataset 1 test saved.')\n",
    "\n",
    "with open(data_path+'test_dataset/test_4_org_1000_112/d2_te_ligands.txt', 'w+') as f:\n",
    "    for lig in d2_te_ligs:\n",
    "        f.write([b for b in lig.split('.') if len(b) == max([len(bs) for bs in lig.split('.')])][0]+'\\n')\n",
    "print('Dataset 2 test saved.')\n",
    "\n",
    "with open(data_path+'test_dataset/test_4_org_1004_122/d3_te_ligands.txt', 'w+') as f:\n",
    "    for lig in d3_te_ligs:\n",
    "        f.write([b for b in lig.split('.') if len(b) == max([len(bs) for bs in lig.split('.')])][0]+'\\n')\n",
    "print('Dataset 3 test saved.')\n",
    "\n",
    "with open(data_path+'test_dataset/test_4_org_1002_103/d4_te_ligands.txt', 'w+') as f:\n",
    "    for lig in d4_te_ligs:\n",
    "        f.write([b for b in lig.split('.') if len(b) == max([len(bs) for bs in lig.split('.')])][0]+'\\n')\n",
    "print('Dataset 4 test saved.')\n",
    "\n",
    "with open(data_path+'test_dataset/test_4_org_1036_124/d5_te_ligands.txt', 'w+') as f:\n",
    "    for lig in d5_te_ligs:\n",
    "        f.write([b for b in lig.split('.') if len(b) == max([len(bs) for bs in lig.split('.')])][0]+'\\n')\n",
    "print('Dataset 5 test saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of ligands (Removing molecules with > 100 atoms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train datasets details:')\n",
    "\n",
    "d1_tr_ligs = []\n",
    "d2_tr_ligs = []\n",
    "d3_tr_ligs = []\n",
    "d4_tr_ligs = []\n",
    "d5_tr_ligs = []\n",
    "\n",
    "with open(data_path+'train_dataset/train_4_org_1042_104/d1_tr_ligands.txt') as f:\n",
    "    for line in f:\n",
    "        d1_tr_ligs.append(line.strip())\n",
    "print('Dataset 1 ligands count =', len(d1_tr_ligs))\n",
    "\n",
    "with open(data_path+'train_dataset/train_4_org_1000_112/d2_tr_ligands.txt') as f:\n",
    "    for line in f:\n",
    "        d2_tr_ligs.append(line.strip())\n",
    "print('Dataset 2 ligands count =', len(d2_tr_ligs))\n",
    "\n",
    "with open(data_path+'train_dataset/train_4_org_1004_122/d3_tr_ligands.txt') as f:\n",
    "    for line in f:\n",
    "        d3_tr_ligs.append(line.strip())\n",
    "print('Dataset 3 ligands count =', len(d3_tr_ligs))\n",
    "\n",
    "with open(data_path+'train_dataset/train_4_org_1002_103/d4_tr_ligands.txt') as f:\n",
    "    for line in f:\n",
    "        d4_tr_ligs.append(line.strip())\n",
    "print('Dataset 4 ligands count =', len(d4_tr_ligs))\n",
    "\n",
    "with open(data_path+'train_dataset/train_4_org_1036_124/d5_tr_ligands.txt') as f:\n",
    "    for line in f:\n",
    "        d5_tr_ligs.append(line.strip())\n",
    "print('Dataset 5 ligands count =', len(d5_tr_ligs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Test datasets details:')\n",
    "\n",
    "d1_te_ligs = []\n",
    "d2_te_ligs = []\n",
    "d3_te_ligs = []\n",
    "d4_te_ligs = []\n",
    "d5_te_ligs = []\n",
    "\n",
    "with open(data_path+'test_dataset/test_4_org_1042_104/d1_te_ligands.txt') as f:\n",
    "    for line in f:\n",
    "        d1_te_ligs.append(line.strip())\n",
    "print('Dataset 1 ligands count =', len(d1_te_ligs))\n",
    "\n",
    "with open(data_path+'test_dataset/test_4_org_1000_112/d2_te_ligands.txt') as f:\n",
    "    for line in f:\n",
    "        d2_te_ligs.append(line.strip())\n",
    "print('Dataset 2 ligands count =', len(d2_te_ligs))\n",
    "\n",
    "with open(data_path+'test_dataset/test_4_org_1004_122/d3_te_ligands.txt') as f:\n",
    "    for line in f:\n",
    "        d3_te_ligs.append(line.strip())\n",
    "print('Dataset 3 ligands count =', len(d3_te_ligs))\n",
    "\n",
    "with open(data_path+'test_dataset/test_4_org_1002_103/d4_te_ligands.txt') as f:\n",
    "    for line in f:\n",
    "        d4_te_ligs.append(line.strip())\n",
    "print('Dataset 4 ligands count =', len(d4_te_ligs))\n",
    "\n",
    "with open(data_path+'test_dataset/test_4_org_1036_124/d5_te_ligands.txt') as f:\n",
    "    for line in f:\n",
    "        d5_te_ligs.append(line.strip())\n",
    "print('Dataset 5 ligands count =', len(d5_te_ligs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing RDKit for extracting and filtering based on molecule information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('/usr/local/lib/python3.7/site-packages/')\n",
    "\n",
    "try:\n",
    "    import rdkit\n",
    "    from rdkit import Chem\n",
    "    from rdkit.Chem import Draw\n",
    "    from rdkit.Chem import rdmolfiles\n",
    "    from rdkit.Chem.Draw import IPythonConsole\n",
    "    IPythonConsole.ipython_useSVG=True\n",
    "except ImportError:\n",
    "    print('Stopping RUNTIME. Colaboratory will restart automatically. Please run again.')\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_atom_count(smiles):\n",
    "    count = 0\n",
    "    filtered = []\n",
    "    for s in smiles:\n",
    "        mol = Chem.MolFromSmiles(s)\n",
    "        if len(mol.GetAtoms()) <= 100:\n",
    "            count+=1\n",
    "            filtered.append(s)\n",
    "    print('No. of molecules with <= 100 atoms =', count)\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1_tr_ligs = filter_by_atom_count(d1_tr_ligs)\n",
    "d2_tr_ligs = filter_by_atom_count(d2_tr_ligs)\n",
    "d3_tr_ligs = filter_by_atom_count(d3_tr_ligs)\n",
    "d4_tr_ligs = filter_by_atom_count(d4_tr_ligs)\n",
    "d5_tr_ligs = filter_by_atom_count(d5_tr_ligs)\n",
    "\n",
    "d1_te_ligs = filter_by_atom_count(d1_te_ligs)\n",
    "d2_te_ligs = filter_by_atom_count(d2_te_ligs)\n",
    "d3_te_ligs = filter_by_atom_count(d3_te_ligs)\n",
    "d4_te_ligs = filter_by_atom_count(d4_te_ligs)\n",
    "d5_te_ligs = filter_by_atom_count(d5_te_ligs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving train datasets\n",
    "\n",
    "with open(data_path+'train_dataset/train_4_org_1042_104/d1_tr_ligands.txt', 'w+') as f:\n",
    "    for lig in d1_tr_ligs:\n",
    "        f.write(lig+'\\n')\n",
    "print('Dataset 1 train saved.')\n",
    "\n",
    "with open(data_path+'train_dataset/train_4_org_1000_112/d2_tr_ligands.txt', 'w+') as f:\n",
    "    for lig in d2_tr_ligs:\n",
    "        f.write(lig+'\\n')\n",
    "print('Dataset 2 train saved.')\n",
    "\n",
    "with open(data_path+'train_dataset/train_4_org_1004_122/d3_tr_ligands.txt', 'w+') as f:\n",
    "    for lig in d3_tr_ligs:\n",
    "        f.write(lig+'\\n')\n",
    "print('Dataset 3 train saved.')\n",
    "\n",
    "with open(data_path+'train_dataset/train_4_org_1002_103/d4_tr_ligands.txt', 'w+') as f:\n",
    "    for lig in d4_tr_ligs:\n",
    "        f.write(lig+'\\n')\n",
    "print('Dataset 4 train saved.')\n",
    "\n",
    "with open(data_path+'train_dataset/train_4_org_1036_124/d5_tr_ligands.txt', 'w+') as f:\n",
    "    for lig in d5_tr_ligs:\n",
    "        f.write(lig+'\\n')\n",
    "print('Dataset 5 train saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Saving test datasets\n",
    "\n",
    "with open(data_path+'test_dataset/test_4_org_1042_104/d1_te_ligands.txt', 'w+') as f:\n",
    "    for lig in d1_te_ligs:\n",
    "        f.write(lig+'\\n')\n",
    "print('Dataset 1 test saved.')\n",
    "\n",
    "with open(data_path+'test_dataset/test_4_org_1000_112/d2_te_ligands.txt', 'w+') as f:\n",
    "    for lig in d2_te_ligs:\n",
    "        f.write(lig+'\\n')\n",
    "print('Dataset 2 test saved.')\n",
    "\n",
    "with open(data_path+'test_dataset/test_4_org_1004_122/d3_te_ligands.txt', 'w+') as f:\n",
    "    for lig in d3_te_ligs:\n",
    "        f.write(lig+'\\n')\n",
    "print('Dataset 3 test saved.')\n",
    "\n",
    "with open(data_path+'test_dataset/test_4_org_1002_103/d4_te_ligands.txt', 'w+') as f:\n",
    "    for lig in d4_te_ligs:\n",
    "        f.write(lig+'\\n')\n",
    "print('Dataset 4 test saved.')\n",
    "\n",
    "with open(data_path+'test_dataset/test_4_org_1036_124/d5_te_ligands.txt', 'w+') as f:\n",
    "    for lig in d5_te_ligs:\n",
    "        f.write(lig+'\\n')\n",
    "print('Dataset 5 test saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train datasets details:')\n",
    "\n",
    "d1_tr_ligs = []\n",
    "d2_tr_ligs = []\n",
    "d3_tr_ligs = []\n",
    "d4_tr_ligs = []\n",
    "d5_tr_ligs = []\n",
    "\n",
    "with open(data_path+'train_dataset/train_4_org_1042_104/d1_tr_ligands.txt') as f:\n",
    "    for line in f:\n",
    "        d1_tr_ligs.append(line.strip())\n",
    "print('Dataset 1 ligands count =', len(d1_tr_ligs))\n",
    "\n",
    "with open(data_path+'train_dataset/train_4_org_1000_112/d2_tr_ligands.txt') as f:\n",
    "    for line in f:\n",
    "        d2_tr_ligs.append(line.strip())\n",
    "print('Dataset 2 ligands count =', len(d2_tr_ligs))\n",
    "\n",
    "with open(data_path+'train_dataset/train_4_org_1004_122/d3_tr_ligands.txt') as f:\n",
    "    for line in f:\n",
    "        d3_tr_ligs.append(line.strip())\n",
    "print('Dataset 3 ligands count =', len(d3_tr_ligs))\n",
    "\n",
    "with open(data_path+'train_dataset/train_4_org_1002_103/d4_tr_ligands.txt') as f:\n",
    "    for line in f:\n",
    "        d4_tr_ligs.append(line.strip())\n",
    "print('Dataset 4 ligands count =', len(d4_tr_ligs))\n",
    "\n",
    "with open(data_path+'train_dataset/train_4_org_1036_124/d5_tr_ligands.txt') as f:\n",
    "    for line in f:\n",
    "        d5_tr_ligs.append(line.strip())\n",
    "print('Dataset 5 ligands count =', len(d5_tr_ligs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Test datasets details:')\n",
    "\n",
    "d1_te_ligs = []\n",
    "d2_te_ligs = []\n",
    "d3_te_ligs = []\n",
    "d4_te_ligs = []\n",
    "d5_te_ligs = []\n",
    "\n",
    "with open(data_path+'test_dataset/test_4_org_1042_104/d1_te_ligands.txt') as f:\n",
    "    for line in f:\n",
    "        d1_te_ligs.append(line.strip())\n",
    "print('Dataset 1 ligands count =', len(d1_te_ligs))\n",
    "\n",
    "with open(data_path+'test_dataset/test_4_org_1000_112/d2_te_ligands.txt') as f:\n",
    "    for line in f:\n",
    "        d2_te_ligs.append(line.strip())\n",
    "print('Dataset 2 ligands count =', len(d2_te_ligs))\n",
    "\n",
    "with open(data_path+'test_dataset/test_4_org_1004_122/d3_te_ligands.txt') as f:\n",
    "    for line in f:\n",
    "        d3_te_ligs.append(line.strip())\n",
    "print('Dataset 3 ligands count =', len(d3_te_ligs))\n",
    "\n",
    "with open(data_path+'test_dataset/test_4_org_1002_103/d4_te_ligands.txt') as f:\n",
    "    for line in f:\n",
    "        d4_te_ligs.append(line.strip())\n",
    "print('Dataset 4 ligands count =', len(d4_te_ligs))\n",
    "\n",
    "with open(data_path+'test_dataset/test_4_org_1036_124/d5_te_ligands.txt') as f:\n",
    "    for line in f:\n",
    "        d5_te_ligs.append(line.strip())\n",
    "print('Dataset 5 ligands count =', len(d5_te_ligs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! We got our ligand datasets. Now we will store all the unique atom types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def give_atom_types(smiles):\n",
    "    atom_types = []\n",
    "    for s in smiles:\n",
    "        mol = Chem.MolFromSmiles(s)\n",
    "        for atom in mol.GetAtoms():\n",
    "            atom_types.append(str(atom.GetSymbol())+','+str(atom.GetFormalCharge())+','+str(atom.GetNumExplicitHs()))\n",
    "    return list(set(atom_types))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atom_types = give_atom_types(d1_tr_ligs)\n",
    "atom_types += give_atom_types(d2_tr_ligs)\n",
    "atom_types += give_atom_types(d3_tr_ligs)\n",
    "atom_types += give_atom_types(d4_tr_ligs)\n",
    "atom_types += give_atom_types(d5_tr_ligs)\n",
    "\n",
    "atom_types += give_atom_types(d1_te_ligs)\n",
    "atom_types += give_atom_types(d2_te_ligs)\n",
    "atom_types += give_atom_types(d3_te_ligs)\n",
    "atom_types += give_atom_types(d4_te_ligs)\n",
    "atom_types += give_atom_types(d5_te_ligs)\n",
    "\n",
    "atom_types = list(set(atom_types))\n",
    "len(atom_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! Now we remove the BindingDB ligands from the ChEMBL dataset to avoid duplicacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chembl_path = '/home/iit/CDGCN/data/chembl/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "chembl_df = pd.read_csv(chembl_path+'chembl.csv', delimiter=';')\n",
    "chembl_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! We have the ChEMBL dataset based on the physiochemical filters as suggested by Grechishnikova. We now further remove molecules with > 100 atoms and perform salt removal. We then remove the bindingdb ligands from the chembl molecules then store the chembl molecules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chembl_mols = []\n",
    "\n",
    "for s in chembl_df['Smiles']:\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(s)\n",
    "        if len(mol.GetAtoms()) <= 100:\n",
    "            chembl_mols.append([b for b in s.split('.') if len(b) == max([len(bs) for bs in s.split('.')])][0])\n",
    "    except:\n",
    "        continue\n",
    "        \n",
    "len(chembl_mols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bindingdb_ligs = list(set(d1_tr_ligs+d2_tr_ligs+d3_tr_ligs+d4_tr_ligs+d5_tr_ligs+d1_te_ligs+d2_te_ligs+d3_te_ligs+d4_te_ligs+d5_te_ligs))\n",
    "len(bindingdb_ligs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(chembl_path+'chembl.txt', 'w+') as f:\n",
    "    for s in list(set(chembl_mols) - set(bindingdb_ligs)):\n",
    "        f.write(s+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Superb! We will now compute the atom types for the chembl molecules and combine them with bindingdb atom types and then store them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chembl_mols = []\n",
    "\n",
    "with open(chembl_path+'chembl.txt') as f:\n",
    "    for line in f:\n",
    "        chembl_mols.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_atom_types = give_atom_types(chembl_mols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(ch_atom_types))\n",
    "atom_types = list(set(ch_atom_types+atom_types))\n",
    "len(atom_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store atom types\n",
    "with open('/home/iit/CDGCN/data/atom_types.txt', 'w+') as f:\n",
    "    for at in atom_types:\n",
    "        f.write(at+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing pretrained protein embedding module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train datasets details:')\n",
    "\n",
    "d1_tr_uprots = []\n",
    "d2_tr_uprots = []\n",
    "d3_tr_uprots = []\n",
    "d4_tr_uprots = []\n",
    "d5_tr_uprots = []\n",
    "\n",
    "with open(data_path+'train_dataset/train_4_org_1042_104/d1_tr_unique_proteins.txt') as f:\n",
    "    for line in f:\n",
    "        d1_tr_uprots.append(line.strip().replace(' ', ''))\n",
    "print('Dataset 1 proteins count =', len(set(d1_tr_uprots)))\n",
    "\n",
    "with open(data_path+'train_dataset/train_4_org_1000_112/d2_tr_unique_proteins.txt') as f:\n",
    "    for line in f:\n",
    "        d2_tr_uprots.append(line.strip().replace(' ', ''))\n",
    "print('Dataset 2 proteins count =', len(set(d2_tr_uprots)))\n",
    "\n",
    "with open(data_path+'train_dataset/train_4_org_1004_122/d3_tr_unique_proteins.txt') as f:\n",
    "    for line in f:\n",
    "        d3_tr_uprots.append(line.strip().replace(' ', ''))\n",
    "print('Dataset 3 proteins count =', len(set(d3_tr_uprots)))\n",
    "\n",
    "with open(data_path+'train_dataset/train_4_org_1002_103/d4_tr_unique_proteins.txt') as f:\n",
    "    for line in f:\n",
    "        d4_tr_uprots.append(line.strip().replace(' ', ''))\n",
    "print('Dataset 4 proteins count =', len(set(d4_tr_uprots)))\n",
    "\n",
    "with open(data_path+'train_dataset/train_4_org_1036_124/d5_tr_unique_proteins.txt') as f:\n",
    "    for line in f:\n",
    "        d5_tr_uprots.append(line.strip().replace(' ', ''))\n",
    "print('Dataset 5 proteins count =', len(set(d5_tr_uprots)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test datasets details:')\n",
    "\n",
    "d1_te_uprots = []\n",
    "d2_te_uprots = []\n",
    "d3_te_uprots = []\n",
    "d4_te_uprots = []\n",
    "d5_te_uprots = []\n",
    "\n",
    "with open(data_path+'test_dataset/test_4_org_1042_104/d1_te_unique_proteins.txt') as f:\n",
    "    for line in f:\n",
    "        d1_te_uprots.append(line.strip().replace(' ', ''))\n",
    "print('Dataset 1 proteins count =', len(set(d1_te_uprots)))\n",
    "\n",
    "with open(data_path+'test_dataset/test_4_org_1000_112/d2_te_unique_proteins.txt') as f:\n",
    "    for line in f:\n",
    "        d2_te_uprots.append(line.strip().replace(' ', ''))\n",
    "print('Dataset 2 proteins count =', len(set(d2_te_uprots)))\n",
    "\n",
    "with open(data_path+'test_dataset/test_4_org_1004_122/d3_te_unique_proteins.txt') as f:\n",
    "    for line in f:\n",
    "        d3_te_uprots.append(line.strip().replace(' ', ''))\n",
    "print('Dataset 3 proteins count =', len(set(d3_te_uprots)))\n",
    "\n",
    "with open(data_path+'test_dataset/test_4_org_1002_103/d4_te_unique_proteins.txt') as f:\n",
    "    for line in f:\n",
    "        d4_te_uprots.append(line.strip().replace(' ', ''))\n",
    "print('Dataset 4 proteins count =', len(set(d4_te_uprots)))\n",
    "\n",
    "with open(data_path+'test_dataset/test_4_org_1036_124/d5_te_unique_proteins.txt') as f:\n",
    "    for line in f:\n",
    "        d5_te_uprots.append(line.strip().replace(' ', ''))\n",
    "print('Dataset 5 proteins count =', len(set(d5_te_uprots)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtaining protein embeddings using pretrained model from Dallago et al."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bio_embeddings.embed import ProtTransBertBFDEmbedder\n",
    "\n",
    "def give_bioembeddings(protein_list):\n",
    "    embedder = ProtTransBertBFDEmbedder()\n",
    "    embedding_list = []\n",
    "    for protein in protein_list:\n",
    "        embedding_list.append(embedder.embed(protein))\n",
    "    return embedding_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1_tr_embeds = give_bioembeddings(d1_tr_uprots)\n",
    "d2_tr_embeds = give_bioembeddings(d2_tr_uprots)\n",
    "d3_tr_embeds = give_bioembeddings(d3_tr_uprots)\n",
    "d4_tr_embeds = give_bioembeddings(d4_tr_uprots)\n",
    "d5_tr_embeds = give_bioembeddings(d5_tr_uprots)\n",
    "\n",
    "d1_te_embeds = give_bioembeddings(d1_te_uprots)\n",
    "d2_te_embeds = give_bioembeddings(d2_te_uprots)\n",
    "d3_te_embeds = give_bioembeddings(d3_te_uprots)\n",
    "d4_te_embeds = give_bioembeddings(d4_te_uprots)\n",
    "d5_te_embeds = give_bioembeddings(d5_te_uprots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(d1_tr_embeds[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving train datasets\n",
    "\n",
    "with open(data_path+'train_dataset/train_4_org_1042_104/d1_tr_unique_bioembeddings.txt', 'w+') as f:\n",
    "    for embed in d1_tr_embeds:\n",
    "        for e in embed:\n",
    "            f.write(str(e)+'\\t')\n",
    "        f.write('\\n')\n",
    "        \n",
    "with open(data_path+'train_dataset/train_4_org_1000_112/d2_tr_unique_bioembeddings.txt', 'w+') as f:\n",
    "    for embed in d2_tr_embeds:\n",
    "        for e in embed:\n",
    "            f.write(str(e)+'\\t')\n",
    "        f.write('\\n')\n",
    "        \n",
    "with open(data_path+'train_dataset/train_4_org_1004_122/d3_tr_unique_bioembeddings.txt', 'w+') as f:\n",
    "    for embed in d3_tr_embeds:\n",
    "        for e in embed:\n",
    "            f.write(str(e)+'\\t')\n",
    "        f.write('\\n')\n",
    "        \n",
    "with open(data_path+'train_dataset/train_4_org_1002_103/d4_tr_unique_bioembeddings.txt', 'w+') as f:\n",
    "    for embed in d4_tr_embeds:\n",
    "        for e in embed:\n",
    "            f.write(str(e)+'\\t')\n",
    "        f.write('\\n')\n",
    "        \n",
    "with open(data_path+'train_dataset/train_4_org_1036_124/d5_tr_unique_bioembeddings.txt', 'w+') as f:\n",
    "    for embed in d5_tr_embeds:\n",
    "        for e in embed:\n",
    "            f.write(str(e)+'\\t')\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving test datasets\n",
    "\n",
    "with open(data_path+'test_dataset/test_4_org_1042_104/d1_te_unique_bioembeddings.txt', 'w+') as f:\n",
    "    for embed in d1_te_embeds:\n",
    "        for e in embed:\n",
    "            f.write(str(e)+'\\t')\n",
    "        f.write('\\n')\n",
    "        \n",
    "with open(data_path+'test_dataset/test_4_org_1000_112/d2_te_unique_bioembeddings.txt', 'w+') as f:\n",
    "    for embed in d2_te_embeds:\n",
    "        for e in embed:\n",
    "            f.write(str(e)+'\\t')\n",
    "        f.write('\\n')\n",
    "        \n",
    "with open(data_path+'test_dataset/test_4_org_1004_122/d3_te_unique_bioembeddings.txt', 'w+') as f:\n",
    "    for embed in d3_te_embeds:\n",
    "        for e in embed:\n",
    "            f.write(str(e)+'\\t')\n",
    "        f.write('\\n')\n",
    "        \n",
    "with open(data_path+'test_dataset/test_4_org_1002_103/d4_te_unique_bioembeddings.txt', 'w+') as f:\n",
    "    for embed in d4_te_embeds:\n",
    "        for e in embed:\n",
    "            f.write(str(e)+'\\t')\n",
    "        f.write('\\n')\n",
    "        \n",
    "with open(data_path+'test_dataset/test_4_org_1036_124/d5_te_unique_bioembeddings.txt', 'w+') as f:\n",
    "    for embed in d5_te_embeds:\n",
    "        for e in embed:\n",
    "            f.write(str(e)+'\\t')\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hurrah! We have stored the pretrained protein embeddings. Now we will construct the final datasets for all three models: CDGCN, Grechishnikova, Li et al.\n",
    "\n",
    "## Finalising all five datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train datasets details:')\n",
    "\n",
    "d1_tr_uprots = []\n",
    "d2_tr_uprots = []\n",
    "d3_tr_uprots = []\n",
    "d4_tr_uprots = []\n",
    "d5_tr_uprots = []\n",
    "\n",
    "with open(data_path+'train_dataset/train_4_org_1042_104/d1_tr_unique_proteins.txt') as f:\n",
    "    for line in f:\n",
    "        d1_tr_uprots.append(line)\n",
    "print('Dataset 1 proteins count =', len(set(d1_tr_uprots)))\n",
    "\n",
    "with open(data_path+'train_dataset/train_4_org_1000_112/d2_tr_unique_proteins.txt') as f:\n",
    "    for line in f:\n",
    "        d2_tr_uprots.append(line)\n",
    "print('Dataset 2 proteins count =', len(set(d2_tr_uprots)))\n",
    "\n",
    "with open(data_path+'train_dataset/train_4_org_1004_122/d3_tr_unique_proteins.txt') as f:\n",
    "    for line in f:\n",
    "        d3_tr_uprots.append(line)\n",
    "print('Dataset 3 proteins count =', len(set(d3_tr_uprots)))\n",
    "\n",
    "with open(data_path+'train_dataset/train_4_org_1002_103/d4_tr_unique_proteins.txt') as f:\n",
    "    for line in f:\n",
    "        d4_tr_uprots.append(line)\n",
    "print('Dataset 4 proteins count =', len(set(d4_tr_uprots)))\n",
    "\n",
    "with open(data_path+'train_dataset/train_4_org_1036_124/d5_tr_unique_proteins.txt') as f:\n",
    "    for line in f:\n",
    "        d5_tr_uprots.append(line)\n",
    "print('Dataset 5 proteins count =', len(set(d5_tr_uprots)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Validation datasets details:')\n",
    "\n",
    "d1_va_uprots = []\n",
    "d2_va_uprots = []\n",
    "d3_va_uprots = []\n",
    "d4_va_uprots = []\n",
    "d5_va_uprots = []\n",
    "\n",
    "with open(data_path+'test_dataset/test_4_org_1042_104/d1_va_unique_proteins.txt') as f:\n",
    "    for line in f:\n",
    "        d1_va_uprots.append(line)\n",
    "print('Dataset 1 proteins count =', len(set(d1_va_uprots)))\n",
    "\n",
    "with open(data_path+'test_dataset/test_4_org_1000_112/d2_va_unique_proteins.txt') as f:\n",
    "    for line in f:\n",
    "        d2_va_uprots.append(line)\n",
    "print('Dataset 2 proteins count =', len(set(d2_va_uprots)))\n",
    "\n",
    "with open(data_path+'test_dataset/test_4_org_1004_122/d3_va_unique_proteins.txt') as f:\n",
    "    for line in f:\n",
    "        d3_va_uprots.append(line)\n",
    "print('Dataset 3 proteins count =', len(set(d3_va_uprots)))\n",
    "\n",
    "with open(data_path+'test_dataset/test_4_org_1002_103/d4_va_unique_proteins.txt') as f:\n",
    "    for line in f:\n",
    "        d4_va_uprots.append(line)\n",
    "print('Dataset 4 proteins count =', len(set(d4_va_uprots)))\n",
    "\n",
    "with open(data_path+'test_dataset/test_4_org_1036_124/d5_va_unique_proteins.txt') as f:\n",
    "    for line in f:\n",
    "        d5_va_uprots.append(line)\n",
    "print('Dataset 5 proteins count =', len(set(d5_va_uprots)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test datasets details:')\n",
    "\n",
    "d1_te_uprots = []\n",
    "d2_te_uprots = []\n",
    "d3_te_uprots = []\n",
    "d4_te_uprots = []\n",
    "d5_te_uprots = []\n",
    "\n",
    "with open(data_path+'test_dataset/test_4_org_1042_104/d1_te_unique_proteins.txt') as f:\n",
    "    for line in f:\n",
    "        d1_te_uprots.append(line)\n",
    "print('Dataset 1 proteins count =', len(set(d1_te_uprots)))\n",
    "\n",
    "with open(data_path+'test_dataset/test_4_org_1000_112/d2_te_unique_proteins.txt') as f:\n",
    "    for line in f:\n",
    "        d2_te_uprots.append(line)\n",
    "print('Dataset 2 proteins count =', len(set(d2_te_uprots)))\n",
    "\n",
    "with open(data_path+'test_dataset/test_4_org_1004_122/d3_te_unique_proteins.txt') as f:\n",
    "    for line in f:\n",
    "        d3_te_uprots.append(line)\n",
    "print('Dataset 3 proteins count =', len(set(d3_te_uprots)))\n",
    "\n",
    "with open(data_path+'test_dataset/test_4_org_1002_103/d4_te_unique_proteins.txt') as f:\n",
    "    for line in f:\n",
    "        d4_te_uprots.append(line)\n",
    "print('Dataset 4 proteins count =', len(set(d4_te_uprots)))\n",
    "\n",
    "with open(data_path+'test_dataset/test_4_org_1036_124/d5_te_unique_proteins.txt') as f:\n",
    "    for line in f:\n",
    "        d5_te_uprots.append(line)\n",
    "print('Dataset 5 proteins count =', len(set(d5_te_uprots)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(17)\n",
    "d1_va_uprots = random.sample(d1_te_uprots, len(d1_te_uprots)//2)\n",
    "d2_va_uprots = random.sample(d2_te_uprots, len(d2_te_uprots)//2)\n",
    "d3_va_uprots = random.sample(d3_te_uprots, len(d3_te_uprots)//2)\n",
    "d4_va_uprots = random.sample(d4_te_uprots, len(d4_te_uprots)//2)\n",
    "d5_va_uprots = random.sample(d5_te_uprots, len(d5_te_uprots)//2)\n",
    "\n",
    "print(len(d1_va_uprots))\n",
    "print(len(d2_va_uprots))\n",
    "print(len(d3_va_uprots))\n",
    "print(len(d4_va_uprots))\n",
    "print(len(d5_va_uprots))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1_te_uprots = list(set(d1_te_uprots)-set(d1_va_uprots))\n",
    "d2_te_uprots = list(set(d2_te_uprots)-set(d2_va_uprots))\n",
    "d3_te_uprots = list(set(d3_te_uprots)-set(d3_va_uprots))\n",
    "d4_te_uprots = list(set(d4_te_uprots)-set(d4_va_uprots))\n",
    "d5_te_uprots = list(set(d5_te_uprots)-set(d5_va_uprots))\n",
    "\n",
    "print(len(d1_te_uprots))\n",
    "print(len(d2_te_uprots))\n",
    "print(len(d3_te_uprots))\n",
    "print(len(d4_te_uprots))\n",
    "print(len(d5_te_uprots))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving validation datasets\n",
    "\n",
    "with open(data_path+'test_dataset/test_4_org_1042_104/d1_va_unique_proteins.txt', 'w+') as f:\n",
    "    for prot in d1_va_uprots:\n",
    "        f.write(prot)\n",
    "print('Dataset 1 test saved.')\n",
    "\n",
    "with open(data_path+'test_dataset/test_4_org_1000_112/d2_va_unique_proteins.txt', 'w+') as f:\n",
    "    for prot in d2_va_uprots:\n",
    "        f.write(prot)\n",
    "print('Dataset 2 test saved.')\n",
    "\n",
    "with open(data_path+'test_dataset/test_4_org_1004_122/d3_va_unique_proteins.txt', 'w+') as f:\n",
    "    for prot in d3_va_uprots:\n",
    "        f.write(prot)\n",
    "print('Dataset 3 test saved.')\n",
    "\n",
    "with open(data_path+'test_dataset/test_4_org_1002_103/d4_va_unique_proteins.txt', 'w+') as f:\n",
    "    for prot in d4_va_uprots:\n",
    "        f.write(prot)\n",
    "print('Dataset 4 test saved.')\n",
    "\n",
    "with open(data_path+'test_dataset/test_4_org_1036_124/d5_va_unique_proteins.txt', 'w+') as f:\n",
    "    for prot in d5_va_uprots:\n",
    "        f.write(prot)\n",
    "print('Dataset 5 test saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving test datasets\n",
    "\n",
    "with open(data_path+'test_dataset/test_4_org_1042_104/d1_te_unique_proteins.txt', 'w+') as f:\n",
    "    for prot in d1_te_uprots:\n",
    "        f.write(prot)\n",
    "print('Dataset 1 test saved.')\n",
    "\n",
    "with open(data_path+'test_dataset/test_4_org_1000_112/d2_te_unique_proteins.txt', 'w+') as f:\n",
    "    for prot in d2_te_uprots:\n",
    "        f.write(prot)\n",
    "print('Dataset 2 test saved.')\n",
    "\n",
    "with open(data_path+'test_dataset/test_4_org_1004_122/d3_te_unique_proteins.txt', 'w+') as f:\n",
    "    for prot in d3_te_uprots:\n",
    "        f.write(prot)\n",
    "print('Dataset 3 test saved.')\n",
    "\n",
    "with open(data_path+'test_dataset/test_4_org_1002_103/d4_te_unique_proteins.txt', 'w+') as f:\n",
    "    for prot in d4_te_uprots:\n",
    "        f.write(prot)\n",
    "print('Dataset 4 test saved.')\n",
    "\n",
    "with open(data_path+'test_dataset/test_4_org_1036_124/d5_te_unique_proteins.txt', 'w+') as f:\n",
    "    for prot in d5_te_uprots:\n",
    "        f.write(prot)\n",
    "print('Dataset 5 test saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grechishnikova"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train datasets details:')\n",
    "\n",
    "d1_tr_prots = []\n",
    "d2_tr_prots = []\n",
    "d3_tr_prots = []\n",
    "d4_tr_prots = []\n",
    "d5_tr_prots = []\n",
    "\n",
    "with open(data_path+'train_dataset/train_4_org_1042_104/proteins_train_4_org_corrected') as f:\n",
    "    for line in f:\n",
    "        d1_tr_prots.append(line)\n",
    "print('Dataset 1 proteins count =', len(set(d1_tr_prots)))\n",
    "\n",
    "with open(data_path+'train_dataset/train_4_org_1000_112/proteins_train_4_org_corrected') as f:\n",
    "    for line in f:\n",
    "        d2_tr_prots.append(line)\n",
    "print('Dataset 2 proteins count =', len(set(d2_tr_prots)))\n",
    "\n",
    "with open(data_path+'train_dataset/train_4_org_1004_122/proteins_train_4_org_corrected') as f:\n",
    "    for line in f:\n",
    "        d3_tr_prots.append(line)\n",
    "print('Dataset 3 proteins count =', len(set(d3_tr_prots)))\n",
    "\n",
    "with open(data_path+'train_dataset/train_4_org_1002_103/proteins_train_4_org_corrected') as f:\n",
    "    for line in f:\n",
    "        d4_tr_prots.append(line)\n",
    "print('Dataset 4 proteins count =', len(set(d4_tr_prots)))\n",
    "\n",
    "with open(data_path+'train_dataset/train_4_org_1036_124/proteins_train_4_org_corrected') as f:\n",
    "    for line in f:\n",
    "        d5_tr_prots.append(line)\n",
    "print('Dataset 5 proteins count =', len(set(d5_tr_prots)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Test datasets details:')\n",
    "\n",
    "d1_te_prots = []\n",
    "d2_te_prots = []\n",
    "d3_te_prots = []\n",
    "d4_te_prots = []\n",
    "d5_te_prots = []\n",
    "\n",
    "with open(data_path+'test_dataset/test_4_org_1042_104/proteins_test_4_org_corrected') as f:\n",
    "    for line in f:\n",
    "        d1_te_prots.append(line)\n",
    "print('Dataset 1 proteins count =', len(set(d1_te_prots)))\n",
    "\n",
    "with open(data_path+'test_dataset/test_4_org_1000_112/proteins_test_4_org_corrected') as f:\n",
    "    for line in f:\n",
    "        d2_te_prots.append(line)\n",
    "print('Dataset 2 proteins count =', len(set(d2_te_prots)))\n",
    "\n",
    "with open(data_path+'test_dataset/test_4_org_1004_122/proteins_test_4_org_corrected') as f:\n",
    "    for line in f:\n",
    "        d3_te_prots.append(line)\n",
    "print('Dataset 3 proteins count =', len(set(d3_te_prots)))\n",
    "\n",
    "with open(data_path+'test_dataset/test_4_org_1002_103/proteins_test_4_org_corrected') as f:\n",
    "    for line in f:\n",
    "        d4_te_prots.append(line)\n",
    "print('Dataset 4 proteins count =', len(set(d4_te_prots)))\n",
    "\n",
    "with open(data_path+'test_dataset/test_4_org_1036_124/proteins_test_4_org_corrected') as f:\n",
    "    for line in f:\n",
    "        d5_te_prots.append(line)\n",
    "print('Dataset 5 proteins count =', len(set(d5_te_prots)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train datasets details:')\n",
    "\n",
    "d1_tr_ligs = []\n",
    "d2_tr_ligs = []\n",
    "d3_tr_ligs = []\n",
    "d4_tr_ligs = []\n",
    "d5_tr_ligs = []\n",
    "\n",
    "with open(data_path+'train_dataset/train_4_org_1042_104/d1_tr_ligands.txt') as f:\n",
    "    for line in f:\n",
    "        d1_tr_ligs.append(line.strip())\n",
    "print('Dataset 1 ligands count =', len(d1_tr_ligs))\n",
    "\n",
    "with open(data_path+'train_dataset/train_4_org_1000_112/d2_tr_ligands.txt') as f:\n",
    "    for line in f:\n",
    "        d2_tr_ligs.append(line.strip())\n",
    "print('Dataset 2 ligands count =', len(d2_tr_ligs))\n",
    "\n",
    "with open(data_path+'train_dataset/train_4_org_1004_122/d3_tr_ligands.txt') as f:\n",
    "    for line in f:\n",
    "        d3_tr_ligs.append(line.strip())\n",
    "print('Dataset 3 ligands count =', len(d3_tr_ligs))\n",
    "\n",
    "with open(data_path+'train_dataset/train_4_org_1002_103/d4_tr_ligands.txt') as f:\n",
    "    for line in f:\n",
    "        d4_tr_ligs.append(line.strip())\n",
    "print('Dataset 4 ligands count =', len(d4_tr_ligs))\n",
    "\n",
    "with open(data_path+'train_dataset/train_4_org_1036_124/d5_tr_ligands.txt') as f:\n",
    "    for line in f:\n",
    "        d5_tr_ligs.append(line.strip())\n",
    "print('Dataset 5 ligands count =', len(d5_tr_ligs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Test datasets details:')\n",
    "\n",
    "d1_te_ligs = []\n",
    "d2_te_ligs = []\n",
    "d3_te_ligs = []\n",
    "d4_te_ligs = []\n",
    "d5_te_ligs = []\n",
    "\n",
    "with open(data_path+'test_dataset/test_4_org_1042_104/d1_te_ligands.txt') as f:\n",
    "    for line in f:\n",
    "        d1_te_ligs.append(line.strip())\n",
    "print('Dataset 1 ligands count =', len(d1_te_ligs))\n",
    "\n",
    "with open(data_path+'test_dataset/test_4_org_1000_112/d2_te_ligands.txt') as f:\n",
    "    for line in f:\n",
    "        d2_te_ligs.append(line.strip())\n",
    "print('Dataset 2 ligands count =', len(d2_te_ligs))\n",
    "\n",
    "with open(data_path+'test_dataset/test_4_org_1004_122/d3_te_ligands.txt') as f:\n",
    "    for line in f:\n",
    "        d3_te_ligs.append(line.strip())\n",
    "print('Dataset 3 ligands count =', len(d3_te_ligs))\n",
    "\n",
    "with open(data_path+'test_dataset/test_4_org_1002_103/d4_te_ligands.txt') as f:\n",
    "    for line in f:\n",
    "        d4_te_ligs.append(line.strip())\n",
    "print('Dataset 4 ligands count =', len(d4_te_ligs))\n",
    "\n",
    "with open(data_path+'test_dataset/test_4_org_1036_124/d5_te_ligands.txt') as f:\n",
    "    for line in f:\n",
    "        d5_te_ligs.append(line.strip())\n",
    "print('Dataset 5 ligands count =', len(d5_te_ligs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proteins = []\n",
    "ligands = []\n",
    "\n",
    "for uprot in d2_tr_uprots:\n",
    "    for i, prot in enumerate(d2_tr_prots):\n",
    "        if prot == uprot:\n",
    "            proteins.append(prot)\n",
    "            ligands.append(d2_tr_ligs[i][0]+d2_tr_ligs[i][1:-1].replace('', ' ')+d2_tr_ligs[i][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(proteins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ligands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(proteins))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(ligands))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_path+'train_dataset/train_4_org_1000_112/d2_tr_proteins.space_sep_seq', 'w+') as f:\n",
    "    for prot in proteins:\n",
    "        f.write(prot)\n",
    "\n",
    "with open(data_path+'train_dataset/train_4_org_1000_112/d2_tr_ligands.space_sep_seq', 'w+') as f:\n",
    "    for lig in ligands:\n",
    "        f.write(lig+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CDGCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proteins = []\n",
    "ligands = []\n",
    "\n",
    "with open(data_path+'train_dataset/train_4_org_1000_112/d2_tr_proteins.space_sep_seq') as f:\n",
    "    for line in f:\n",
    "        proteins.append(line)\n",
    "\n",
    "with open(data_path+'train_dataset/train_4_org_1000_112/d2_tr_ligands.space_sep_seq') as f:\n",
    "    for line in f:\n",
    "        ligands.append(line.strip().replace(' ', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ligands[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(ligands))\n",
    "print(len(proteins))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(d2_tr_uprots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeds = []\n",
    "\n",
    "with open(data_path+'test_dataset/test_4_org_1004_122/d3_te_unique_bioembeddings.txt') as f:\n",
    "    for line in f:\n",
    "        embeds.append(line.strip()+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = []\n",
    "\n",
    "for i, prot in enumerate(proteins):\n",
    "    for j, uprot in enumerate(d3_va_uprots):\n",
    "        if prot == uprot:\n",
    "            lines.append(ligands[i]+'\\t'+embeds[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_path+'test_dataset/test_4_org_1004_122/d3_va_cdgcn.txt', 'w+') as f:\n",
    "    for line in lines:\n",
    "        f.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv(data_path+'test_dataset/test_4_org_1004_122/d3_va_cdgcn.txt', delimiter='\\t', header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Li et al."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "embeds = []\n",
    "\n",
    "for emb in np.identity(1000, dtype=np.int):\n",
    "    embstr = ''\n",
    "    for e in emb:\n",
    "        embstr += str(e)+'\\t'\n",
    "    embeds.append(embstr.strip()+'\\n')\n",
    "embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = []\n",
    "\n",
    "for i, prot in enumerate(proteins):\n",
    "    for j, uprot in enumerate(d2_tr_uprots):\n",
    "        if prot == uprot:\n",
    "            lines.append(ligands[i]+'\\t'+embeds[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_path+'train_dataset/train_4_org_1000_112/d2_tr_li.txt', 'w+') as f:\n",
    "    for line in lines:\n",
    "        f.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv(data_path+'train_dataset/train_4_org_1000_112/d2_tr_li.txt', delimiter='\\t', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
