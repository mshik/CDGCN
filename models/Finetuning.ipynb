{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/iit\n"
     ]
    }
   ],
   "source": [
    "%cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1282,
     "status": "ok",
     "timestamp": 1613395455582,
     "user": {
      "displayName": "Shikha Mallick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjoRyV4TB5l3kSyBeE28ldIN2S1glHF3FTZvVEf_Q=s64",
      "userId": "13917264246341255180"
     },
     "user_tz": -330
    },
    "id": "L15osP6jjS28",
    "outputId": "5ba6a7d3-69f9-44d1-904b-0c160b70c9ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\r\n",
      "Copyright (c) 2005-2021 NVIDIA Corporation\r\n",
      "Built on Sun_Feb_14_21:12:58_PST_2021\r\n",
      "Cuda compilation tools, release 11.2, V11.2.152\r\n",
      "Build cuda_11.2.r11.2/compiler.29618528_0\r\n"
     ]
    }
   ],
   "source": [
    "# Check cuda version\n",
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QA-E_qsef5-L"
   },
   "source": [
    "# Import RDKit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "executionInfo": {
     "elapsed": 155754,
     "status": "ok",
     "timestamp": 1613395613914,
     "user": {
      "displayName": "Shikha Mallick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjoRyV4TB5l3kSyBeE28ldIN2S1glHF3FTZvVEf_Q=s64",
      "userId": "13917264246341255180"
     },
     "user_tz": -330
    },
    "id": "aSRopUnCRdvD"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('/usr/local/lib/python3.7/site-packages/')\n",
    "\n",
    "try:\n",
    "    import rdkit\n",
    "    from rdkit import Chem\n",
    "    from rdkit.Chem import Draw\n",
    "    from rdkit.Chem import rdmolfiles\n",
    "    from rdkit.Chem.Draw import IPythonConsole\n",
    "    IPythonConsole.ipython_useSVG=True\n",
    "except ImportError:\n",
    "    print('Stopping RUNTIME. Colaboratory will restart automatically. Please run again.')\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 167
    },
    "executionInfo": {
     "elapsed": 154612,
     "status": "ok",
     "timestamp": 1613395613917,
     "user": {
      "displayName": "Shikha Mallick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjoRyV4TB5l3kSyBeE28ldIN2S1glHF3FTZvVEf_Q=s64",
      "userId": "13917264246341255180"
     },
     "user_tz": -330
    },
    "id": "DsV4EWH_9ZmF",
    "outputId": "0355e97c-99cd-41e9-b6df-d23034bac70f"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "image/svg+xml": [
       "<?xml version='1.0' encoding='iso-8859-1'?>\n",
       "<svg version='1.1' baseProfile='full'\n",
       "              xmlns='http://www.w3.org/2000/svg'\n",
       "                      xmlns:rdkit='http://www.rdkit.org/xml'\n",
       "                      xmlns:xlink='http://www.w3.org/1999/xlink'\n",
       "                  xml:space='preserve'\n",
       "width='450px' height='150px' viewBox='0 0 450 150'>\n",
       "<!-- END OF HEADER -->\n",
       "<rect style='opacity:1.0;fill:#FFFFFF;stroke:none' width='450' height='150' x='0' y='0'> </rect>\n",
       "<path class='bond-0' d='M 68.2133,38.0896 L 75.6676,47.3812' style='fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
       "<path class='bond-0' d='M 75.6676,47.3812 L 83.1219,56.6727' style='fill:none;fill-rule:evenodd;stroke:#FF0000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
       "<path class='bond-1' d='M 112.087,57.3447 L 112.272,58.5455' style='fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:1.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
       "<path class='bond-1' d='M 106.932,57.5225 L 107.301,59.9242' style='fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:1.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
       "<path class='bond-1' d='M 101.777,57.7002 L 102.331,61.3028' style='fill:none;fill-rule:evenodd;stroke:#FF0000;stroke-width:1.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
       "<path class='bond-1' d='M 96.622,57.878 L 97.3604,62.6815' style='fill:none;fill-rule:evenodd;stroke:#FF0000;stroke-width:1.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
       "<path class='bond-1' d='M 91.4671,58.0558 L 92.39,64.0601' style='fill:none;fill-rule:evenodd;stroke:#FF0000;stroke-width:1.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
       "<path class='bond-2' d='M 117.242,57.1669 L 128.256,28.8602' style='fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
       "<path class='bond-27' d='M 117.242,57.1669 L 136.249,80.8589' style='fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
       "<path class='bond-3' d='M 128.256,28.8602 L 158.278,24.2455' style='fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
       "<path class='bond-4' d='M 177.285,47.9374 L 160.647,22.3447 L 155.909,26.1462 Z' style='fill:#000000;fill-rule:evenodd;fill-opacity:1;stroke:#000000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1;' />\n",
       "<path class='bond-5' d='M 177.285,47.9374 L 166.271,76.2442' style='fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
       "<path class='bond-7' d='M 177.285,47.9374 L 191.198,20.9371' style='fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
       "<path class='bond-30' d='M 177.285,47.9374 L 198.664,69.5129' style='fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
       "<path class='bond-6' d='M 166.271,76.2442 L 136.249,80.8589' style='fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
       "<path class='bond-8' d='M 191.198,20.9371 L 221.176,25.8254' style='fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
       "<path class='bond-9' d='M 221.176,25.8254 L 225.791,55.8468' style='fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
       "<path class='bond-9' d='M 227.872,29.4057 L 231.103,50.4207' style='fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
       "<path class='bond-28' d='M 221.176,25.8254 L 244.868,6.81818' style='fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
       "<path class='bond-10' d='M 225.791,55.8468 L 198.664,69.5129' style='fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
       "<path class='bond-16' d='M 225.791,55.8468 L 254.097,66.8611' style='fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
       "<path class='bond-11' d='M 198.664,69.5129 L 196.182,66.4394 L 196.094,72.5136 Z' style='fill:#000000;fill-rule:evenodd;fill-opacity:1;stroke:#000000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1;' />\n",
       "<path class='bond-31' d='M 198.664,69.5129 L 199.868,69.8969' style='fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
       "<path class='bond-31' d='M 199.868,69.8969 L 201.072,70.281' style='fill:none;fill-rule:evenodd;stroke:#0000FF;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
       "<path class='bond-12' d='M 196.138,69.4765 L 198.386,69.8954' style='fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
       "<path class='bond-12' d='M 198.386,69.8954 L 200.635,70.3142' style='fill:none;fill-rule:evenodd;stroke:#FF0000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
       "<path class='bond-13' d='M 200.54,70.3955 L 196.441,69.7272' style='fill:none;fill-rule:evenodd;stroke:#FF0000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
       "<path class='bond-13' d='M 196.441,69.7272 L 192.342,69.0588' style='fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
       "<path class='bond-14' d='M 192.342,69.0588 L 194.966,69.4261' style='fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
       "<path class='bond-14' d='M 194.966,69.4261 L 197.589,69.7935' style='fill:none;fill-rule:evenodd;stroke:#0000FF;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
       "<path class='bond-14' d='M 192.624,72.7787 L 194.46,73.0358' style='fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
       "<path class='bond-14' d='M 194.46,73.0358 L 196.297,73.2929' style='fill:none;fill-rule:evenodd;stroke:#0000FF;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
       "<path class='bond-15' d='M 192.342,69.0588 L 175.244,98.7235' style='fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
       "<path class='bond-15' d='M 175.244,98.7235 L 158.146,128.388' style='fill:none;fill-rule:evenodd;stroke:#0000FF;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
       "<path class='bond-17' d='M 254.097,66.8611 L 277.789,47.8539' style='fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
       "<path class='bond-17' d='M 253.85,59.2716 L 270.434,45.9666' style='fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
       "<path class='bond-18' d='M 277.789,47.8539 L 273.175,17.8324' style='fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
       "<path class='bond-20' d='M 277.789,47.8539 L 306.096,58.8682' style='fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
       "<path class='bond-19' d='M 273.175,17.8324 L 244.868,6.81818' style='fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
       "<path class='bond-19' d='M 266.726,21.8416 L 246.911,14.1317' style='fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
       "<path class='bond-21' d='M 306.096,58.8682 L 310.711,88.8896' style='fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
       "<path class='bond-21' d='M 312.792,62.4484 L 316.023,83.4635' style='fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
       "<path class='bond-29' d='M 306.096,58.8682 L 329.788,39.861' style='fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
       "<path class='bond-22' d='M 310.711,88.8896 L 322.853,93.6144' style='fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
       "<path class='bond-22' d='M 322.853,93.6144 L 334.996,98.3391' style='fill:none;fill-rule:evenodd;stroke:#0000FF;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
       "<path class='bond-23' d='M 343.039,96.6776 L 352.874,88.7871' style='fill:none;fill-rule:evenodd;stroke:#0000FF;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
       "<path class='bond-23' d='M 352.874,88.7871 L 362.709,80.8967' style='fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
       "<path class='bond-23' d='M 342.188,89.572 L 349.073,84.0487' style='fill:none;fill-rule:evenodd;stroke:#0000FF;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
       "<path class='bond-23' d='M 349.073,84.0487 L 355.957,78.5254' style='fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
       "<path class='bond-24' d='M 362.709,80.8967 L 358.095,50.8752' style='fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
       "<path class='bond-25' d='M 358.095,50.8752 L 329.788,39.861' style='fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
       "<path class='bond-25' d='M 351.646,54.8844 L 331.831,47.1744' style='fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
       "<path class='bond-26' d='M 358.095,50.8752 L 381.787,31.868' style='fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
       "<path  class='atom-1' d='M 83.2719 61.8059\n",
       "Q 83.2719 59.7405, 84.2925 58.5863\n",
       "Q 85.3131 57.4321, 87.2205 57.4321\n",
       "Q 89.128 57.4321, 90.1486 58.5863\n",
       "Q 91.1692 59.7405, 91.1692 61.8059\n",
       "Q 91.1692 63.8957, 90.1365 65.0863\n",
       "Q 89.1037 66.2648, 87.2205 66.2648\n",
       "Q 85.3252 66.2648, 84.2925 65.0863\n",
       "Q 83.2719 63.9078, 83.2719 61.8059\n",
       "M 87.2205 65.2929\n",
       "Q 88.5327 65.2929, 89.2374 64.4181\n",
       "Q 89.9542 63.5312, 89.9542 61.8059\n",
       "Q 89.9542 60.1171, 89.2374 59.2667\n",
       "Q 88.5327 58.404, 87.2205 58.404\n",
       "Q 85.9084 58.404, 85.1916 59.2545\n",
       "Q 84.4869 60.105, 84.4869 61.8059\n",
       "Q 84.4869 63.5433, 85.1916 64.4181\n",
       "Q 85.9084 65.2929, 87.2205 65.2929\n",
       "' fill='#FF0000'/>\n",
       "<path  class='atom-13' d='M 200.855 71.1152\n",
       "Q 200.855 69.0497, 201.876 67.8955\n",
       "Q 202.896 66.7413, 204.804 66.7413\n",
       "Q 206.711 66.7413, 207.732 67.8955\n",
       "Q 208.752 69.0497, 208.752 71.1152\n",
       "Q 208.752 73.2049, 207.72 74.3956\n",
       "Q 206.687 75.5741, 204.804 75.5741\n",
       "Q 202.908 75.5741, 201.876 74.3956\n",
       "Q 200.855 73.217, 200.855 71.1152\n",
       "M 204.804 74.6021\n",
       "Q 206.116 74.6021, 206.821 73.7273\n",
       "Q 207.537 72.8404, 207.537 71.1152\n",
       "Q 207.537 69.4264, 206.821 68.5759\n",
       "Q 206.116 67.7133, 204.804 67.7133\n",
       "Q 203.492 67.7133, 202.775 68.5637\n",
       "Q 202.07 69.4142, 202.07 71.1152\n",
       "Q 202.07 72.8526, 202.775 73.7273\n",
       "Q 203.492 74.6021, 204.804 74.6021\n",
       "' fill='#FF0000'/>\n",
       "<path  class='atom-15' d='M 199.17 65.9801\n",
       "L 201.989 70.5362\n",
       "Q 202.268 70.9857, 202.718 71.7998\n",
       "Q 203.168 72.6138, 203.192 72.6624\n",
       "L 203.192 65.9801\n",
       "L 204.334 65.9801\n",
       "L 204.334 74.582\n",
       "L 203.155 74.582\n",
       "L 200.13 69.6007\n",
       "Q 199.778 69.0175, 199.401 68.3493\n",
       "Q 199.037 67.681, 198.927 67.4745\n",
       "L 198.927 74.582\n",
       "L 197.81 74.582\n",
       "L 197.81 65.9801\n",
       "L 199.17 65.9801\n",
       "' fill='#0000FF'/>\n",
       "<path  class='atom-15' d='M 205.367 65.9801\n",
       "L 206.533 65.9801\n",
       "L 206.533 69.6371\n",
       "L 210.931 69.6371\n",
       "L 210.931 65.9801\n",
       "L 212.098 65.9801\n",
       "L 212.098 74.582\n",
       "L 210.931 74.582\n",
       "L 210.931 70.6091\n",
       "L 206.533 70.6091\n",
       "L 206.533 74.582\n",
       "L 205.367 74.582\n",
       "L 205.367 65.9801\n",
       "' fill='#0000FF'/>\n",
       "<path  class='atom-15' d='M 212.819 67.5306\n",
       "L 214.335 67.5306\n",
       "L 214.335 65.9349\n",
       "L 215.008 65.9349\n",
       "L 215.008 67.5306\n",
       "L 216.564 67.5306\n",
       "L 216.564 68.108\n",
       "L 215.008 68.108\n",
       "L 215.008 69.7117\n",
       "L 214.335 69.7117\n",
       "L 214.335 68.108\n",
       "L 212.819 68.108\n",
       "L 212.819 67.5306\n",
       "' fill='#0000FF'/>\n",
       "<path  class='atom-16' d='M 139.35 130.279\n",
       "L 140.517 130.279\n",
       "L 140.517 133.936\n",
       "L 144.915 133.936\n",
       "L 144.915 130.279\n",
       "L 146.081 130.279\n",
       "L 146.081 138.881\n",
       "L 144.915 138.881\n",
       "L 144.915 134.908\n",
       "L 140.517 134.908\n",
       "L 140.517 138.881\n",
       "L 139.35 138.881\n",
       "L 139.35 130.279\n",
       "' fill='#0000FF'/>\n",
       "<path  class='atom-16' d='M 146.498 138.579\n",
       "Q 146.707 138.042, 147.204 137.745\n",
       "Q 147.701 137.44, 148.391 137.44\n",
       "Q 149.249 137.44, 149.73 137.905\n",
       "Q 150.211 138.371, 150.211 139.196\n",
       "Q 150.211 140.038, 149.585 140.824\n",
       "Q 148.968 141.61, 147.685 142.54\n",
       "L 150.307 142.54\n",
       "L 150.307 143.182\n",
       "L 146.482 143.182\n",
       "L 146.482 142.645\n",
       "Q 147.541 141.891, 148.166 141.329\n",
       "Q 148.8 140.768, 149.104 140.263\n",
       "Q 149.409 139.758, 149.409 139.237\n",
       "Q 149.409 138.691, 149.136 138.387\n",
       "Q 148.864 138.082, 148.391 138.082\n",
       "Q 147.934 138.082, 147.629 138.266\n",
       "Q 147.324 138.451, 147.108 138.86\n",
       "L 146.498 138.579\n",
       "' fill='#0000FF'/>\n",
       "<path  class='atom-16' d='M 152.676 130.279\n",
       "L 155.495 134.835\n",
       "Q 155.774 135.285, 156.224 136.099\n",
       "Q 156.674 136.913, 156.698 136.961\n",
       "L 156.698 130.279\n",
       "L 157.84 130.279\n",
       "L 157.84 138.881\n",
       "L 156.661 138.881\n",
       "L 153.636 133.9\n",
       "Q 153.284 133.316, 152.907 132.648\n",
       "Q 152.543 131.98, 152.433 131.773\n",
       "L 152.433 138.881\n",
       "L 151.316 138.881\n",
       "L 151.316 130.279\n",
       "L 152.676 130.279\n",
       "' fill='#0000FF'/>\n",
       "<path  class='atom-23' d='M 337.116 95.6029\n",
       "L 339.935 100.159\n",
       "Q 340.214 100.609, 340.664 101.423\n",
       "Q 341.113 102.237, 341.138 102.285\n",
       "L 341.138 95.6029\n",
       "L 342.28 95.6029\n",
       "L 342.28 104.205\n",
       "L 341.101 104.205\n",
       "L 338.076 99.2235\n",
       "Q 337.723 98.6403, 337.347 97.9721\n",
       "Q 336.982 97.3039, 336.873 97.0973\n",
       "L 336.873 104.205\n",
       "L 335.755 104.205\n",
       "L 335.755 95.6029\n",
       "L 337.116 95.6029\n",
       "' fill='#0000FF'/>\n",
       "<path d='M 196.64,71.5378 L 196.64,67.4879 L 200.689,67.4879 L 200.689,71.5378 L 196.64,71.5378' style='fill:none;stroke:#FF0000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1;' />\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<rdkit.Chem.rdchem.Mol at 0x7f6c5ece1cb0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mol = Chem.MolFromSmiles('CO[C@@H]1CC[C@@]2(CC1)Cc1c([C@]32COC(=[NH+]3)N)cc(cc1)c1cncc(c1)C')\n",
    "mol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ZXs3kkbgLFc"
   },
   "source": [
    "# Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "executionInfo": {
     "elapsed": 3978,
     "status": "ok",
     "timestamp": 1613396078946,
     "user": {
      "displayName": "Shikha Mallick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjoRyV4TB5l3kSyBeE28ldIN2S1glHF3FTZvVEf_Q=s64",
      "userId": "13917264246341255180"
     },
     "user_tz": -330
    },
    "id": "c1oWZq_59bg8"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "import argparse\n",
    "import itertools\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "import networkx as nx\n",
    "from scipy import sparse\n",
    "from mxnet.gluon import nn\n",
    "from mxnet.autograd import Function\n",
    "from mxnet.gluon.data import Dataset\n",
    "from mxnet import gluon, autograd, nd\n",
    "from mxnet.gluon.data import DataLoader\n",
    "from abc import ABCMeta, abstractmethod\n",
    "from mxnet.gluon.data.sampler import Sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wpac4lJSgQxl"
   },
   "source": [
    "# Initialize Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "executionInfo": {
     "elapsed": 1655,
     "status": "ok",
     "timestamp": 1613396539509,
     "user": {
      "displayName": "Shikha Mallick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjoRyV4TB5l3kSyBeE28ldIN2S1glHF3FTZvVEf_Q=s64",
      "userId": "13917264246341255180"
     },
     "user_tz": -330
    },
    "id": "8G1r73TG9lUz"
   },
   "outputs": [],
   "source": [
    "batch_size = 8   # training batch size\n",
    "batch_size_test = 8   # test batch size\n",
    "k = 5   # number of generation paths\n",
    "p = 0.8   # randomness parameter alpha\n",
    "F_e = 16    # initial hidden embedding size for each node in a graph\n",
    "F_h = [32, 64, 128, 128, 256, 256]    # output sizes of each GCN layer\n",
    "F_skip = 256    # size of skip connection layer\n",
    "F_c = [512, ]   # hidden sizes of fully connected layers after graph convolution\n",
    "Fh_policy = 128   # hidden size for policy layer\n",
    "activation = 'relu'   # activation function\n",
    "max_epochs = 10    # maximum number of epochs\n",
    "patience = 10   # how many steps in past to check test loss for early stopping\n",
    "lr = 1e-4   # initial learning rate\n",
    "decay = 0.002    # initial weight decay\n",
    "decay_step = 100    # perform weight decay after 100 steps\n",
    "clip_grad = 3.0    # gradient clipping factor\n",
    "summary_step = 200    # store model and training metrics after every 200 steps\n",
    "N_rnn = 3   # number of layers used in GRUs\n",
    "N_C = 6165   # size of pretrained protein embedding (DGGNP) or one hot encoding (Li et al.))\n",
    "is_continuous = False   # load previous model or not\n",
    "train_only = True   # train only mode or not, in train only mode validation is not done\n",
    "ckpt_dir = 'DGGNP/outputs/logs/'   # logs directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MyVlI5TngYlE"
   },
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "executionInfo": {
     "elapsed": 8894,
     "status": "ok",
     "timestamp": 1613396085863,
     "user": {
      "displayName": "Shikha Mallick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjoRyV4TB5l3kSyBeE28ldIN2S1glHF3FTZvVEf_Q=s64",
      "userId": "13917264246341255180"
     },
     "user_tz": -330
    },
    "id": "5Z1qfqURXt7N"
   },
   "outputs": [],
   "source": [
    "def read_data(file_name):\n",
    "    smiles = []\n",
    "    with open(file_name) as f:\n",
    "        for line in f:\n",
    "            smiles.append(line.strip())\n",
    "    return smiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10246,
     "status": "ok",
     "timestamp": 1613396088085,
     "user": {
      "displayName": "Shikha Mallick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjoRyV4TB5l3kSyBeE28ldIN2S1glHF3FTZvVEf_Q=s64",
      "userId": "13917264246341255180"
     },
     "user_tz": -330
    },
    "id": "W3-NXeK0-EMT",
    "outputId": "8b4d843b-6dde-4d9b-f32f-cb1160bbd733"
   },
   "outputs": [],
   "source": [
    "t2t_data_train = '/home/iit/DGGNP/data/bindingdb/train_dataset/train_4_org_1004_122/d3_tr_dggnp.txt'\n",
    "dataset_train = read_data(t2t_data_train)\n",
    "if train_only == False:\n",
    "    t2t_data_test = '/home/iit/DGGNP/data/bindingdb/test_dataset/test_4_org_1004_122/d3_va_dggnp.txt'\n",
    "    dataset_test = read_data(t2t_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8702,
     "status": "ok",
     "timestamp": 1613396088088,
     "user": {
      "displayName": "Shikha Mallick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjoRyV4TB5l3kSyBeE28ldIN2S1glHF3FTZvVEf_Q=s64",
      "userId": "13917264246341255180"
     },
     "user_tz": -330
    },
    "id": "T5PQ9LJBbi9_",
    "outputId": "860a28b8-b328-4d8b-e8a0-7eb0884b5aa4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143999\n",
      "<class 'list'>\n",
      "['CC(C)[C@@]1(NC(=O)[C@@H]2C=C3c4cccc5[nH]cc(c45)C[C@H]3N(C)C2)O[C@@]2(O)[C@@H]3CCCN3C(=O)[C@H](Cc3ccccc3)N2C1=O\\t1\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0', 'CC(C)[C@H]1C(=O)N2CCC[C@H]2[C@]2(O)O[C@](NC(=O)[C@@H]3C=C4c5cccc6[nH]cc(c56)C[C@H]4N(C)C3)(C(C)C)C(=O)N12\\t1\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0', 'Cc1cc(C(C)(C)C)c(O)c(C)c1CC1=NCCN1\\t1\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0', 'COc1ccc(OC)c2c1C[C@H]1c3cc4c(cc3CCN1C2)OCCO4\\t1\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0', 'c1ccc2c(c1)CC[C@@H](CNCCCNC1=NCCCN1)O2\\t1\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0']\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset_train))\n",
    "print(type(dataset_train))\n",
    "print(dataset_train[:5])\n",
    "if train_only == False:\n",
    "    print(len(dataset_test))\n",
    "    print(type(dataset_test))\n",
    "    print(dataset_test[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating dataset iterable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IterableDataset(Dataset):\n",
    "\n",
    "    __metaclass__ = ABCMeta\n",
    "\n",
    "    def __iter__(self):\n",
    "        return (self[i] for i in range(len(self)))\n",
    "\n",
    "\n",
    "class Lambda(IterableDataset):\n",
    "    \"\"\"\n",
    "    Preprocessing fn\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset, fn=lambda _x: _x):\n",
    "        random.seed(17)\n",
    "        random.shuffle(dataset)\n",
    "        self.dataset = dataset\n",
    "        self.fn = fn\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.fn(self.dataset[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating mini-batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "executionInfo": {
     "elapsed": 1348,
     "status": "ok",
     "timestamp": 1613396167320,
     "user": {
      "displayName": "Shikha Mallick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjoRyV4TB5l3kSyBeE28ldIN2S1glHF3FTZvVEf_Q=s64",
      "userId": "13917264246341255180"
     },
     "user_tz": -330
    },
    "id": "WrVHTVVh5gmF"
   },
   "outputs": [],
   "source": [
    "class BalancedSampler(Sampler):\n",
    "\n",
    "    def __init__(self, cost, batch_size):\n",
    "        index = np.argsort(cost).tolist()\n",
    "        chunk_size = int(float(len(cost))/batch_size)\n",
    "        self.index = []\n",
    "        for i in range(batch_size):\n",
    "            self.index.append(index[i*chunk_size:(i + 1)*chunk_size])\n",
    "\n",
    "    def _g(self):\n",
    "        # shuffle data\n",
    "        for index_i in self.index:\n",
    "            random.shuffle(index_i)\n",
    "\n",
    "        for batch_index in zip(*self.index):\n",
    "            yield batch_index\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self._g()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separating smiles and protein embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conditional(object):\n",
    "\n",
    "    __metaclass__ = ABCMeta\n",
    "\n",
    "    @abstractmethod\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class Delimited(Conditional):\n",
    "\n",
    "    def __init__(self, d='\\t'):\n",
    "        self.d = d\n",
    "\n",
    "    def __call__(self, line):\n",
    "        line = line.strip('\\n').strip('\\r')\n",
    "        line = line.split(self.d)\n",
    "\n",
    "        smiles = line[0]\n",
    "        c = np.array([float(c_i) for c_i in line[1:]], dtype=np.float32)\n",
    "\n",
    "        return smiles, c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YJZwJjJzA6Jp"
   },
   "source": [
    "# Obtain all molecular properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "executionInfo": {
     "elapsed": 4331,
     "status": "ok",
     "timestamp": 1613396105585,
     "user": {
      "displayName": "Shikha Mallick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjoRyV4TB5l3kSyBeE28ldIN2S1glHF3FTZvVEf_Q=s64",
      "userId": "13917264246341255180"
     },
     "user_tz": -330
    },
    "id": "aYI8y_swGSEI"
   },
   "outputs": [],
   "source": [
    "class MoleculeSpec(object):\n",
    "\n",
    "    def __init__(self, file_name='DGGNP/data/atom_types.txt'):\n",
    "        self.atom_types = []\n",
    "        self.atom_symbols = []\n",
    "        with open(file_name) as f:\n",
    "            for line in f:\n",
    "                atom_type_i = line.strip('\\n').split(',')\n",
    "                self.atom_types.append((atom_type_i[0], int(atom_type_i[1]), int(atom_type_i[2])))\n",
    "                if atom_type_i[0] not in self.atom_symbols:\n",
    "                    self.atom_symbols.append(atom_type_i[0])\n",
    "        self.bond_orders = [Chem.BondType.AROMATIC,\n",
    "                            Chem.BondType.SINGLE,\n",
    "                            Chem.BondType.DOUBLE,\n",
    "                            Chem.BondType.TRIPLE]\n",
    "        self.max_iter = 120\n",
    "\n",
    "    def get_atom_type(self, atom):\n",
    "        atom_symbol = atom.GetSymbol()\n",
    "        atom_charge = atom.GetFormalCharge()\n",
    "        atom_hs = atom.GetNumExplicitHs()\n",
    "        return self.atom_types.index((atom_symbol, atom_charge, atom_hs))\n",
    "\n",
    "    def get_bond_type(self, bond):\n",
    "        return self.bond_orders.index(bond.GetBondType())\n",
    "\n",
    "    def index_to_atom(self, idx):\n",
    "        atom_symbol, atom_charge, atom_hs = self.atom_types[idx]\n",
    "        a = Chem.Atom(atom_symbol)\n",
    "        a.SetFormalCharge(atom_charge)\n",
    "        a.SetNumExplicitHs(atom_hs)\n",
    "        return a\n",
    "\n",
    "    def index_to_bond(self, mol, begin_id, end_id, idx):\n",
    "        mol.AddBond(begin_id, end_id, self.bond_orders[idx])\n",
    "\n",
    "    @property\n",
    "    def num_atom_types(self):\n",
    "        return len(self.atom_types)\n",
    "\n",
    "    @property\n",
    "    def num_bond_types(self):\n",
    "        return len(self.bond_orders)\n",
    "\n",
    "_mol_spec = None\n",
    "\n",
    "def get_mol_spec():\n",
    "    global _mol_spec\n",
    "    if _mol_spec is None:\n",
    "        _mol_spec = MoleculeSpec()\n",
    "    return _mol_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility functions for data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "executionInfo": {
     "elapsed": 2145,
     "status": "ok",
     "timestamp": 1613396164586,
     "user": {
      "displayName": "Shikha Mallick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjoRyV4TB5l3kSyBeE28ldIN2S1glHF3FTZvVEf_Q=s64",
      "userId": "13917264246341255180"
     },
     "user_tz": -330
    },
    "id": "Kr_byaDb0Apb"
   },
   "outputs": [],
   "source": [
    "def get_graph_from_smiles(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "\n",
    "    # build graph\n",
    "    atom_types, atom_ranks, bonds, bond_types = [], [], [], []\n",
    "    for a, r in zip(mol.GetAtoms(), Chem.CanonicalRankAtoms(mol)):\n",
    "        atom_types.append(get_mol_spec().get_atom_type(a))\n",
    "        atom_ranks.append(r)\n",
    "    for b in mol.GetBonds():\n",
    "        idx_1, idx_2, bt = b.GetBeginAtomIdx(), b.GetEndAtomIdx(), get_mol_spec().get_bond_type(b)\n",
    "        bonds.append([idx_1, idx_2])\n",
    "        bond_types.append(bt)\n",
    "\n",
    "    # build nx graph\n",
    "    graph = nx.Graph()\n",
    "    graph.add_nodes_from(range(len(atom_types)))\n",
    "    graph.add_edges_from(bonds)\n",
    "\n",
    "    return graph, atom_types, atom_ranks, bonds, bond_types\n",
    "\n",
    "\n",
    "def get_graph_from_smiles_list(smiles_list):\n",
    "    graph_list = []\n",
    "    for smiles in smiles_list:\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "\n",
    "        # build graph\n",
    "        atom_types, bonds, bond_types = [], [], []\n",
    "        for a in mol.GetAtoms():\n",
    "            atom_types.append(get_mol_spec().get_atom_type(a))\n",
    "        for b in mol.GetBonds():\n",
    "            idx_1, idx_2, bt = b.GetBeginAtomIdx(), b.GetEndAtomIdx(), get_mol_spec().get_bond_type(b)\n",
    "            bonds.append([idx_1, idx_2])\n",
    "            bond_types.append(bt)\n",
    "\n",
    "        X_0 = np.array(atom_types, dtype=np.int64)\n",
    "        A_0 = np.concatenate([np.array(bonds, dtype=np.int64),\n",
    "                              np.array(bond_types, dtype=np.int64)[:, np.newaxis]],\n",
    "                             axis=1)\n",
    "        graph_list.append([X_0, A_0])\n",
    "    return graph_list\n",
    "\n",
    "\n",
    "def traverse_graph(graph, atom_ranks, current_node=None, step_ids=None, p=0.9, log_p=0.0):\n",
    "    if current_node is None:\n",
    "        next_nodes = range(len(atom_ranks))\n",
    "        step_ids = [-1, ] * len(next_nodes)\n",
    "        next_node_ranks = atom_ranks\n",
    "    else:\n",
    "        next_nodes = graph.neighbors(current_node)  # get neighbor nodes\n",
    "        next_nodes = [n for n in next_nodes if step_ids[n] < 0] # filter visited nodes\n",
    "        next_node_ranks = [atom_ranks[n] for n in next_nodes] # get ranks for neighbors\n",
    "    next_nodes = [n for n, r in sorted(zip(next_nodes, next_node_ranks), key=lambda _x:_x[1])] # sort by rank\n",
    "\n",
    "    # iterate through neighbors\n",
    "    while len(next_nodes) > 0:\n",
    "        if len(next_nodes)==1:\n",
    "            next_node = next_nodes[0]\n",
    "        elif random.random() >= (1 - p):\n",
    "            next_node = next_nodes[0]\n",
    "            log_p += np.log(p)\n",
    "        else:\n",
    "            next_node = next_nodes[random.randint(1, len(next_nodes) - 1)]\n",
    "            log_p += np.log((1.0 - p) / (len(next_nodes) - 1))\n",
    "        step_ids[next_node] = max(step_ids) + 1\n",
    "        _, log_p = traverse_graph(graph, atom_ranks, next_node, step_ids, p, log_p)\n",
    "        next_nodes = [n for n in next_nodes if step_ids[n] < 0] # filter visited nodes\n",
    "\n",
    "    return step_ids, log_p\n",
    "\n",
    "\n",
    "def single_reorder(X_0, A_0, step_ids):\n",
    "    X_0, A_0 = np.copy(X_0), np.copy(A_0)\n",
    "\n",
    "    step_ids = np.array(step_ids, dtype=np.int64)\n",
    "\n",
    "    # sort by step_ids\n",
    "    sorted_ids = np.argsort(step_ids)\n",
    "    X_0 = X_0[sorted_ids]\n",
    "    A_0[:, 0], A_0[:, 1] = step_ids[A_0[:, 0]], step_ids[A_0[:, 1]]\n",
    "    max_b, min_b = np.amax(A_0[:, :2], axis=1), np.amin(A_0[:, :2], axis=1)\n",
    "    A_0 = A_0[np.lexsort([-min_b, max_b]), :]\n",
    "\n",
    "    # separate append and connect\n",
    "    max_b, min_b = np.amax(A_0[:, :2], axis=1), np.amin(A_0[:, :2], axis=1)\n",
    "    is_append = np.concatenate([np.array([True]), max_b[1:] > max_b[:-1]])\n",
    "    A_0 = np.concatenate([np.where(is_append[:, np.newaxis],\n",
    "                                 np.stack([min_b, max_b], axis=1),\n",
    "                                 np.stack([max_b, min_b], axis=1)),\n",
    "                        A_0[:, -1:]], axis=1)\n",
    "\n",
    "    return X_0, A_0\n",
    "\n",
    "\n",
    "def single_expand(X_0, A_0):\n",
    "    X_0, A_0 = np.copy(X_0), np.copy(A_0)\n",
    "\n",
    "    # expand X\n",
    "    is_append_iter = np.less(A_0[:, 0], A_0[:, 1]).astype(np.int64)\n",
    "    NX = np.cumsum(np.pad(is_append_iter, [[1, 0]], mode='constant', constant_values=1))\n",
    "    shift = np.cumsum(np.pad(NX, [[1, 0]], mode='constant')[:-1])\n",
    "    X_index = np.arange(NX.sum(), dtype=np.int64) - np.repeat(shift, NX)\n",
    "    X = X_0[X_index]\n",
    "\n",
    "    # expand A\n",
    "    _, A_index = np.tril_indices(A_0.shape[0])\n",
    "    A = A_0[A_index, :]\n",
    "    NA = np.arange(A_0.shape[0] + 1)\n",
    "\n",
    "    # get action\n",
    "    # action_type, atom_type, bond_type, append_pos, connect_pos\n",
    "    action_type = 1 - is_append_iter\n",
    "    atom_type = np.where(action_type == 0, X_0[A_0[:, 1]], 0)\n",
    "    bond_type = A_0[:, 2]\n",
    "    append_pos = np.where(action_type == 0, A_0[:, 0], 0)\n",
    "    connect_pos = np.where(action_type == 1, A_0[:, 1], 0)\n",
    "    actions = np.stack([action_type, atom_type, bond_type, append_pos, connect_pos],\n",
    "                       axis=1)\n",
    "    last_action = [[2, 0, 0, 0, 0]]\n",
    "    actions = np.append(actions, last_action, axis=0)\n",
    "\n",
    "    action_0 = np.array([X_0[0]], dtype=np.int64)\n",
    "\n",
    "    # }}}\n",
    "\n",
    "    # {{{ Get mask\n",
    "    last_atom_index = shift + NX - 1\n",
    "    last_atom_mask = np.zeros_like(X)\n",
    "    last_atom_mask[last_atom_index] = np.where(\n",
    "        np.pad(is_append_iter, [[1, 0]], mode='constant', constant_values=1) == 1,\n",
    "        np.ones_like(last_atom_index),\n",
    "        np.ones_like(last_atom_index) * 2)\n",
    "    # }}}\n",
    "\n",
    "    return action_0, X, NX, A, NA, actions, last_atom_mask\n",
    "\n",
    "\n",
    "def get_d(A, X):\n",
    "    _to_sparse = lambda _A, _X: sparse.coo_matrix((np.ones([_A.shape[0] * 2], dtype=np.int64),\n",
    "                                                   (np.concatenate([_A[:, 0], _A[:, 1]], axis=0),\n",
    "                                                    np.concatenate([_A[:, 1], _A[:, 0]], axis=0))),\n",
    "                                                  shape=[_X.shape[0], ] * 2)\n",
    "    A_sparse = _to_sparse(A, X)\n",
    "\n",
    "    d2 = A_sparse * A_sparse\n",
    "    d3 = d2 * A_sparse\n",
    "\n",
    "    # get D_2\n",
    "    D_2 = np.stack(d2.nonzero(), axis=1)\n",
    "    D_2 = D_2[D_2[:, 0] < D_2[:, 1], :]\n",
    "\n",
    "    # get D_3\n",
    "    D_3 = np.stack(d3.nonzero(), axis=1)\n",
    "    D_3 = D_3[D_3[:, 0] < D_3[:, 1], :]\n",
    "\n",
    "    # remove D_1 elements from D_3\n",
    "    D_3_sparse = _to_sparse(D_3, X)\n",
    "    D_3_sparse = D_3_sparse - D_3_sparse.multiply(A_sparse)\n",
    "    D_3 = np.stack(D_3_sparse.nonzero(), axis=1)\n",
    "    D_3 = D_3[D_3[:, 0] < D_3[:, 1], :]\n",
    "\n",
    "    return D_2, D_3\n",
    "\n",
    "\n",
    "def merge_single_0(X_0, A_0, NX_0, NA_0):\n",
    "    # shift_ids\n",
    "    cumsum = np.cumsum(np.pad(NX_0, [[1, 0]], mode='constant')[:-1])\n",
    "    A_0[:, :2] += np.stack([np.repeat(cumsum, NA_0), ] * 2, axis=1)\n",
    "\n",
    "    # get D\n",
    "    D_0_2, D_0_3 = get_d(A_0, X_0)\n",
    "\n",
    "    # split A\n",
    "    A_split = []\n",
    "    for i in range(get_mol_spec().num_bond_types):\n",
    "        A_i = A_0[A_0[:, 2] == i, :2]\n",
    "        A_split.append(A_i)\n",
    "    A_split.extend([D_0_2, D_0_3])\n",
    "    A_0 = A_split\n",
    "\n",
    "    # NX_rep\n",
    "    NX_rep_0 = np.repeat(np.arange(NX_0.shape[0]), NX_0)\n",
    "\n",
    "    return X_0, A_0, NX_0, NX_rep_0\n",
    "\n",
    "\n",
    "def merge_single(X, A,\n",
    "                 NX, NA,\n",
    "                 mol_ids, rep_ids, iw_ids,\n",
    "                 action_0, actions,\n",
    "                 last_append_mask,\n",
    "                 log_p):\n",
    "    X, A, NX, NX_rep = merge_single_0(X, A, NX, NA)\n",
    "    cumsum = np.cumsum(np.pad(NX, [[1, 0]], mode='constant')[:-1])\n",
    "    actions[:, -2] += cumsum * (actions[:, 0] == 0)\n",
    "    actions[:, -1] += cumsum * (actions[:, 0] == 1)\n",
    "    mol_ids_rep = np.repeat(mol_ids, NX)\n",
    "    rep_ids_rep = np.repeat(rep_ids, NX)\n",
    "\n",
    "    return X, A,\\\n",
    "           mol_ids_rep, rep_ids_rep, iw_ids,\\\n",
    "           last_append_mask,\\\n",
    "           NX, NX_rep,\\\n",
    "           action_0, actions, \\\n",
    "           log_p\n",
    "\n",
    "def process_single(smiles, k, p):\n",
    "    graph, atom_types, atom_ranks, bonds, bond_types = get_graph_from_smiles(smiles)\n",
    "\n",
    "    # original\n",
    "    X_0 = np.array(atom_types, dtype=np.int64)\n",
    "    A_0 = np.concatenate([np.array(bonds, dtype=np.int64),\n",
    "                          np.array(bond_types, dtype=np.int64)[:, np.newaxis]],\n",
    "                         axis=1)\n",
    "\n",
    "    X, A = [], []\n",
    "    NX, NA = [], []\n",
    "    mol_ids, rep_ids, iw_ids = [], [], []\n",
    "    action_0, actions = [], []\n",
    "    last_append_mask = []\n",
    "    log_p = []\n",
    "\n",
    "    # sampling generation paths\n",
    "    for i in range(k):\n",
    "        step_ids_i, log_p_i = traverse_graph(graph, atom_ranks, p=p)\n",
    "        X_i, A_i = single_reorder(X_0, A_0, step_ids_i)\n",
    "        action_0_i, X_i, NX_i, A_i, NA_i, actions_i, last_atom_mask_i = single_expand(X_i, A_i)\n",
    "\n",
    "        # appends\n",
    "        X.append(X_i)\n",
    "        A.append(A_i)\n",
    "        NX.append(NX_i)\n",
    "        NA.append(NA_i)\n",
    "        action_0.append(action_0_i)\n",
    "        actions.append(actions_i)\n",
    "        last_append_mask.append(last_atom_mask_i)\n",
    "\n",
    "        mol_ids.append(np.zeros_like(NX_i, dtype=np.int64))\n",
    "        rep_ids.append(np.ones_like(NX_i, dtype=np.int64) * i)\n",
    "        iw_ids.append(np.ones_like(NX_i, dtype=np.int64) * i)\n",
    "\n",
    "        log_p.append(log_p_i)\n",
    "\n",
    "    # concatenate\n",
    "    X = np.concatenate(X, axis=0)\n",
    "    A = np.concatenate(A, axis = 0)\n",
    "    NX = np.concatenate(NX, axis = 0)\n",
    "    NA = np.concatenate(NA, axis = 0)\n",
    "    action_0 = np.concatenate(action_0, axis = 0)\n",
    "    actions = np.concatenate(actions, axis = 0)\n",
    "    last_append_mask = np.concatenate(last_append_mask, axis = 0)\n",
    "    mol_ids = np.concatenate(mol_ids, axis = 0)\n",
    "    rep_ids = np.concatenate(rep_ids, axis = 0)\n",
    "    iw_ids = np.concatenate(iw_ids, axis = 0)\n",
    "    log_p = np.array(log_p, dtype=np.float32)\n",
    "\n",
    "    return X, A, NX, NA, mol_ids, rep_ids, iw_ids, action_0, actions, last_append_mask, log_p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ofH-B-Wg6HMf"
   },
   "source": [
    "# Loading data on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "executionInfo": {
     "elapsed": 1650,
     "status": "ok",
     "timestamp": 1613396168179,
     "user": {
      "displayName": "Shikha Mallick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjoRyV4TB5l3kSyBeE28ldIN2S1glHF3FTZvVEf_Q=s64",
      "userId": "13917264246341255180"
     },
     "user_tz": -330
    },
    "id": "lKHpGShB7Nhh"
   },
   "outputs": [],
   "source": [
    "class MolLoader(DataLoader):\n",
    "    \"\"\"Load graph based molecule representation from SMILES\"\"\"\n",
    "    def __init__(self, dataset, batch_size=10, num_workers=0,\n",
    "                 k=10, p=0.9, shuffle=False, sampler=None, batch_sampler=None):\n",
    "        self.k = k\n",
    "        self.p = p\n",
    "\n",
    "        # batch_sampler, sampler and shuffle are mutually exclusive\n",
    "        if batch_sampler is not None:\n",
    "            super(MolLoader, self).__init__(dataset, batch_sampler=batch_sampler,\n",
    "                                            num_workers=num_workers, batchify_fn=self._collate_fn)\n",
    "        elif sampler is not None:\n",
    "            super(MolLoader, self).__init__(dataset, sampler=sampler,\n",
    "                                            num_workers=num_workers, batchify_fn=self._collate_fn,\n",
    "                                            last_batch='rollover')\n",
    "        else:\n",
    "            super(MolLoader, self).__init__(dataset, batch_size, shuffle=shuffle,\n",
    "                                            num_workers=num_workers, batchify_fn=self._collate_fn,\n",
    "                                            last_batch='rollover')\n",
    "\n",
    "\n",
    "    def _collate_fn(self, batch):\n",
    "        # names = X, A,\n",
    "        #         NX, NA,\n",
    "        #         mol_ids, rep_ids, iw_ids,\n",
    "        #         action_0, actions,\n",
    "        #         last_append_mask,\n",
    "        #         log_p\n",
    "\n",
    "        shapes = [[0], [0, 3],\n",
    "                  [0], [0],\n",
    "                  [0], [0], [0],\n",
    "                  [0], [0, 5],\n",
    "                  [0],\n",
    "                  [0]]\n",
    "        dtypes = [np.int64, np.int64,\n",
    "                  np.int64, np.int64,\n",
    "                  np.int64, np.int64, np.int64,\n",
    "                  np.int64, np.int64,\n",
    "                  np.int64,\n",
    "                  np.float32]\n",
    "\n",
    "        _build = lambda: [np.zeros(shape=s, dtype=d) for s, d in zip(shapes, dtypes)]\n",
    "        _append = lambda _r0, _r1: [np.concatenate([__r0, __r1], axis=0)\n",
    "                                    for __r0, __r1 in zip(_r0, _r1)]\n",
    "\n",
    "        X, A, \\\n",
    "        NX, NA, \\\n",
    "        mol_ids, rep_ids, iw_ids, \\\n",
    "        action_0, actions, \\\n",
    "        last_append_mask, \\\n",
    "        log_p = _build()\n",
    "\n",
    "\n",
    "        for i, record_in in enumerate(batch):\n",
    "            smiles = record_in\n",
    "\n",
    "            X_i, A_i, \\\n",
    "            NX_i, NA_i, \\\n",
    "            mol_ids_i, rep_ids_i, iw_ids_i, \\\n",
    "            action_0_i, actions_i, \\\n",
    "            last_append_mask_i, log_p_i = process_single(smiles, self.k, self.p)\n",
    "\n",
    "            if i != 0:\n",
    "                mol_ids_i += mol_ids[-1] + 1\n",
    "                iw_ids_i += iw_ids[-1] + 1\n",
    "\n",
    "            X, A, \\\n",
    "            NX, NA, \\\n",
    "            mol_ids, rep_ids, iw_ids, \\\n",
    "            action_0, actions, \\\n",
    "            last_append_mask, \\\n",
    "            log_p = _append([X, A,\n",
    "                             NX, NA,\n",
    "                             mol_ids, rep_ids, iw_ids,\n",
    "                             action_0, actions,\n",
    "                             last_append_mask,\n",
    "                             log_p],\n",
    "                            [X_i, A_i,\n",
    "                             NX_i, NA_i,\n",
    "                             mol_ids_i, rep_ids_i, iw_ids_i,\n",
    "                             action_0_i, actions_i,\n",
    "                             last_append_mask_i,\n",
    "                             log_p_i])\n",
    "\n",
    "        X, A, \\\n",
    "        mol_ids_rep, rep_ids_rep, iw_ids, \\\n",
    "        last_append_mask, \\\n",
    "        NX, NX_rep, \\\n",
    "        action_0, actions, \\\n",
    "        log_p = merge_single(X, A,\n",
    "                                   NX, NA,\n",
    "                                   mol_ids, rep_ids, iw_ids,\n",
    "                                   action_0, actions,\n",
    "                                   last_append_mask,\n",
    "                                   log_p)\n",
    "\n",
    "        result_out = [X, A,\n",
    "                      mol_ids_rep, rep_ids_rep, iw_ids,\n",
    "                      last_append_mask,\n",
    "                      NX, NX_rep,\n",
    "                      action_0, actions,\n",
    "                      log_p]\n",
    "\n",
    "        return result_out\n",
    "\n",
    "    @staticmethod\n",
    "    def from_numpy_to_tensor(record):\n",
    "        \"\"\"Convert numpy to tensor and place it to a specific device\"\"\"\n",
    "        [X, A,\n",
    "         mol_ids_rep, rep_ids_rep, iw_ids,\n",
    "         last_append_mask,\n",
    "         NX, NX_rep,\n",
    "         action_0, actions,\n",
    "         log_p] = record\n",
    "\n",
    "        X = nd.array(X, ctx=mx.gpu(), dtype='int64')\n",
    "        A_sparse = []\n",
    "        for A_i in A:\n",
    "            if A_i.shape[0] == 0:\n",
    "                A_sparse.append(None)\n",
    "            else:\n",
    "                # transpose may not be supported in gpu\n",
    "                A_i = np.concatenate([A_i, A_i[:, [1, 0]]], axis=0)\n",
    "\n",
    "                # construct csr matrix ...\n",
    "                data = np.ones((A_i.shape[0], ), dtype=np.float32)\n",
    "                row, col = A_i[:, 0], A_i[:, 1]\n",
    "                A_sparse_i = nd.sparse.csr_matrix((data, (row, col)),\n",
    "                                                  shape=tuple([int(X.shape[0]), ]*2),\n",
    "                                                  ctx=mx.gpu(),\n",
    "                                                  dtype='float32')\n",
    "\n",
    "                # append to list\n",
    "                A_sparse.append(A_sparse_i)\n",
    "\n",
    "        batch_size, iw_size = (mol_ids_rep.max() + 1).item(), \\\n",
    "                              (rep_ids_rep.max() + 1).item()\n",
    "\n",
    "        mol_ids_rep, rep_ids_rep, iw_ids, \\\n",
    "        last_append_mask, \\\n",
    "        NX, NX_rep, action_0, actions = [nd.array(_x, ctx=mx.gpu(), dtype='int64')\n",
    "                                         for _x in [mol_ids_rep, rep_ids_rep, iw_ids,\n",
    "                                                    last_append_mask,\n",
    "                                                    NX, NX_rep, action_0, actions]]\n",
    "\n",
    "        log_p = nd.array(log_p, ctx=mx.gpu(), dtype='float32')\n",
    "\n",
    "        record = [X, A_sparse, iw_ids, last_append_mask,\n",
    "                  NX, NX_rep, action_0, actions, log_p,\n",
    "                  batch_size, iw_size]\n",
    "\n",
    "\n",
    "        return record\n",
    "\n",
    "\n",
    "class MolRNNLoader(MolLoader):\n",
    "\n",
    "    def _collate_fn(self, batch):\n",
    "        result_out = super(MolRNNLoader, self)._collate_fn(batch)\n",
    "\n",
    "        # things ready for rnn\n",
    "        mol_list = [Chem.MolFromSmiles(batch_i) for batch_i in batch]\n",
    "        # preparing mapping\n",
    "        graph_to_rnn = np.zeros((len(batch), self.k, get_mol_spec().max_iter), dtype=np.int64)\n",
    "        rnn_to_graph = []\n",
    "        cum_sum = 0\n",
    "        for i, mol_i in enumerate(mol_list):\n",
    "            num_iter = mol_i.GetNumBonds() + 1\n",
    "            for k in range(self.k):\n",
    "                graph_to_rnn[i, k, :num_iter] = (np.arange(num_iter) + cum_sum)\n",
    "\n",
    "                rnn_to_graph_0 = np.ones([num_iter,], dtype=np.int64) * i\n",
    "                rnn_to_graph_1 = np.ones_like(rnn_to_graph_0) * k\n",
    "                rnn_to_graph_2 = np.arange(num_iter)\n",
    "                rnn_to_graph.append(np.stack([rnn_to_graph_0, rnn_to_graph_1, rnn_to_graph_2], axis=0))\n",
    "\n",
    "                cum_sum += num_iter\n",
    "        rnn_to_graph = np.concatenate(rnn_to_graph, axis=1)\n",
    "        NX_cum = np.cumsum(result_out[6])\n",
    "\n",
    "        result_out = result_out + [graph_to_rnn, rnn_to_graph, NX_cum]\n",
    "\n",
    "        return result_out\n",
    "\n",
    "    @staticmethod\n",
    "    def from_numpy_to_tensor(record):\n",
    "        [X, A,\n",
    "         mol_ids_rep, rep_ids_rep, iw_ids,\n",
    "         last_append_mask,\n",
    "         NX, NX_rep,\n",
    "         action_0, actions,\n",
    "         log_p,\n",
    "         graph_to_rnn, rnn_to_graph, NX_cum] = record\n",
    "\n",
    "        output = MolLoader.from_numpy_to_tensor([X, A,\n",
    "                                                 mol_ids_rep, rep_ids_rep, iw_ids,\n",
    "                                                 last_append_mask,\n",
    "                                                 NX, NX_rep,\n",
    "                                                 action_0, actions,\n",
    "                                                 log_p])\n",
    "\n",
    "        graph_to_rnn, rnn_to_graph, NX_cum =\\\n",
    "            nd.array(graph_to_rnn, ctx=mx.gpu(), dtype='int64'),\\\n",
    "            nd.array(rnn_to_graph, ctx=mx.gpu(), dtype='int64'), \\\n",
    "            nd.array(NX_cum, ctx=mx.gpu(), dtype='int64')\n",
    "\n",
    "        output = output + [graph_to_rnn, rnn_to_graph, NX_cum]\n",
    "\n",
    "        return output\n",
    "    \n",
    "class CMolRNNLoader(MolRNNLoader):\n",
    "\n",
    "    def __init__(self, dataset, batch_size=10, num_workers=0,\n",
    "                 k=10, p=0.9, shuffle=False, sampler=None, batch_sampler=None,\n",
    "                 conditional=None):\n",
    "        if conditional is None:\n",
    "            raise ValueError('Conditional function is not set, '\n",
    "                             'use unconditional version instead')\n",
    "        if not callable(conditional):\n",
    "            raise TypeError('Provided condition is not callable')\n",
    "\n",
    "        self.conditional = conditional\n",
    "\n",
    "        super(CMolRNNLoader, self).__init__(dataset, batch_size, num_workers,\n",
    "                                            k, p, shuffle, sampler, batch_sampler)\n",
    "\n",
    "    def _collate_fn(self, batch):\n",
    "        smiles_list, c = [], []\n",
    "        for record_i in batch:\n",
    "            smiles_i, c_i = self.conditional(record_i)\n",
    "            smiles_list.append(smiles_i)\n",
    "            c.append(c_i)\n",
    "        c = np.stack(c, axis=0)\n",
    "\n",
    "        output = super(CMolRNNLoader, self)._collate_fn(smiles_list)\n",
    "        output.append(c)\n",
    "\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def from_numpy_to_tensor(record):\n",
    "        [X, A,\n",
    "         mol_ids_rep, rep_ids_rep, iw_ids,\n",
    "         last_append_mask,\n",
    "         NX, NX_rep,\n",
    "         action_0, actions,\n",
    "         log_p,\n",
    "         graph_to_rnn, rnn_to_graph, NX_cum,\n",
    "         c] = record\n",
    "\n",
    "        output = MolRNNLoader.from_numpy_to_tensor([X, A,\n",
    "                                                    mol_ids_rep, rep_ids_rep, iw_ids,\n",
    "                                                    last_append_mask,\n",
    "                                                    NX, NX_rep,\n",
    "                                                    action_0, actions,\n",
    "                                                    log_p,\n",
    "                                                    graph_to_rnn, rnn_to_graph, NX_cum])\n",
    "        ids = nd.array(mol_ids_rep, ctx=mx.gpu(), dtype='int64')\n",
    "\n",
    "        c = nd.array(c, ctx=mx.gpu(), dtype='float32')\n",
    "\n",
    "        output = output + [c, ids]\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining graph convolution and other functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "executionInfo": {
     "elapsed": 2407,
     "status": "ok",
     "timestamp": 1613396170945,
     "user": {
      "displayName": "Shikha Mallick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjoRyV4TB5l3kSyBeE28ldIN2S1glHF3FTZvVEf_Q=s64",
      "userId": "13917264246341255180"
     },
     "user_tz": -330
    },
    "id": "L53rlixAICeU"
   },
   "outputs": [],
   "source": [
    "class GraphConvFn(Function):\n",
    "\n",
    "    def __init__(self, A):\n",
    "        self.A = A # type: nd.sparse.CSRNDArray\n",
    "        self.A_T = self.A # assume symmetric\n",
    "        super(GraphConvFn, self).__init__()\n",
    "\n",
    "    def forward(self, X):\n",
    "        if self.A is not None:\n",
    "            if len(X.shape) > 2:\n",
    "                X_resized = X.reshape((X.shape[0], -1))\n",
    "                output = nd.sparse.dot(self.A, X_resized)\n",
    "                output = output.reshape([-1, ] + [X.shape[i] for i in range(1, len(X.shape))])\n",
    "            else:\n",
    "                output = nd.sparse.dot(self.A, X)\n",
    "            return output\n",
    "        else:\n",
    "            return nd.zeros_like(X)\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "\n",
    "        if self.A is not None:\n",
    "            if len(grad_output.shape) > 2:\n",
    "                grad_output_resized = grad_output.reshape((grad_output.shape[0], -1))\n",
    "                grad_input = nd.sparse.dot(self.A_T, grad_output_resized)\n",
    "                grad_input = grad_input.reshape([-1] + [grad_output.shape[i]\n",
    "                                                        for i in range(1, len(grad_output.shape))])\n",
    "            else:\n",
    "                grad_input = nd.sparse.dot(self.A_T, grad_output)\n",
    "            return grad_input\n",
    "        else:\n",
    "            return nd.zeros_like(grad_output)\n",
    "\n",
    "\n",
    "class EfficientGraphConvFn(Function):\n",
    "    \"\"\"Save memory by re-computation\"\"\"\n",
    "\n",
    "    def __init__(self, A_list):\n",
    "        self.A_list = A_list\n",
    "        super(EfficientGraphConvFn, self).__init__()\n",
    "\n",
    "    def forward(self, X, W):\n",
    "        X_list = [X]\n",
    "        for A in self.A_list:\n",
    "            if A is not None:\n",
    "                X_list.append(nd.sparse.dot(A, X))\n",
    "            else:\n",
    "                X_list.append(nd.zeros_like(X))\n",
    "        X_out = nd.concat(*X_list, dim=1)\n",
    "        self.save_for_backward(X, W)\n",
    "\n",
    "        return nd.dot(X_out, W)\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        X, W = self.saved_tensors\n",
    "\n",
    "        # recompute X_out\n",
    "        X_list = [X, ]\n",
    "        for A in self.A_list:\n",
    "            if A is not None:\n",
    "                X_list.append(nd.sparse.dot(A, X))\n",
    "            else:\n",
    "                X_list.append(nd.zeros_like(X))\n",
    "        X_out = nd.concat(*X_list, dim=1)\n",
    "\n",
    "        grad_W = nd.dot(X_out.T, grad_output)\n",
    "\n",
    "        grad_X_out = nd.dot(grad_output, W.T)\n",
    "        grad_X_out_list = nd.split(grad_X_out, num_outputs=len(self.A_list) + 1)\n",
    "\n",
    "\n",
    "        grad_X = [grad_X_out_list[0], ]\n",
    "        for A, grad_X_out in zip(self.A_list, grad_X_out_list[1:]):\n",
    "            if A is not None:\n",
    "                grad_X.append(nd.sparse.dot(A, grad_X_out))\n",
    "            else:\n",
    "                grad_X.append(nd.zeros_like(grad_X_out))\n",
    "\n",
    "        grad_X = sum(grad_X)\n",
    "\n",
    "        return grad_X, grad_W\n",
    "\n",
    "\n",
    "class SegmentSumFn(GraphConvFn):\n",
    "\n",
    "    def __init__(self, idx, num_seg):\n",
    "        # build A\n",
    "        # construct coo\n",
    "        data = nd.ones(idx.shape[0], ctx=idx.context, dtype='int64')\n",
    "        row, col = idx, nd.arange(idx.shape[0], ctx=idx.context, dtype='int64')\n",
    "        shape = (num_seg, int(idx.shape[0]))\n",
    "        sparse = nd.sparse.csr_matrix((data, (row, col)), shape=shape,\n",
    "                                      ctx=idx.context, dtype='float32')\n",
    "        super(SegmentSumFn, self).__init__(sparse)\n",
    "\n",
    "        sparse = nd.sparse.csr_matrix((data, (col, row)), shape=(shape[1], shape[0]),\n",
    "                                      ctx=idx.context, dtype='float32')\n",
    "        self.A_T = sparse\n",
    "\n",
    "\n",
    "def squeeze(input, axis):\n",
    "    assert input.shape[axis] == 1\n",
    "\n",
    "    new_shape = list(input.shape)\n",
    "    del new_shape[axis]\n",
    "\n",
    "    return input.reshape(new_shape)\n",
    "\n",
    "\n",
    "def unsqueeze(input, axis):\n",
    "    return nd.expand_dims(input, axis=axis)\n",
    "\n",
    "\n",
    "def logsumexp(inputs, axis=None, keepdims=False):\n",
    "    \"\"\"Numerically stable logsumexp.\n",
    "    Args:\n",
    "        inputs: A Variable with any shape.\n",
    "        axis: An integer.\n",
    "        keepdims: A boolean.\n",
    "    Returns:\n",
    "        Equivalent of log(sum(exp(inputs), dim=dim, keepdim=keepdim)).\n",
    "    Adopted from: https://github.com/pytorch/pytorch/issues/2591\n",
    "    \"\"\"\n",
    "    # For a 1-D array x (any array along a single dimension),\n",
    "    # log sum exp(x) = s + log sum exp(x - s)\n",
    "    # with s = max(x) being a common choice.\n",
    "    if axis is None:\n",
    "        inputs = inputs.reshape([-1])\n",
    "        axis = 0\n",
    "    s = nd.max(inputs, axis=axis, keepdims=True)\n",
    "    outputs = s + (inputs - s).exp().sum(axis=axis, keepdims=True).log()\n",
    "    if not keepdims:\n",
    "        outputs = nd.sum(outputs, axis=axis, keepdims=False)\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def get_activation(name):\n",
    "    activation_dict = {\n",
    "        'relu':nd.relu,\n",
    "        'tanh':nd.tanh\n",
    "    }\n",
    "    return activation_dict[name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "executionInfo": {
     "elapsed": 2359,
     "status": "ok",
     "timestamp": 1613396170398,
     "user": {
      "displayName": "Shikha Mallick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjoRyV4TB5l3kSyBeE28ldIN2S1glHF3FTZvVEf_Q=s64",
      "userId": "13917264246341255180"
     },
     "user_tz": -330
    },
    "id": "L6W61n2LKi8S"
   },
   "outputs": [],
   "source": [
    "class Linear_BN(nn.Sequential):\n",
    "    def __init__(self, F_in, F_out):\n",
    "        super(Linear_BN, self).__init__()\n",
    "        self.add(nn.Dense(F_out, in_units=F_in, use_bias=False))\n",
    "        self.add(BatchNorm(in_channels=F_out))\n",
    "\n",
    "\n",
    "class GraphConv(nn.Block):\n",
    "\n",
    "    def __init__(self, Fin, Fout, D):\n",
    "        super(GraphConv, self).__init__()\n",
    "\n",
    "        # model settings\n",
    "        self.Fin = Fin\n",
    "        self.Fout = Fout\n",
    "        self.D = D\n",
    "\n",
    "        # model parameters\n",
    "        self.W = self.params.get('w', shape=(self.Fin * (self.D + 1), self.Fout),\n",
    "                                 init=None, allow_deferred_init=False)\n",
    "\n",
    "    def forward(self, X, A_list):\n",
    "        try:\n",
    "            assert len(A_list) == self.D\n",
    "        except AssertionError as e:\n",
    "            print(self.D, len(A_list))\n",
    "            raise e\n",
    "        return EfficientGraphConvFn(A_list)(X, self.W.data(X.context))\n",
    "\n",
    "\n",
    "class Policy(nn.Block):\n",
    "\n",
    "    def __init__(self, F_in, F_h, N_A, N_B, k=1):\n",
    "        super(Policy, self).__init__()\n",
    "        self.F_in = F_in # number of input features for each atom\n",
    "        self.F_h = F_h # number of context variables\n",
    "        self.N_A = N_A # number of atom types\n",
    "        self.N_B = N_B # number of bond types\n",
    "        self.k = k # number of softmax used in the mixture\n",
    "\n",
    "\n",
    "        with self.name_scope():\n",
    "            self.linear_h = Linear_BN(F_in * 2, self.F_h * k)\n",
    "            self.linear_h_t = Linear_BN(F_in, self.F_h * k)\n",
    "\n",
    "            self.linear_x = nn.Dense(self.N_B + self.N_B*self.N_A, in_units=self.F_h)\n",
    "            self.linear_x_t = nn.Dense(1, in_units=self.F_h)\n",
    "\n",
    "            if self.k > 1:\n",
    "                self.linear_pi = nn.Dense(self.k, in_units=self.F_in)\n",
    "            else:\n",
    "                self.linear_pi = None\n",
    "\n",
    "    def forward(self, X, NX, NX_rep, X_end=None):\n",
    "        # segment mean for X\n",
    "        if X_end is None:\n",
    "            X_end = SegmentSumFn(NX_rep, NX.shape[0])(X)/nd.cast(fn.unsqueeze(NX, 1), 'float32')\n",
    "        X = nd.concat(X, X_end[NX_rep, :], dim=1)\n",
    "\n",
    "        X_h = nd.relu(self.linear_h(X)).reshape([-1, self.F_h])\n",
    "        X_h_end = nd.relu(self.linear_h_t(X_end)).reshape([-1, self.F_h])\n",
    "\n",
    "        X_x = nd.exp(self.linear_x(X_h)).reshape([-1, self.k, self.N_B + self.N_B*self.N_A])\n",
    "        X_x_end = nd.exp(self.linear_x_t(X_h_end)).reshape([-1, self.k, 1])\n",
    "\n",
    "        X_sum = nd.sum(SegmentSumFn(NX_rep, NX.shape[0])(X_x), -1, keepdims=True) + X_x_end\n",
    "        X_sum_gathered = X_sum[NX_rep, :, :]\n",
    "\n",
    "        X_softmax = X_x / X_sum_gathered\n",
    "        X_softmax_end = X_x_end/ X_sum\n",
    "\n",
    "        if self.k > 1:\n",
    "            pi = unsqueeze(nd.softmax(self.linear_pi(X_end), axis=1), -1)\n",
    "            pi_gathered = pi[NX_rep, :, :]\n",
    "\n",
    "            X_softmax = nd.sum(X_softmax * pi_gathered, axis=1)\n",
    "            X_softmax_end = nd.sum(X_softmax_end * pi, axis=1)\n",
    "        else:\n",
    "            X_softmax = squeeze(X_softmax, 1)\n",
    "            X_softmax_end = squeeze(X_softmax_end, 1)\n",
    "\n",
    "        # generate output probabilities\n",
    "        connect, append = X_softmax[:, :self.N_B], X_softmax[:, self.N_B:]\n",
    "        append = append.reshape([-1, self.N_A, self.N_B])\n",
    "        end = squeeze(X_softmax_end, -1)\n",
    "\n",
    "        return append, connect, end\n",
    "\n",
    "\n",
    "class BatchNorm(nn.Block):\n",
    "\n",
    "    def __init__(self, in_channels, momentum=0.9, eps=1e-5):\n",
    "        super(BatchNorm, self).__init__()\n",
    "        self.F = in_channels\n",
    "\n",
    "        self.bn_weight = self.params.get('bn_weight', shape=(self.F,), init=mx.init.One(),\n",
    "                                         allow_deferred_init=False)\n",
    "        self.bn_bias = self.params.get('bn_bias', shape=(self.F,), init=mx.init.Zero(),\n",
    "                                       allow_deferred_init=False)\n",
    "\n",
    "        self.running_mean = self.params.get('running_mean', grad_req='null',\n",
    "                                            shape=(self.F,),\n",
    "                                            init=mx.init.Zero(),\n",
    "                                            allow_deferred_init=False,\n",
    "                                            differentiable=False)\n",
    "        self.running_var = self.params.get('running_var', grad_req='null',\n",
    "                                           shape=(self.F,),\n",
    "                                           init=mx.init.One(),\n",
    "                                           allow_deferred_init=False,\n",
    "                                           differentiable=False)\n",
    "        self.momentum = momentum\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        if autograd.is_training():\n",
    "            return nd.BatchNorm(x,\n",
    "                                gamma=self.bn_weight.data(x.context),\n",
    "                                beta=self.bn_bias.data(x.context),\n",
    "                                moving_mean=self.running_mean.data(x.context),\n",
    "                                moving_var=self.running_var.data(x.context),\n",
    "                                eps=self.eps, momentum=self.momentum,\n",
    "                                use_global_stats=False)\n",
    "        else:\n",
    "            return nd.BatchNorm(x,\n",
    "                                gamma=self.bn_weight.data(x.context),\n",
    "                                beta=self.bn_bias.data(x.context),\n",
    "                                moving_mean=self.running_mean.data(x.context),\n",
    "                                moving_var=self.running_var.data(x.context),\n",
    "                                eps=self.eps, momentum=self.momentum,\n",
    "                                use_global_stats=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "executionInfo": {
     "elapsed": 1789,
     "status": "ok",
     "timestamp": 1613396170946,
     "user": {
      "displayName": "Shikha Mallick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjoRyV4TB5l3kSyBeE28ldIN2S1glHF3FTZvVEf_Q=s64",
      "userId": "13917264246341255180"
     },
     "user_tz": -330
    },
    "id": "ZkogfBJ8BxQF"
   },
   "source": [
    "# Building generative network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoleculeGenerator(nn.Block):\n",
    "\n",
    "    __metaclass__ = ABCMeta\n",
    "\n",
    "    def __init__(self, N_A, N_B, D, F_e, F_skip, F_c, Fh_policy, activation,\n",
    "                 *args, **kwargs):\n",
    "        super(MoleculeGenerator, self).__init__()\n",
    "        self.N_A = N_A\n",
    "        self.N_B = N_B\n",
    "        self.D = D\n",
    "        self.F_e = F_e\n",
    "        self.F_skip = F_skip\n",
    "        self.F_c = list(F_c) if isinstance(F_c, tuple) else F_c\n",
    "        self.Fh_policy = Fh_policy\n",
    "        self.activation = get_activation(activation)\n",
    "\n",
    "        with self.name_scope():\n",
    "            # embeddings\n",
    "            self.embedding_atom = nn.Embedding(self.N_A, self.F_e)\n",
    "            self.embedding_mask = nn.Embedding(3, self.F_e)\n",
    "\n",
    "            # graph conv\n",
    "            self._build_graph_conv(*args, **kwargs)\n",
    "\n",
    "            # fully connected\n",
    "            self.dense = nn.Sequential()\n",
    "            for i, (f_in, f_out) in enumerate(zip([self.F_skip, ] + self.F_c[:-1], self.F_c)):\n",
    "                self.dense.add(Linear_BN(f_in, f_out))\n",
    "\n",
    "            # policy\n",
    "            self.policy_0 = self.params.get('policy_0', shape=[self.N_A, ],\n",
    "                                            init=mx.init.Zero(),\n",
    "                                            allow_deferred_init=False)\n",
    "            self.policy_h = Policy(self.F_c[-1], self.Fh_policy, self.N_A, self.N_B)\n",
    "\n",
    "        self.mode = 'loss'\n",
    "\n",
    "    @abstractmethod\n",
    "    def _build_graph_conv(self, *args, **kwargs):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def _graph_conv_forward(self, X, A):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _policy_0(self, ctx):\n",
    "        policy_0 = nd.exp(self.policy_0.data(ctx))\n",
    "        policy_0 = policy_0/policy_0.sum()\n",
    "        return policy_0\n",
    "\n",
    "    def _policy(self, X, A, NX, NX_rep, last_append_mask):\n",
    "        # get initial embedding\n",
    "        X = self.embedding_atom(X) + self.embedding_mask(last_append_mask)\n",
    "\n",
    "        # convolution\n",
    "        X = self._graph_conv_forward(X, A)\n",
    "\n",
    "        # linear\n",
    "        X = self.dense(X)\n",
    "\n",
    "        # policy\n",
    "        append, connect, end = self.policy_h(X, NX, NX_rep)\n",
    "\n",
    "        return append, connect, end\n",
    "\n",
    "    def _likelihood(self, init, append, connect, end,\n",
    "                    action_0, actions, iw_ids, log_p_sigma,\n",
    "                    batch_size, iw_size):\n",
    "\n",
    "        # decompose action:\n",
    "        action_type, node_type, edge_type, append_pos, connect_pos = \\\n",
    "            actions[:, 0], actions[:, 1], actions[:, 2], actions[:, 3], actions[:, 4]\n",
    "        _log_mask = lambda _x, _mask: _mask * nd.log(_x + 1e-10) + (1- _mask) * nd.zeros_like(_x)\n",
    "\n",
    "        # init\n",
    "        init = init.reshape([batch_size * iw_size, self.N_A])\n",
    "        index = nd.stack(nd.arange(action_0.shape[0], ctx=action_0.context, dtype='int64'), action_0, axis=0)\n",
    "        loss_init = nd.log(nd.gather_nd(init, index) + 1e-10)\n",
    "\n",
    "        # end\n",
    "        loss_end = _log_mask(end, nd.cast(action_type == 2, 'float32'))\n",
    "\n",
    "        # append\n",
    "        index = nd.stack(append_pos, node_type, edge_type, axis=0)\n",
    "        loss_append = _log_mask(nd.gather_nd(append, index), nd.cast(action_type == 0, 'float32'))\n",
    "\n",
    "        # connect\n",
    "        index = nd.stack(connect_pos, edge_type, axis=0)\n",
    "        loss_connect = _log_mask(nd.gather_nd(connect, index), nd.cast(action_type == 1, 'float32'))\n",
    "\n",
    "        # sum up results\n",
    "        log_p_x = loss_end + loss_append + loss_connect\n",
    "        log_p_x = squeeze(SegmentSumFn(iw_ids, batch_size*iw_size)(unsqueeze(log_p_x, -1)), -1)\n",
    "        log_p_x = log_p_x + loss_init\n",
    "\n",
    "        # reshape\n",
    "        log_p_x = log_p_x.reshape([batch_size, iw_size])\n",
    "        log_p_sigma = log_p_sigma.reshape([batch_size, iw_size])\n",
    "        l = log_p_x - log_p_sigma\n",
    "        l = logsumexp(l, axis=1) - math.log(float(iw_size))\n",
    "        return l\n",
    "\n",
    "    def forward(self, *input):\n",
    "        if self.mode=='loss' or self.mode=='likelihood':\n",
    "            X, A, iw_ids, last_append_mask, \\\n",
    "            NX, NX_rep, action_0, actions, log_p, \\\n",
    "            batch_size, iw_size = input\n",
    "\n",
    "            init = self._policy_0(X.context).tile([batch_size * iw_size, 1])\n",
    "            append, connect, end = self._policy(X, A, NX, NX_rep, last_append_mask)\n",
    "            l = self._likelihood(init, append, connect, end, action_0, actions, iw_ids, log_p, batch_size, iw_size)\n",
    "            if self.mode=='likelihood':\n",
    "                return l\n",
    "            else:\n",
    "                return -l.mean()\n",
    "        elif self.mode == 'decode_0':\n",
    "            return self._policy_0(input[0])\n",
    "        elif self.mode == 'decode_step':\n",
    "            X, A, NX, NX_rep, last_append_mask = input\n",
    "            return self._policy(X, A, NX, NX_rep, last_append_mask)\n",
    "\n",
    "\n",
    "class MoleculeGenerator_RNN(MoleculeGenerator):\n",
    "\n",
    "    __metaclass__ = ABCMeta\n",
    "\n",
    "    def __init__(self, N_A, N_B, D, F_e, F_skip, F_c, Fh_policy, activation,\n",
    "                 N_rnn, *args, **kwargs):\n",
    "        super(MoleculeGenerator_RNN, self).__init__(N_A, N_B, D, F_e, F_skip, F_c, Fh_policy, activation,\n",
    "                                                    *args, **kwargs)\n",
    "        self.N_rnn = N_rnn\n",
    "\n",
    "        with self.name_scope():\n",
    "            self.rnn = gluon.rnn.GRU(hidden_size=self.F_c[-1],\n",
    "                                     num_layers=self.N_rnn,\n",
    "                                     layout='NTC', input_size=self.F_c[-1] * 2)\n",
    "\n",
    "    def _rnn_train(self, X, NX, NX_rep, graph_to_rnn, rnn_to_graph, NX_cum):\n",
    "        X_avg = SegmentSumFn(NX_rep, NX.shape[0])(X) / nd.cast(unsqueeze(NX, 1), 'float32')\n",
    "        X_curr = nd.take(X, indices=NX_cum-1)\n",
    "        X = nd.concat(X_avg, X_curr, dim=1)\n",
    "\n",
    "        # rnn\n",
    "        X = nd.take(X, indices=graph_to_rnn) # batch_size, iw_size, length, num_features\n",
    "        batch_size, iw_size, length, num_features = X.shape\n",
    "        X = X.reshape([batch_size*iw_size, length, num_features])\n",
    "        X = self.rnn(X)\n",
    "\n",
    "        X = X.reshape([batch_size, iw_size, length, -1])\n",
    "        X = nd.gather_nd(X, indices=rnn_to_graph)\n",
    "\n",
    "        return X\n",
    "\n",
    "    def _rnn_test(self, X, NX, NX_rep, NX_cum, h):\n",
    "        # note: one partition for one molecule\n",
    "        X_avg = SegmentSumFn(NX_rep, NX.shape[0])(X) / nd.cast(unsqueeze(NX, 1), 'float32')\n",
    "        X_curr = nd.take(X, indices=NX_cum - 1)\n",
    "        X = nd.concat(X_avg, X_curr, dim=1) # size: [NX, F_in * 2]\n",
    "\n",
    "        # rnn\n",
    "        X = unsqueeze(X, axis=1)\n",
    "        X, h = self.rnn(X, h)\n",
    "\n",
    "        X = squeeze(X, axis=1)\n",
    "        return X, h\n",
    "\n",
    "    def _policy(self, X, A, NX, NX_rep, last_append_mask, graph_to_rnn, rnn_to_graph, NX_cum):\n",
    "        # get initial embedding\n",
    "        X = self.embedding_atom(X) + self.embedding_mask(last_append_mask)\n",
    "\n",
    "        # convolution\n",
    "        X = self._graph_conv_forward(X, A)\n",
    "\n",
    "        # linear\n",
    "        X = self.dense(X)\n",
    "\n",
    "        # rnn\n",
    "        X_mol = self._rnn_train(X, NX, NX_rep, graph_to_rnn, rnn_to_graph, NX_cum)\n",
    "\n",
    "        # policy\n",
    "        append, connect, end = self.policy_h(X, NX, NX_rep, X_mol)\n",
    "\n",
    "        return append, connect, end\n",
    "\n",
    "    def _decode_step(self, X, A, NX, NX_rep, last_append_mask, NX_cum, h):\n",
    "        # get initial embedding\n",
    "        X = self.embedding_atom(X) + self.embedding_mask(last_append_mask)\n",
    "\n",
    "        # convolution\n",
    "        X = self._graph_conv_forward(X, A)\n",
    "\n",
    "        # linear\n",
    "        X = self.dense(X)\n",
    "\n",
    "        # rnn\n",
    "        X_mol, h = self._rnn_test(X, NX, NX_rep, NX_cum, h)\n",
    "\n",
    "        # policy\n",
    "        append, connect, end = self.policy_h(X, NX, NX_rep, X_mol)\n",
    "\n",
    "        return append, connect, end, h\n",
    "\n",
    "    def forward(self, *input):\n",
    "        if self.mode=='loss' or self.mode=='likelihood':\n",
    "            X, A, iw_ids, last_append_mask, \\\n",
    "            NX, NX_rep, action_0, actions, log_p, \\\n",
    "            batch_size, iw_size, \\\n",
    "            graph_to_rnn, rnn_to_graph, NX_cum = input\n",
    "\n",
    "            init = self._policy_0(X.context).tile([batch_size * iw_size, 1])\n",
    "            append, connect, end = self._policy(X, A, NX, NX_rep, last_append_mask, graph_to_rnn, rnn_to_graph, NX_cum)\n",
    "            l = self._likelihood(init, append, connect, end, action_0, actions, iw_ids, log_p, batch_size, iw_size)\n",
    "            if self.mode=='likelihood':\n",
    "                return l\n",
    "            else:\n",
    "                return -l.mean()\n",
    "        elif self.mode == 'decode_0':\n",
    "            return self._policy_0(input[0])\n",
    "        elif self.mode == 'decode_step':\n",
    "            X, A, NX, NX_rep, last_append_mask, NX_cum, h = input\n",
    "            return self._decode_step(X, A, NX, NX_rep, last_append_mask, NX_cum, h)\n",
    "        else:\n",
    "            raise ValueError\n",
    "            \n",
    "class _TwoLayerDense(nn.Block):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(_TwoLayerDense, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.input_size = input_size\n",
    "\n",
    "        with self.name_scope():\n",
    "            # config 1\n",
    "            self.input = nn.Dense(self.hidden_size, use_bias=False, in_units=self.input_size)\n",
    "            self.bn_input = BatchNorm(in_channels=self.hidden_size)\n",
    "            self.output = nn.Dense(self.output_size, use_bias=True, in_units=self.hidden_size)\n",
    "            \n",
    "            # config 2\n",
    "            #self.bn_input = BatchNorm(in_channels=self.input_size)\n",
    "            #self.output = nn.Dense(self.output_size, use_bias=True, in_units=self.input_size)\n",
    "            \n",
    "            # config 3\n",
    "            #self.input1 = nn.Dense(self.hidden_size, use_bias=False, in_units=self.input_size)\n",
    "            #self.bn_input1 = BatchNorm(in_channels=self.hidden_size)\n",
    "            #self.input2 = nn.Dense(self.hidden_size, use_bias=False, in_units=self.hidden_size)\n",
    "            #self.bn_input2 = BatchNorm(in_channels=self.hidden_size)\n",
    "            #self.output = nn.Dense(self.output_size, use_bias=True, in_units=self.hidden_size)\n",
    "\n",
    "    def forward(self, c):\n",
    "        # config 1\n",
    "        return nd.softmax(self.output(nd.relu(self.bn_input(self.input(c)))), axis=-1)\n",
    "        \n",
    "        # config 2\n",
    "        #return nd.softmax(self.output(nd.relu(self.bn_input(c))), axis=-1)\n",
    "    \n",
    "        # config 3\n",
    "        #return nd.softmax(self.output(nd.relu(self.bn_input2(self.input2(nd.relu(self.bn_input1(self.input1(c))))))), axis=-1)\n",
    "\n",
    "\n",
    "class CMoleculeGenerator_RNN(MoleculeGenerator_RNN):\n",
    "    __metaclass__ = ABCMeta\n",
    "\n",
    "    def __init__(self, N_A, N_B, N_C, D,\n",
    "                 F_e, F_skip, F_c, Fh_policy,\n",
    "                 activation, N_rnn,\n",
    "                 *args, **kwargs):\n",
    "        self.N_C = N_C # number of conditional variables\n",
    "        super(CMoleculeGenerator_RNN, self).__init__(N_A, N_B, D,\n",
    "                                                     F_e, F_skip, F_c, Fh_policy,\n",
    "                                                     activation, N_rnn,\n",
    "                                                     *args, **kwargs)\n",
    "        with self.name_scope():\n",
    "            self.dense_policy_0 = _TwoLayerDense(self.N_C, self.N_A * 3, self.N_A)\n",
    "\n",
    "    @abstractmethod\n",
    "    def _graph_conv_forward(self, X, A, c, ids):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _policy_0(self, c):\n",
    "        return self.dense_policy_0(c) + 0.0 * self.policy_0.data(c.context)\n",
    "\n",
    "    def _policy(self, X, A, NX, NX_rep, last_append_mask,\n",
    "                graph_to_rnn, rnn_to_graph, NX_cum,\n",
    "                c, ids):\n",
    "        # get initial embedding\n",
    "        X = self.embedding_atom(X) + self.embedding_mask(last_append_mask)\n",
    "\n",
    "        # convolution\n",
    "        X = self._graph_conv_forward(X, A, c, ids)\n",
    "\n",
    "        # linear\n",
    "        X = self.dense(X)\n",
    "\n",
    "        # rnn\n",
    "        X_mol = self._rnn_train(X, NX, NX_rep, graph_to_rnn, rnn_to_graph, NX_cum)\n",
    "\n",
    "        # policy\n",
    "        append, connect, end = self.policy_h(X, NX, NX_rep, X_mol)\n",
    "\n",
    "        return append, connect, end\n",
    "\n",
    "    def _decode_step(self, X, A, NX, NX_rep, last_append_mask, NX_cum, h, c, ids):\n",
    "        # get initial embedding\n",
    "        X = self.embedding_atom(X) + self.embedding_mask(last_append_mask)\n",
    "\n",
    "        # convolution\n",
    "        X = self._graph_conv_forward(X, A, c, ids)\n",
    "\n",
    "        # linear\n",
    "        X = self.dense(X)\n",
    "\n",
    "        # rnn\n",
    "        X_mol, h = self._rnn_test(X, NX, NX_rep, NX_cum, h)\n",
    "\n",
    "        # policy\n",
    "        append, connect, end = self.policy_h(X, NX, NX_rep, X_mol)\n",
    "\n",
    "        return append, connect, end, h\n",
    "\n",
    "\n",
    "    def forward(self, *input):\n",
    "        if self.mode=='loss' or self.mode=='likelihood':\n",
    "            X, A, iw_ids, last_append_mask, \\\n",
    "            NX, NX_rep, action_0, actions, log_p, \\\n",
    "            batch_size, iw_size, \\\n",
    "            graph_to_rnn, rnn_to_graph, NX_cum, \\\n",
    "            c, ids = input\n",
    "\n",
    "            init = nd.tile(unsqueeze(self._policy_0(c), axis=1), [1, iw_size, 1])\n",
    "            append, connect, end = self._policy(X, A, NX, NX_rep, last_append_mask,\n",
    "                                                graph_to_rnn, rnn_to_graph, NX_cum,\n",
    "                                                c, ids)\n",
    "            l = self._likelihood(init, append, connect, end,\n",
    "                                 action_0, actions, iw_ids, log_p,\n",
    "                                 batch_size, iw_size)\n",
    "            if self.mode=='likelihood':\n",
    "                return l\n",
    "            else:\n",
    "                return -l.mean()\n",
    "        elif self.mode == 'decode_0':\n",
    "            return self._policy_0(*input)\n",
    "        elif self.mode == 'decode_step':\n",
    "            X, A, NX, NX_rep, last_append_mask, NX_cum, h, c, ids = input\n",
    "            return self._decode_step(X, A, NX, NX_rep, last_append_mask, NX_cum, h, c, ids)\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "class CVanillaMolGen_RNN(CMoleculeGenerator_RNN):\n",
    "\n",
    "    def __init__(self, N_A, N_B, N_C, D,\n",
    "                 F_e, F_h, F_skip, F_c, Fh_policy,\n",
    "                 activation, N_rnn, rename=False):\n",
    "        self.rename = rename\n",
    "        super(CVanillaMolGen_RNN, self).__init__(N_A, N_B, N_C, D,\n",
    "                                                 F_e, F_skip, F_c, Fh_policy,\n",
    "                                                 activation, N_rnn,\n",
    "                                                 F_h)\n",
    "\n",
    "    def _build_graph_conv(self, F_h):\n",
    "        self.F_h = list(F_h) if isinstance(F_h, tuple) else F_h\n",
    "        self.conv, self.bn = [], []\n",
    "        for i, (f_in, f_out) in enumerate(zip([self.F_e] + self.F_h[:-1], self.F_h)):\n",
    "            conv = GraphConv(f_in, f_out, self.N_B + self.D)\n",
    "            self.conv.append(conv)\n",
    "            self.register_child(conv)\n",
    "\n",
    "            if i != 0:\n",
    "                bn = BatchNorm(in_channels=f_in)\n",
    "                self.register_child(bn)\n",
    "            else:\n",
    "                bn = None\n",
    "            self.bn.append(bn)\n",
    "\n",
    "        self.bn_skip = BatchNorm(in_channels=sum(self.F_h))\n",
    "        self.linear_skip = Linear_BN(sum(self.F_h), self.F_skip)\n",
    "\n",
    "        # projectors for conditional variable (protein embedding)\n",
    "        self.linear_c = []\n",
    "        for i, f_out in enumerate(self.F_h):\n",
    "            if self.rename:\n",
    "                linear_c = nn.Dense(f_out, use_bias=False, in_units=self.N_C, prefix='cond_{}'.format(i))\n",
    "            else:\n",
    "                linear_c = nn.Dense(f_out, use_bias=False, in_units=self.N_C)\n",
    "            self.register_child(linear_c)\n",
    "            self.linear_c.append(linear_c)\n",
    "\n",
    "    def _graph_conv_forward(self, X, A, c, ids):\n",
    "        X_out = [X]\n",
    "        for conv, bn, linear_c in zip(self.conv, self.bn, self.linear_c):\n",
    "            X = X_out[-1]\n",
    "            if bn is not None:\n",
    "                X_out.append(conv(self.activation(bn(X)), A) + linear_c(c)[ids, :])\n",
    "            else:\n",
    "                X_out.append(conv(X, A) + linear_c(c)[ids, :])\n",
    "        X_out = nd.concat(*X_out[1:], dim=1)\n",
    "        return self.activation(self.linear_skip(self.activation(self.bn_skip(X_out))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early Stopping Implementation\n",
    "class EarlyStopping(object):\n",
    "    \n",
    "    def __init__(self, patience=5, delta=1e-2, less_is_better=True):\n",
    "\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.less_is_better = less_is_better\n",
    "        self.best = math.inf if less_is_better else -math.inf\n",
    "        self.counter = 0\n",
    "\n",
    "    def update(self, loss):\n",
    "        if self.best is None:\n",
    "            self.best = loss\n",
    "            return\n",
    "\n",
    "        if self.less_is_better:\n",
    "            # Best loss updated.\n",
    "            if loss < self.best - self.delta:\n",
    "                self.best = loss\n",
    "                self.counter = 0\n",
    "            else:\n",
    "                self.counter += 1\n",
    "        else:\n",
    "            # Best loss updated.\n",
    "            if loss > self.best + self.delta:\n",
    "                self.best = loss\n",
    "                self.counter = 0\n",
    "            else:\n",
    "                self.counter += 1\n",
    "\n",
    "    def step(self, loss):\n",
    "        self.update(loss)\n",
    "\n",
    "        # Return True if the counter exceeded patience.\n",
    "        return self.counter >= self.patience\n",
    "\n",
    "    def get_best_score(self):\n",
    "        return self.best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and preprocessing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "if all([os.path.isfile(os.path.join(ckpt_dir, _n)) for _n in ['log.out', 'ckpt.params', 'trainer.status']]):\n",
    "    is_continuous = True\n",
    "else:\n",
    "    is_continuous = False\n",
    "\n",
    "cond = Delimited()\n",
    "    \n",
    "if train_only:\n",
    "    dataset = Lambda(dataset_train, lambda _x: _x.strip('\\n').strip('\\r'))\n",
    "\n",
    "    # get sampler and loader for training set\n",
    "    sampler_train = BalancedSampler(cost=[len(l.split('\\t')[0]) for l in dataset], batch_size=batch_size)\n",
    "    loader_train = CMolRNNLoader(dataset, batch_sampler=sampler_train, num_workers=num_workers,\n",
    "                                      k=k, p=p, conditional=cond)\n",
    "\n",
    "    loader_test = []\n",
    "\n",
    "else:\n",
    "    if all([os.path.isfile(os.path.join(ckpt_dir, _n)) for _n in ['log.out', 'ckpt.params', 'trainer.status']]):\n",
    "        is_continuous = True\n",
    "    else:\n",
    "        is_continuous = False\n",
    "\n",
    "    db_train = Lambda(dataset_train, lambda _x: _x.strip('\\n').strip('\\r'))\n",
    "    # get sampler and loader for training set\n",
    "    sampler_train = BalancedSampler(cost=[len(l.split('\\t')[0]) for l in db_train], batch_size=batch_size)\n",
    "    loader_train = CMolRNNLoader(db_train, batch_sampler=sampler_train, num_workers=num_workers,\n",
    "                                      k=k, p=p, conditional=cond)\n",
    "    \n",
    "    db_test = Lambda(dataset_test, lambda _x: _x.strip('\\n').strip('\\r'))\n",
    "    # get sampler and loader for test set\n",
    "    sampler_test = BalancedSampler(cost=[len(l.split('\\t')[0]) for l in db_test], batch_size=batch_size_test)\n",
    "    loader_test = CMolRNNLoader(db_test, batch_sampler=sampler_test, num_workers=num_workers,\n",
    "                                      k=k, p=p, conditional=cond)\n",
    "\n",
    "# get iterator\n",
    "it_train, it_test = iter(loader_train), iter(loader_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construcing the model and initializing the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "z2d5DCSM5715"
   },
   "outputs": [],
   "source": [
    "# build model\n",
    "if not is_continuous:\n",
    "    configs = {'N_C': N_C,\n",
    "               'F_e': F_e,\n",
    "               'F_h': F_h,\n",
    "               'F_skip': F_skip,\n",
    "               'F_c': F_c,\n",
    "               'Fh_policy': Fh_policy,\n",
    "               'activation': activation,\n",
    "               'rename': True,\n",
    "               'N_rnn': N_rnn}\n",
    "    with open(os.path.join(ckpt_dir, 'configs.json'), 'w') as f:\n",
    "        json.dump(configs, f)\n",
    "else:\n",
    "    with open(os.path.join(ckpt_dir, 'configs.json')) as f:\n",
    "        configs = json.load(f)\n",
    "\n",
    "model = CVanillaMolGen_RNN(get_mol_spec().num_atom_types, get_mol_spec().num_bond_types, D=2, **configs)\n",
    "\n",
    "ctx = mx.gpu()\n",
    "model.collect_params().initialize(mx.init.Xavier(), force_reinit=True, ctx=ctx)\n",
    "if not is_continuous:\n",
    "    if cond_type == 'kinase':\n",
    "        model.load_parameters(os.path.join(ckpt_dir, 'ckpt.params'), ctx=ctx, allow_missing=True)\n",
    "else:\n",
    "    model.load_parameters(os.path.join(ckpt_dir, 'ckpt.params'), ctx=ctx)\n",
    "\n",
    "# construct optimizer\n",
    "opt = mx.optimizer.Adam(learning_rate=lr, clip_gradient=clip_grad)\n",
    "trainer = gluon.Trainer(model.collect_params(), opt)\n",
    "if is_continuous:\n",
    "    trainer.load_states(os.path.join(ckpt_dir, 'trainer.status'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "1wmxxeJc82lx",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss =  30.44795036315918\n",
      "200 iterations done!\n",
      "Training loss =  27.948688507080078\n",
      "400 iterations done!\n",
      "Training loss =  25.90536880493164\n",
      "600 iterations done!\n",
      "Training loss =  33.702857971191406\n",
      "800 iterations done!\n",
      "Training loss =  27.16719627380371\n",
      "1000 iterations done!\n",
      "Training loss =  28.53228759765625\n",
      "1200 iterations done!\n",
      "Training loss =  27.295475006103516\n",
      "1400 iterations done!\n",
      "Training loss =  30.342578887939453\n",
      "1600 iterations done!\n",
      "Training loss =  24.00511932373047\n",
      "1800 iterations done!\n",
      "Training loss =  27.45720100402832\n",
      "2000 iterations done!\n",
      "Training loss =  24.900548934936523\n",
      "2200 iterations done!\n",
      "Training loss =  26.579559326171875\n",
      "2400 iterations done!\n",
      "Training loss =  24.04629135131836\n",
      "2600 iterations done!\n",
      "Training loss =  28.8890323638916\n",
      "2800 iterations done!\n",
      "Training loss =  27.3372859954834\n",
      "3000 iterations done!\n",
      "Training loss =  25.975238800048828\n",
      "3200 iterations done!\n",
      "Training loss =  26.389936447143555\n",
      "3400 iterations done!\n",
      "Training loss =  28.03075408935547\n",
      "3600 iterations done!\n",
      "Training loss =  27.896154403686523\n",
      "3800 iterations done!\n",
      "Training loss =  24.135150909423828\n",
      "4000 iterations done!\n",
      "Training loss =  26.256303787231445\n",
      "4200 iterations done!\n",
      "Training loss =  28.877153396606445\n",
      "4400 iterations done!\n",
      "Training loss =  23.372634887695312\n",
      "4600 iterations done!\n",
      "Training loss =  22.519765853881836\n",
      "4800 iterations done!\n",
      "Training loss =  26.42304229736328\n",
      "5000 iterations done!\n",
      "Training loss =  25.533784866333008\n",
      "5200 iterations done!\n",
      "Training loss =  22.29503631591797\n",
      "5400 iterations done!\n",
      "Training loss =  25.47079086303711\n",
      "5600 iterations done!\n",
      "Training loss =  23.29143524169922\n",
      "5800 iterations done!\n",
      "Training loss =  30.34313201904297\n",
      "6000 iterations done!\n",
      "Training loss =  25.12381935119629\n",
      "6200 iterations done!\n",
      "Training loss =  24.663589477539062\n",
      "6400 iterations done!\n",
      "Training loss =  28.959087371826172\n",
      "6600 iterations done!\n",
      "Training loss =  28.85036277770996\n",
      "6800 iterations done!\n",
      "Training loss =  24.692384719848633\n",
      "7000 iterations done!\n",
      "Training loss =  24.345375061035156\n",
      "7200 iterations done!\n",
      "Training loss =  28.940601348876953\n",
      "7400 iterations done!\n",
      "Training loss =  26.284208297729492\n",
      "7600 iterations done!\n",
      "Training loss =  28.876264572143555\n",
      "7800 iterations done!\n",
      "Training loss =  20.05600929260254\n",
      "8000 iterations done!\n",
      "Training loss =  22.87044334411621\n",
      "8200 iterations done!\n",
      "Training loss =  29.85398292541504\n",
      "8400 iterations done!\n",
      "Training loss =  21.896081924438477\n",
      "8600 iterations done!\n",
      "Training loss =  21.67778778076172\n",
      "8800 iterations done!\n",
      "Training loss =  22.629396438598633\n",
      "9000 iterations done!\n",
      "Training loss =  24.338714599609375\n",
      "9200 iterations done!\n",
      "Training loss =  25.468095779418945\n",
      "9400 iterations done!\n",
      "Training loss =  21.079713821411133\n",
      "9600 iterations done!\n",
      "Training loss =  23.725448608398438\n",
      "9800 iterations done!\n",
      "Training loss =  36.75794982910156\n",
      "10000 iterations done!\n",
      "Training loss =  23.973583221435547\n",
      "10200 iterations done!\n",
      "Training loss =  27.26320457458496\n",
      "10400 iterations done!\n",
      "Training loss =  31.201082229614258\n",
      "10600 iterations done!\n",
      "Training loss =  29.808334350585938\n",
      "10800 iterations done!\n",
      "Training loss =  22.20343589782715\n",
      "11000 iterations done!\n",
      "Training loss =  26.254459381103516\n",
      "11200 iterations done!\n",
      "Training loss =  26.278732299804688\n",
      "11400 iterations done!\n",
      "Training loss =  24.536785125732422\n",
      "11600 iterations done!\n",
      "Training loss =  23.41236114501953\n",
      "11800 iterations done!\n",
      "Training loss =  26.869855880737305\n",
      "12000 iterations done!\n",
      "Training loss =  20.337303161621094\n",
      "12200 iterations done!\n",
      "Training loss =  26.03917121887207\n",
      "12400 iterations done!\n",
      "Training loss =  20.515457153320312\n",
      "12600 iterations done!\n",
      "Training loss =  22.533428192138672\n",
      "12800 iterations done!\n",
      "Training loss =  27.614797592163086\n",
      "13000 iterations done!\n",
      "Training loss =  25.974769592285156\n",
      "13200 iterations done!\n",
      "Training loss =  22.003276824951172\n",
      "13400 iterations done!\n",
      "Training loss =  22.063077926635742\n",
      "13600 iterations done!\n",
      "Training loss =  22.19416046142578\n",
      "13800 iterations done!\n",
      "Training loss =  29.061023712158203\n",
      "14000 iterations done!\n",
      "Training loss =  24.632423400878906\n",
      "14200 iterations done!\n",
      "Training loss =  24.66856575012207\n",
      "14400 iterations done!\n",
      "Training loss =  23.207489013671875\n",
      "14600 iterations done!\n",
      "Training loss =  28.56706428527832\n",
      "14800 iterations done!\n",
      "Training loss =  22.32059669494629\n",
      "15000 iterations done!\n",
      "Training loss =  21.91100311279297\n",
      "15200 iterations done!\n",
      "Training loss =  21.71562957763672\n",
      "15400 iterations done!\n",
      "Training loss =  21.51149559020996\n",
      "15600 iterations done!\n",
      "Training loss =  26.725099563598633\n",
      "15800 iterations done!\n",
      "Training loss =  24.655899047851562\n",
      "16000 iterations done!\n",
      "Training loss =  27.798742294311523\n",
      "16200 iterations done!\n",
      "Training loss =  24.191762924194336\n",
      "16400 iterations done!\n",
      "Training loss =  21.20008087158203\n",
      "16600 iterations done!\n",
      "Training loss =  24.774499893188477\n",
      "16800 iterations done!\n",
      "Training loss =  24.377872467041016\n",
      "17000 iterations done!\n",
      "Training loss =  30.58194923400879\n",
      "17200 iterations done!\n",
      "Training loss =  16.260541915893555\n",
      "17400 iterations done!\n",
      "Training loss =  26.53032875061035\n",
      "17600 iterations done!\n",
      "Training loss =  21.39825439453125\n",
      "17800 iterations done!\n",
      "Epoch 1 complete.\n",
      "Training loss =  23.771141052246094\n",
      "18000 iterations done!\n",
      "Training loss =  24.970827102661133\n",
      "18200 iterations done!\n",
      "Training loss =  23.928112030029297\n",
      "18400 iterations done!\n",
      "Training loss =  22.934768676757812\n",
      "18600 iterations done!\n",
      "Training loss =  32.683292388916016\n",
      "18800 iterations done!\n",
      "Training loss =  21.303752899169922\n",
      "19000 iterations done!\n",
      "Training loss =  23.94549560546875\n",
      "19200 iterations done!\n",
      "Training loss =  23.22739601135254\n",
      "19400 iterations done!\n",
      "Training loss =  24.351221084594727\n",
      "19600 iterations done!\n",
      "Training loss =  21.937755584716797\n",
      "19800 iterations done!\n",
      "Training loss =  25.77959442138672\n",
      "20000 iterations done!\n",
      "Training loss =  24.092939376831055\n",
      "20200 iterations done!\n",
      "Training loss =  24.801006317138672\n",
      "20400 iterations done!\n",
      "Training loss =  21.68482208251953\n",
      "20600 iterations done!\n",
      "Training loss =  28.1956844329834\n",
      "20800 iterations done!\n",
      "Training loss =  21.076879501342773\n",
      "21000 iterations done!\n",
      "Training loss =  21.140399932861328\n",
      "21200 iterations done!\n",
      "Training loss =  24.271190643310547\n",
      "21400 iterations done!\n",
      "Training loss =  22.437946319580078\n",
      "21600 iterations done!\n",
      "Training loss =  17.3121337890625\n",
      "21800 iterations done!\n",
      "Training loss =  23.674480438232422\n",
      "22000 iterations done!\n",
      "Training loss =  23.905149459838867\n",
      "22200 iterations done!\n",
      "Training loss =  22.683664321899414\n",
      "22400 iterations done!\n",
      "Training loss =  20.067670822143555\n",
      "22600 iterations done!\n",
      "Training loss =  23.874258041381836\n",
      "22800 iterations done!\n",
      "Training loss =  26.562545776367188\n",
      "23000 iterations done!\n",
      "Training loss =  26.79338836669922\n",
      "23200 iterations done!\n",
      "Training loss =  22.874858856201172\n",
      "23400 iterations done!\n",
      "Training loss =  22.297391891479492\n",
      "23600 iterations done!\n",
      "Training loss =  27.867321014404297\n",
      "23800 iterations done!\n",
      "Training loss =  23.144865036010742\n",
      "24000 iterations done!\n",
      "Training loss =  19.66004753112793\n",
      "24200 iterations done!\n",
      "Training loss =  20.693286895751953\n",
      "24400 iterations done!\n",
      "Training loss =  21.931886672973633\n",
      "24600 iterations done!\n",
      "Training loss =  30.06253433227539\n",
      "24800 iterations done!\n",
      "Training loss =  19.346694946289062\n",
      "25000 iterations done!\n",
      "Training loss =  26.757219314575195\n",
      "25200 iterations done!\n",
      "Training loss =  22.50600242614746\n",
      "25400 iterations done!\n",
      "Training loss =  23.511362075805664\n",
      "25600 iterations done!\n",
      "Training loss =  27.076858520507812\n",
      "25800 iterations done!\n",
      "Training loss =  25.256227493286133\n",
      "26000 iterations done!\n",
      "Training loss =  20.62872314453125\n",
      "26200 iterations done!\n",
      "Training loss =  17.16639518737793\n",
      "26400 iterations done!\n",
      "Training loss =  20.684967041015625\n",
      "26600 iterations done!\n",
      "Training loss =  19.979372024536133\n",
      "26800 iterations done!\n",
      "Training loss =  21.069581985473633\n",
      "27000 iterations done!\n",
      "Training loss =  24.382478713989258\n",
      "27200 iterations done!\n",
      "Training loss =  19.30234146118164\n",
      "27400 iterations done!\n",
      "Training loss =  25.32318878173828\n",
      "27600 iterations done!\n",
      "Training loss =  17.648466110229492\n",
      "27800 iterations done!\n",
      "Training loss =  28.29478645324707\n",
      "28000 iterations done!\n",
      "Training loss =  20.38995361328125\n",
      "28200 iterations done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss =  27.08895492553711\n",
      "28400 iterations done!\n",
      "Training loss =  21.187454223632812\n",
      "28600 iterations done!\n",
      "Training loss =  25.38812255859375\n",
      "28800 iterations done!\n",
      "Training loss =  24.150798797607422\n",
      "29000 iterations done!\n",
      "Training loss =  20.5521183013916\n",
      "29200 iterations done!\n",
      "Training loss =  22.36893081665039\n",
      "29400 iterations done!\n",
      "Training loss =  25.502161026000977\n",
      "29600 iterations done!\n",
      "Training loss =  21.547592163085938\n",
      "29800 iterations done!\n",
      "Training loss =  24.382715225219727\n",
      "30000 iterations done!\n",
      "Training loss =  21.348854064941406\n",
      "30200 iterations done!\n",
      "Training loss =  24.437294006347656\n",
      "30400 iterations done!\n",
      "Training loss =  23.199785232543945\n",
      "30600 iterations done!\n",
      "Training loss =  23.67119598388672\n",
      "30800 iterations done!\n",
      "Training loss =  18.133543014526367\n",
      "31000 iterations done!\n",
      "Training loss =  20.95908546447754\n",
      "31200 iterations done!\n",
      "Training loss =  20.315269470214844\n",
      "31400 iterations done!\n",
      "Training loss =  19.680622100830078\n",
      "31600 iterations done!\n",
      "Training loss =  23.1119441986084\n",
      "31800 iterations done!\n",
      "Training loss =  22.35801887512207\n",
      "32000 iterations done!\n",
      "Training loss =  20.483901977539062\n",
      "32200 iterations done!\n",
      "Training loss =  24.20126724243164\n",
      "32400 iterations done!\n",
      "Training loss =  20.979904174804688\n",
      "32600 iterations done!\n",
      "Training loss =  17.128158569335938\n",
      "32800 iterations done!\n",
      "Training loss =  21.794631958007812\n",
      "33000 iterations done!\n",
      "Training loss =  18.86304473876953\n",
      "33200 iterations done!\n",
      "Training loss =  20.613563537597656\n",
      "33400 iterations done!\n",
      "Training loss =  21.493581771850586\n",
      "33600 iterations done!\n",
      "Training loss =  19.474225997924805\n",
      "33800 iterations done!\n",
      "Training loss =  19.6092472076416\n",
      "34000 iterations done!\n",
      "Training loss =  22.57147979736328\n",
      "34200 iterations done!\n",
      "Training loss =  24.901695251464844\n",
      "34400 iterations done!\n",
      "Training loss =  20.475624084472656\n",
      "34600 iterations done!\n",
      "Training loss =  22.010982513427734\n",
      "34800 iterations done!\n",
      "Training loss =  24.07413673400879\n",
      "35000 iterations done!\n",
      "Training loss =  25.57243537902832\n",
      "35200 iterations done!\n",
      "Training loss =  20.48052406311035\n",
      "35400 iterations done!\n",
      "Training loss =  24.403181076049805\n",
      "35600 iterations done!\n",
      "Training loss =  22.813650131225586\n",
      "35800 iterations done!\n",
      "Epoch 2 complete.\n",
      "Training loss =  23.671648025512695\n",
      "36000 iterations done!\n",
      "Training loss =  25.350812911987305\n",
      "36200 iterations done!\n",
      "Training loss =  30.359464645385742\n",
      "36400 iterations done!\n",
      "Training loss =  24.568132400512695\n",
      "36600 iterations done!\n",
      "Training loss =  16.841215133666992\n",
      "36800 iterations done!\n",
      "Training loss =  18.31858253479004\n",
      "37000 iterations done!\n",
      "Training loss =  20.470867156982422\n",
      "37200 iterations done!\n",
      "Training loss =  22.20148468017578\n",
      "37400 iterations done!\n",
      "Training loss =  17.516738891601562\n",
      "37600 iterations done!\n",
      "Training loss =  21.41982650756836\n",
      "37800 iterations done!\n",
      "Training loss =  21.474586486816406\n",
      "38000 iterations done!\n",
      "Training loss =  21.364093780517578\n",
      "38200 iterations done!\n",
      "Training loss =  21.949432373046875\n",
      "38400 iterations done!\n",
      "Training loss =  24.099411010742188\n",
      "38600 iterations done!\n",
      "Training loss =  19.30516242980957\n",
      "38800 iterations done!\n",
      "Training loss =  25.08566665649414\n",
      "39000 iterations done!\n",
      "Training loss =  20.93209457397461\n",
      "39200 iterations done!\n",
      "Training loss =  21.476057052612305\n",
      "39400 iterations done!\n",
      "Training loss =  19.42569923400879\n",
      "39600 iterations done!\n",
      "Training loss =  21.442291259765625\n",
      "39800 iterations done!\n",
      "Training loss =  24.79973793029785\n",
      "40000 iterations done!\n",
      "Training loss =  28.374927520751953\n",
      "40200 iterations done!\n",
      "Training loss =  20.689279556274414\n",
      "40400 iterations done!\n",
      "Training loss =  18.83304786682129\n",
      "40600 iterations done!\n",
      "Training loss =  21.76898956298828\n",
      "40800 iterations done!\n",
      "Training loss =  28.683246612548828\n",
      "41000 iterations done!\n",
      "Training loss =  23.049558639526367\n",
      "41200 iterations done!\n",
      "Training loss =  22.243961334228516\n",
      "41400 iterations done!\n",
      "Training loss =  17.65028190612793\n",
      "41600 iterations done!\n",
      "Training loss =  23.524438858032227\n",
      "41800 iterations done!\n",
      "Training loss =  22.946712493896484\n",
      "42000 iterations done!\n",
      "Training loss =  15.409846305847168\n",
      "42200 iterations done!\n",
      "Training loss =  24.578567504882812\n",
      "42400 iterations done!\n",
      "Training loss =  16.109235763549805\n",
      "42600 iterations done!\n",
      "Training loss =  23.19533348083496\n",
      "42800 iterations done!\n",
      "Training loss =  19.54787826538086\n",
      "43000 iterations done!\n",
      "Training loss =  20.28156852722168\n",
      "43200 iterations done!\n",
      "Training loss =  20.46742057800293\n",
      "43400 iterations done!\n",
      "Training loss =  23.74976348876953\n",
      "43600 iterations done!\n",
      "Training loss =  23.932052612304688\n",
      "43800 iterations done!\n",
      "Training loss =  27.120960235595703\n",
      "44000 iterations done!\n",
      "Training loss =  21.221630096435547\n",
      "44200 iterations done!\n",
      "Training loss =  21.84674835205078\n",
      "44400 iterations done!\n",
      "Training loss =  23.65363121032715\n",
      "44600 iterations done!\n",
      "Training loss =  25.9222354888916\n",
      "44800 iterations done!\n",
      "Training loss =  18.696413040161133\n",
      "45000 iterations done!\n",
      "Training loss =  28.45592498779297\n",
      "45200 iterations done!\n",
      "Training loss =  24.48686408996582\n",
      "45400 iterations done!\n",
      "Training loss =  18.23562240600586\n",
      "45600 iterations done!\n",
      "Training loss =  24.006332397460938\n",
      "45800 iterations done!\n",
      "Training loss =  25.02344512939453\n",
      "46000 iterations done!\n",
      "Training loss =  25.019760131835938\n",
      "46200 iterations done!\n",
      "Training loss =  19.131237030029297\n",
      "46400 iterations done!\n",
      "Training loss =  16.507831573486328\n",
      "46600 iterations done!\n",
      "Training loss =  26.269031524658203\n",
      "46800 iterations done!\n",
      "Training loss =  22.01458740234375\n",
      "47000 iterations done!\n",
      "Training loss =  20.76650047302246\n",
      "47200 iterations done!\n",
      "Training loss =  19.2950496673584\n",
      "47400 iterations done!\n",
      "Training loss =  25.00428581237793\n",
      "47600 iterations done!\n",
      "Training loss =  24.135013580322266\n",
      "47800 iterations done!\n",
      "Training loss =  19.949962615966797\n",
      "48000 iterations done!\n",
      "Training loss =  23.055879592895508\n",
      "48200 iterations done!\n",
      "Training loss =  18.926944732666016\n",
      "48400 iterations done!\n",
      "Training loss =  22.490751266479492\n",
      "48600 iterations done!\n",
      "Training loss =  20.15962791442871\n",
      "48800 iterations done!\n",
      "Training loss =  20.39676284790039\n",
      "49000 iterations done!\n",
      "Training loss =  16.17078971862793\n",
      "49200 iterations done!\n",
      "Training loss =  18.41087532043457\n",
      "49400 iterations done!\n",
      "Training loss =  13.907051086425781\n",
      "49600 iterations done!\n",
      "Training loss =  23.385217666625977\n",
      "49800 iterations done!\n",
      "Training loss =  21.168136596679688\n",
      "50000 iterations done!\n",
      "Training loss =  17.06437110900879\n",
      "50200 iterations done!\n",
      "Training loss =  19.079204559326172\n",
      "50400 iterations done!\n",
      "Training loss =  23.1540584564209\n",
      "50600 iterations done!\n",
      "Training loss =  31.5367431640625\n",
      "50800 iterations done!\n",
      "Training loss =  20.289316177368164\n",
      "51000 iterations done!\n",
      "Training loss =  18.424222946166992\n",
      "51200 iterations done!\n",
      "Training loss =  27.058074951171875\n",
      "51400 iterations done!\n",
      "Training loss =  23.47585678100586\n",
      "51600 iterations done!\n",
      "Training loss =  22.047290802001953\n",
      "51800 iterations done!\n",
      "Training loss =  24.452909469604492\n",
      "52000 iterations done!\n",
      "Training loss =  20.225046157836914\n",
      "52200 iterations done!\n",
      "Training loss =  21.765745162963867\n",
      "52400 iterations done!\n",
      "Training loss =  19.93246078491211\n",
      "52600 iterations done!\n",
      "Training loss =  27.132034301757812\n",
      "52800 iterations done!\n",
      "Training loss =  27.77355194091797\n",
      "53000 iterations done!\n",
      "Training loss =  21.64777374267578\n",
      "53200 iterations done!\n",
      "Training loss =  26.379003524780273\n",
      "53400 iterations done!\n",
      "Training loss =  20.57752227783203\n",
      "53600 iterations done!\n",
      "Training loss =  25.610889434814453\n",
      "53800 iterations done!\n",
      "Epoch 3 complete.\n",
      "Training loss =  17.53510093688965\n",
      "54000 iterations done!\n",
      "Training loss =  21.536638259887695\n",
      "54200 iterations done!\n",
      "Training loss =  22.387805938720703\n",
      "54400 iterations done!\n",
      "Training loss =  18.18122673034668\n",
      "54600 iterations done!\n",
      "Training loss =  18.910295486450195\n",
      "54800 iterations done!\n",
      "Training loss =  18.855865478515625\n",
      "55000 iterations done!\n",
      "Training loss =  24.48943328857422\n",
      "55200 iterations done!\n",
      "Training loss =  21.3897705078125\n",
      "55400 iterations done!\n",
      "Training loss =  18.38418960571289\n",
      "55600 iterations done!\n",
      "Training loss =  23.744457244873047\n",
      "55800 iterations done!\n",
      "Training loss =  19.33925437927246\n",
      "56000 iterations done!\n",
      "Training loss =  20.013019561767578\n",
      "56200 iterations done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss =  19.626665115356445\n",
      "56400 iterations done!\n",
      "Training loss =  23.378067016601562\n",
      "56600 iterations done!\n",
      "Training loss =  18.0224666595459\n",
      "56800 iterations done!\n",
      "Training loss =  25.162025451660156\n",
      "57000 iterations done!\n",
      "Training loss =  20.57828712463379\n",
      "57200 iterations done!\n",
      "Training loss =  19.155675888061523\n",
      "57400 iterations done!\n",
      "Training loss =  21.310056686401367\n",
      "57600 iterations done!\n",
      "Training loss =  22.571260452270508\n",
      "57800 iterations done!\n",
      "Training loss =  20.29074478149414\n",
      "58000 iterations done!\n",
      "Training loss =  20.120418548583984\n",
      "58200 iterations done!\n",
      "Training loss =  25.985349655151367\n",
      "58400 iterations done!\n",
      "Training loss =  19.02935791015625\n",
      "58600 iterations done!\n",
      "Training loss =  22.331504821777344\n",
      "58800 iterations done!\n",
      "Training loss =  21.796661376953125\n",
      "59000 iterations done!\n",
      "Training loss =  23.052820205688477\n",
      "59200 iterations done!\n",
      "Training loss =  21.07208251953125\n",
      "59400 iterations done!\n",
      "Training loss =  24.29633331298828\n",
      "59600 iterations done!\n",
      "Training loss =  17.367919921875\n",
      "59800 iterations done!\n",
      "Training loss =  21.76873779296875\n",
      "60000 iterations done!\n",
      "Training loss =  22.269527435302734\n",
      "60200 iterations done!\n",
      "Training loss =  26.551183700561523\n",
      "60400 iterations done!\n",
      "Training loss =  17.405248641967773\n",
      "60600 iterations done!\n",
      "Training loss =  20.21445083618164\n",
      "60800 iterations done!\n",
      "Training loss =  21.086570739746094\n",
      "61000 iterations done!\n",
      "Training loss =  19.258846282958984\n",
      "61200 iterations done!\n",
      "Training loss =  23.17886734008789\n",
      "61400 iterations done!\n",
      "Training loss =  22.905141830444336\n",
      "61600 iterations done!\n",
      "Training loss =  17.97874641418457\n",
      "61800 iterations done!\n",
      "Training loss =  19.492374420166016\n",
      "62000 iterations done!\n",
      "Training loss =  21.141342163085938\n",
      "62200 iterations done!\n",
      "Training loss =  20.725662231445312\n",
      "62400 iterations done!\n",
      "Training loss =  17.632444381713867\n",
      "62600 iterations done!\n",
      "Training loss =  17.609710693359375\n",
      "62800 iterations done!\n",
      "Training loss =  22.26786231994629\n",
      "63000 iterations done!\n",
      "Training loss =  16.604738235473633\n",
      "63200 iterations done!\n",
      "Training loss =  18.38860321044922\n",
      "63400 iterations done!\n",
      "Training loss =  22.677045822143555\n",
      "63600 iterations done!\n",
      "Training loss =  19.6804141998291\n",
      "63800 iterations done!\n",
      "Training loss =  27.171039581298828\n",
      "64000 iterations done!\n",
      "Training loss =  25.710845947265625\n",
      "64200 iterations done!\n",
      "Training loss =  20.957536697387695\n",
      "64400 iterations done!\n",
      "Training loss =  22.672948837280273\n",
      "64600 iterations done!\n",
      "Training loss =  17.83710479736328\n",
      "64800 iterations done!\n",
      "Training loss =  14.587146759033203\n",
      "65000 iterations done!\n",
      "Training loss =  17.957008361816406\n",
      "65200 iterations done!\n",
      "Training loss =  19.48512077331543\n",
      "65400 iterations done!\n",
      "Training loss =  19.029958724975586\n",
      "65600 iterations done!\n",
      "Training loss =  18.551921844482422\n",
      "65800 iterations done!\n",
      "Training loss =  14.810060501098633\n",
      "66000 iterations done!\n",
      "Training loss =  19.336345672607422\n",
      "66200 iterations done!\n",
      "Training loss =  22.36983871459961\n",
      "66400 iterations done!\n",
      "Training loss =  20.253297805786133\n",
      "66600 iterations done!\n",
      "Training loss =  20.136634826660156\n",
      "66800 iterations done!\n",
      "Training loss =  16.805278778076172\n",
      "67000 iterations done!\n",
      "Training loss =  20.77400016784668\n",
      "67200 iterations done!\n",
      "Training loss =  24.228734970092773\n",
      "67400 iterations done!\n",
      "Training loss =  22.94316291809082\n",
      "67600 iterations done!\n",
      "Training loss =  16.368438720703125\n",
      "67800 iterations done!\n",
      "Training loss =  21.016332626342773\n",
      "68000 iterations done!\n",
      "Training loss =  18.749332427978516\n",
      "68200 iterations done!\n",
      "Training loss =  19.106508255004883\n",
      "68400 iterations done!\n",
      "Training loss =  21.749216079711914\n",
      "68600 iterations done!\n",
      "Training loss =  21.655410766601562\n",
      "68800 iterations done!\n",
      "Training loss =  20.974592208862305\n",
      "69000 iterations done!\n",
      "Training loss =  21.375043869018555\n",
      "69200 iterations done!\n",
      "Training loss =  20.353044509887695\n",
      "69400 iterations done!\n",
      "Training loss =  23.703659057617188\n",
      "69600 iterations done!\n",
      "Training loss =  18.854135513305664\n",
      "69800 iterations done!\n",
      "Training loss =  17.283061981201172\n",
      "70000 iterations done!\n",
      "Training loss =  27.550457000732422\n",
      "70200 iterations done!\n",
      "Training loss =  22.175846099853516\n",
      "70400 iterations done!\n",
      "Training loss =  18.3734188079834\n",
      "70600 iterations done!\n",
      "Training loss =  23.72743797302246\n",
      "70800 iterations done!\n",
      "Training loss =  21.032588958740234\n",
      "71000 iterations done!\n",
      "Training loss =  20.054096221923828\n",
      "71200 iterations done!\n",
      "Training loss =  23.495807647705078\n",
      "71400 iterations done!\n",
      "Training loss =  25.88524627685547\n",
      "71600 iterations done!\n",
      "Training loss =  23.118576049804688\n",
      "71800 iterations done!\n",
      "Epoch 4 complete.\n",
      "Training loss =  24.612545013427734\n",
      "72000 iterations done!\n",
      "Training loss =  19.290456771850586\n",
      "72200 iterations done!\n",
      "Training loss =  24.98995018005371\n",
      "72400 iterations done!\n",
      "Training loss =  17.617563247680664\n",
      "72600 iterations done!\n",
      "Training loss =  16.097309112548828\n",
      "72800 iterations done!\n",
      "Training loss =  23.040435791015625\n",
      "73000 iterations done!\n",
      "Training loss =  23.04204559326172\n",
      "73200 iterations done!\n",
      "Training loss =  21.540725708007812\n",
      "73400 iterations done!\n",
      "Training loss =  19.549560546875\n",
      "73600 iterations done!\n",
      "Training loss =  18.52394676208496\n",
      "73800 iterations done!\n",
      "Training loss =  21.9637451171875\n",
      "74000 iterations done!\n",
      "Training loss =  22.505260467529297\n",
      "74200 iterations done!\n",
      "Training loss =  22.096765518188477\n",
      "74400 iterations done!\n",
      "Training loss =  22.13035774230957\n",
      "74600 iterations done!\n",
      "Training loss =  17.6414737701416\n",
      "74800 iterations done!\n",
      "Training loss =  26.80605125427246\n",
      "75000 iterations done!\n",
      "Training loss =  22.243610382080078\n",
      "75200 iterations done!\n",
      "Training loss =  20.797657012939453\n",
      "75400 iterations done!\n",
      "Training loss =  20.434471130371094\n",
      "75600 iterations done!\n",
      "Training loss =  22.5203914642334\n",
      "75800 iterations done!\n",
      "Training loss =  22.608993530273438\n",
      "76000 iterations done!\n",
      "Training loss =  20.85572052001953\n",
      "76200 iterations done!\n",
      "Training loss =  17.73895263671875\n",
      "76400 iterations done!\n",
      "Training loss =  20.222326278686523\n",
      "76600 iterations done!\n",
      "Training loss =  23.95557403564453\n",
      "76800 iterations done!\n",
      "Training loss =  29.262113571166992\n",
      "77000 iterations done!\n",
      "Training loss =  16.173526763916016\n",
      "77200 iterations done!\n",
      "Training loss =  20.10453987121582\n",
      "77400 iterations done!\n",
      "Training loss =  18.43744659423828\n",
      "77600 iterations done!\n",
      "Training loss =  22.436723709106445\n",
      "77800 iterations done!\n",
      "Training loss =  11.890999794006348\n",
      "78000 iterations done!\n",
      "Training loss =  20.279071807861328\n",
      "78200 iterations done!\n",
      "Training loss =  16.131324768066406\n",
      "78400 iterations done!\n",
      "Training loss =  18.553110122680664\n",
      "78600 iterations done!\n",
      "Training loss =  16.23725128173828\n",
      "78800 iterations done!\n",
      "Training loss =  18.571002960205078\n",
      "79000 iterations done!\n",
      "Training loss =  22.516008377075195\n",
      "79200 iterations done!\n",
      "Training loss =  20.332788467407227\n",
      "79400 iterations done!\n",
      "Training loss =  23.33910369873047\n",
      "79600 iterations done!\n",
      "Training loss =  27.192777633666992\n",
      "79800 iterations done!\n",
      "Training loss =  27.753849029541016\n",
      "80000 iterations done!\n",
      "Training loss =  19.76780128479004\n",
      "80200 iterations done!\n",
      "Training loss =  17.07474136352539\n",
      "80400 iterations done!\n",
      "Training loss =  26.334941864013672\n",
      "80600 iterations done!\n",
      "Training loss =  23.467958450317383\n",
      "80800 iterations done!\n",
      "Training loss =  21.535049438476562\n",
      "81000 iterations done!\n",
      "Training loss =  20.539785385131836\n",
      "81200 iterations done!\n",
      "Training loss =  22.576507568359375\n",
      "81400 iterations done!\n",
      "Training loss =  18.09197235107422\n",
      "81600 iterations done!\n",
      "Training loss =  23.745803833007812\n",
      "81800 iterations done!\n",
      "Training loss =  19.313392639160156\n",
      "82000 iterations done!\n",
      "Training loss =  22.05854034423828\n",
      "82200 iterations done!\n",
      "Training loss =  19.658458709716797\n",
      "82400 iterations done!\n",
      "Training loss =  21.702598571777344\n",
      "82600 iterations done!\n",
      "Training loss =  22.699922561645508\n",
      "82800 iterations done!\n",
      "Training loss =  19.892000198364258\n",
      "83000 iterations done!\n",
      "Training loss =  22.011924743652344\n",
      "83200 iterations done!\n",
      "Training loss =  22.71416664123535\n",
      "83400 iterations done!\n",
      "Training loss =  19.27362823486328\n",
      "83600 iterations done!\n",
      "Training loss =  18.198017120361328\n",
      "83800 iterations done!\n",
      "Training loss =  16.89836311340332\n",
      "84000 iterations done!\n",
      "Training loss =  18.802234649658203\n",
      "84200 iterations done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss =  22.561180114746094\n",
      "84400 iterations done!\n",
      "Training loss =  28.568716049194336\n",
      "84600 iterations done!\n",
      "Training loss =  19.410009384155273\n",
      "84800 iterations done!\n",
      "Training loss =  23.60946273803711\n",
      "85000 iterations done!\n",
      "Training loss =  15.693608283996582\n",
      "85200 iterations done!\n",
      "Training loss =  18.888065338134766\n",
      "85400 iterations done!\n",
      "Training loss =  22.528457641601562\n",
      "85600 iterations done!\n",
      "Training loss =  20.320695877075195\n",
      "85800 iterations done!\n",
      "Training loss =  19.384443283081055\n",
      "86000 iterations done!\n",
      "Training loss =  24.53950309753418\n",
      "86200 iterations done!\n",
      "Training loss =  18.41878890991211\n",
      "86400 iterations done!\n",
      "Training loss =  29.640127182006836\n",
      "86600 iterations done!\n",
      "Training loss =  19.877622604370117\n",
      "86800 iterations done!\n",
      "Training loss =  16.5949764251709\n",
      "87000 iterations done!\n",
      "Training loss =  26.04971694946289\n",
      "87200 iterations done!\n",
      "Training loss =  21.26497459411621\n",
      "87400 iterations done!\n",
      "Training loss =  23.226469039916992\n",
      "87600 iterations done!\n",
      "Training loss =  18.466564178466797\n",
      "87800 iterations done!\n",
      "Training loss =  21.383365631103516\n",
      "88000 iterations done!\n",
      "Training loss =  19.9687442779541\n",
      "88200 iterations done!\n",
      "Training loss =  19.492042541503906\n",
      "88400 iterations done!\n",
      "Training loss =  22.288251876831055\n",
      "88600 iterations done!\n",
      "Training loss =  20.863290786743164\n",
      "88800 iterations done!\n",
      "Training loss =  24.409624099731445\n",
      "89000 iterations done!\n",
      "Training loss =  23.914554595947266\n",
      "89200 iterations done!\n",
      "Training loss =  20.221981048583984\n",
      "89400 iterations done!\n",
      "Training loss =  20.782257080078125\n",
      "89600 iterations done!\n",
      "Training loss =  19.69119644165039\n",
      "89800 iterations done!\n",
      "Epoch 5 complete.\n",
      "Training loss =  15.55657958984375\n",
      "90000 iterations done!\n",
      "Training loss =  22.662206649780273\n",
      "90200 iterations done!\n",
      "Training loss =  19.22008514404297\n",
      "90400 iterations done!\n",
      "Training loss =  24.33843994140625\n",
      "90600 iterations done!\n",
      "Training loss =  30.272825241088867\n",
      "90800 iterations done!\n",
      "Training loss =  20.425472259521484\n",
      "91000 iterations done!\n",
      "Training loss =  27.055198669433594\n",
      "91200 iterations done!\n",
      "Training loss =  22.919357299804688\n",
      "91400 iterations done!\n",
      "Training loss =  19.64101791381836\n",
      "91600 iterations done!\n",
      "Training loss =  21.440475463867188\n",
      "91800 iterations done!\n",
      "Training loss =  20.328142166137695\n",
      "92000 iterations done!\n",
      "Training loss =  21.981224060058594\n",
      "92200 iterations done!\n",
      "Training loss =  18.502561569213867\n",
      "92400 iterations done!\n",
      "Training loss =  26.686779022216797\n",
      "92600 iterations done!\n",
      "Training loss =  21.065759658813477\n",
      "92800 iterations done!\n",
      "Training loss =  21.27407455444336\n",
      "93000 iterations done!\n",
      "Training loss =  19.245716094970703\n",
      "93200 iterations done!\n",
      "Training loss =  19.082548141479492\n",
      "93400 iterations done!\n",
      "Training loss =  19.789602279663086\n",
      "93600 iterations done!\n",
      "Training loss =  30.56368064880371\n",
      "93800 iterations done!\n",
      "Training loss =  18.516292572021484\n",
      "94000 iterations done!\n",
      "Training loss =  17.770156860351562\n",
      "94200 iterations done!\n",
      "Training loss =  28.123071670532227\n",
      "94400 iterations done!\n",
      "Training loss =  18.765005111694336\n",
      "94600 iterations done!\n",
      "Training loss =  14.90744686126709\n",
      "94800 iterations done!\n",
      "Training loss =  19.988277435302734\n",
      "95000 iterations done!\n",
      "Training loss =  16.38020896911621\n",
      "95200 iterations done!\n",
      "Training loss =  16.610366821289062\n",
      "95400 iterations done!\n",
      "Training loss =  22.564178466796875\n",
      "95600 iterations done!\n",
      "Training loss =  22.27430534362793\n",
      "95800 iterations done!\n",
      "Training loss =  18.16546058654785\n",
      "96000 iterations done!\n",
      "Training loss =  18.09605598449707\n",
      "96200 iterations done!\n",
      "Training loss =  18.57940101623535\n",
      "96400 iterations done!\n",
      "Training loss =  16.030296325683594\n",
      "96600 iterations done!\n",
      "Training loss =  23.36377716064453\n",
      "96800 iterations done!\n",
      "Training loss =  17.881013870239258\n",
      "97000 iterations done!\n",
      "Training loss =  21.065196990966797\n",
      "97200 iterations done!\n",
      "Training loss =  21.38791847229004\n",
      "97400 iterations done!\n",
      "Training loss =  19.269878387451172\n",
      "97600 iterations done!\n",
      "Training loss =  17.459943771362305\n",
      "97800 iterations done!\n",
      "Training loss =  22.660552978515625\n",
      "98000 iterations done!\n",
      "Training loss =  15.75514030456543\n",
      "98200 iterations done!\n",
      "Training loss =  26.21269416809082\n",
      "98400 iterations done!\n",
      "Training loss =  19.32632064819336\n",
      "98600 iterations done!\n",
      "Training loss =  22.278444290161133\n",
      "98800 iterations done!\n",
      "Training loss =  19.073144912719727\n",
      "99000 iterations done!\n",
      "Training loss =  16.109743118286133\n",
      "99200 iterations done!\n",
      "Training loss =  20.692733764648438\n",
      "99400 iterations done!\n",
      "Training loss =  17.29264259338379\n",
      "99600 iterations done!\n",
      "Training loss =  23.449708938598633\n",
      "99800 iterations done!\n",
      "Training loss =  20.45334243774414\n",
      "100000 iterations done!\n",
      "Training loss =  23.126739501953125\n",
      "100200 iterations done!\n",
      "Training loss =  22.6401309967041\n",
      "100400 iterations done!\n",
      "Training loss =  21.49386978149414\n",
      "100600 iterations done!\n",
      "Training loss =  17.852584838867188\n",
      "100800 iterations done!\n",
      "Training loss =  21.972660064697266\n",
      "101000 iterations done!\n",
      "Training loss =  14.661673545837402\n",
      "101200 iterations done!\n",
      "Training loss =  17.643156051635742\n",
      "101400 iterations done!\n",
      "Training loss =  23.11798858642578\n",
      "101600 iterations done!\n",
      "Training loss =  18.32538604736328\n",
      "101800 iterations done!\n",
      "Training loss =  23.277172088623047\n",
      "102000 iterations done!\n",
      "Training loss =  27.047042846679688\n",
      "102200 iterations done!\n",
      "Training loss =  25.734445571899414\n",
      "102400 iterations done!\n",
      "Training loss =  22.355201721191406\n",
      "102600 iterations done!\n",
      "Training loss =  25.765687942504883\n",
      "102800 iterations done!\n",
      "Training loss =  17.78668975830078\n",
      "103000 iterations done!\n",
      "Training loss =  15.140922546386719\n",
      "103200 iterations done!\n",
      "Training loss =  20.794795989990234\n",
      "103400 iterations done!\n",
      "Training loss =  17.85672378540039\n",
      "103600 iterations done!\n",
      "Training loss =  19.036474227905273\n",
      "103800 iterations done!\n",
      "Training loss =  20.201499938964844\n",
      "104000 iterations done!\n",
      "Training loss =  16.851350784301758\n",
      "104200 iterations done!\n",
      "Training loss =  29.022390365600586\n",
      "104400 iterations done!\n",
      "Training loss =  21.033580780029297\n",
      "104600 iterations done!\n",
      "Training loss =  24.578407287597656\n",
      "104800 iterations done!\n",
      "Training loss =  25.747812271118164\n",
      "105000 iterations done!\n",
      "Training loss =  19.33148765563965\n",
      "105200 iterations done!\n",
      "Training loss =  17.30091667175293\n",
      "105400 iterations done!\n",
      "Training loss =  19.28293228149414\n",
      "105600 iterations done!\n",
      "Training loss =  19.821258544921875\n",
      "105800 iterations done!\n",
      "Training loss =  16.194883346557617\n",
      "106000 iterations done!\n",
      "Training loss =  19.81754493713379\n",
      "106200 iterations done!\n",
      "Training loss =  22.905296325683594\n",
      "106400 iterations done!\n",
      "Training loss =  21.512887954711914\n",
      "106600 iterations done!\n",
      "Training loss =  16.28066635131836\n",
      "106800 iterations done!\n",
      "Training loss =  21.481321334838867\n",
      "107000 iterations done!\n",
      "Training loss =  20.690563201904297\n",
      "107200 iterations done!\n",
      "Training loss =  23.9056453704834\n",
      "107400 iterations done!\n",
      "Training loss =  21.0223445892334\n",
      "107600 iterations done!\n",
      "Training loss =  12.20608139038086\n",
      "107800 iterations done!\n",
      "Epoch 6 complete.\n",
      "Training loss =  21.092147827148438\n",
      "108000 iterations done!\n",
      "Training loss =  24.34069061279297\n",
      "108200 iterations done!\n",
      "Training loss =  20.118791580200195\n",
      "108400 iterations done!\n",
      "Training loss =  21.314807891845703\n",
      "108600 iterations done!\n",
      "Training loss =  21.35540771484375\n",
      "108800 iterations done!\n",
      "Training loss =  22.028789520263672\n",
      "109000 iterations done!\n",
      "Training loss =  22.21334457397461\n",
      "109200 iterations done!\n",
      "Training loss =  27.0351619720459\n",
      "109400 iterations done!\n",
      "Training loss =  20.53855323791504\n",
      "109600 iterations done!\n",
      "Training loss =  21.420719146728516\n",
      "109800 iterations done!\n",
      "Training loss =  14.439251899719238\n",
      "110000 iterations done!\n",
      "Training loss =  26.181970596313477\n",
      "110200 iterations done!\n",
      "Training loss =  21.20466423034668\n",
      "110400 iterations done!\n",
      "Training loss =  18.598079681396484\n",
      "110600 iterations done!\n",
      "Training loss =  17.236286163330078\n",
      "110800 iterations done!\n",
      "Training loss =  20.10637092590332\n",
      "111000 iterations done!\n",
      "Training loss =  21.247777938842773\n",
      "111200 iterations done!\n",
      "Training loss =  16.881927490234375\n",
      "111400 iterations done!\n",
      "Training loss =  23.029678344726562\n",
      "111600 iterations done!\n",
      "Training loss =  17.75568389892578\n",
      "111800 iterations done!\n",
      "Training loss =  21.08230209350586\n",
      "112000 iterations done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss =  21.31230926513672\n",
      "112200 iterations done!\n",
      "Training loss =  16.714744567871094\n",
      "112400 iterations done!\n",
      "Training loss =  21.929569244384766\n",
      "112600 iterations done!\n",
      "Training loss =  23.873271942138672\n",
      "112800 iterations done!\n",
      "Training loss =  23.57910919189453\n",
      "113000 iterations done!\n",
      "Training loss =  19.41652488708496\n",
      "113200 iterations done!\n",
      "Training loss =  17.400733947753906\n",
      "113400 iterations done!\n",
      "Training loss =  18.332265853881836\n",
      "113600 iterations done!\n",
      "Training loss =  20.437463760375977\n",
      "113800 iterations done!\n",
      "Training loss =  17.463716506958008\n",
      "114000 iterations done!\n",
      "Training loss =  20.7996826171875\n",
      "114200 iterations done!\n",
      "Training loss =  16.21112823486328\n",
      "114400 iterations done!\n",
      "Training loss =  20.030868530273438\n",
      "114600 iterations done!\n",
      "Training loss =  16.035694122314453\n",
      "114800 iterations done!\n",
      "Training loss =  19.287960052490234\n",
      "115000 iterations done!\n",
      "Training loss =  28.92349624633789\n",
      "115200 iterations done!\n",
      "Training loss =  17.134241104125977\n",
      "115400 iterations done!\n",
      "Training loss =  20.329235076904297\n",
      "115600 iterations done!\n",
      "Training loss =  18.310447692871094\n",
      "115800 iterations done!\n",
      "Training loss =  17.954084396362305\n",
      "116000 iterations done!\n",
      "Training loss =  19.272262573242188\n",
      "116200 iterations done!\n",
      "Training loss =  19.1187801361084\n",
      "116400 iterations done!\n",
      "Training loss =  19.471736907958984\n",
      "116600 iterations done!\n",
      "Training loss =  16.841041564941406\n",
      "116800 iterations done!\n",
      "Training loss =  19.58827018737793\n",
      "117000 iterations done!\n",
      "Training loss =  20.59629249572754\n",
      "117200 iterations done!\n",
      "Training loss =  18.693641662597656\n",
      "117400 iterations done!\n",
      "Training loss =  17.54092025756836\n",
      "117600 iterations done!\n",
      "Training loss =  32.10380554199219\n",
      "117800 iterations done!\n",
      "Training loss =  21.640071868896484\n",
      "118000 iterations done!\n",
      "Training loss =  16.794710159301758\n",
      "118200 iterations done!\n",
      "Training loss =  17.537202835083008\n",
      "118400 iterations done!\n",
      "Training loss =  22.036479949951172\n",
      "118600 iterations done!\n",
      "Training loss =  20.499557495117188\n",
      "118800 iterations done!\n",
      "Training loss =  19.402118682861328\n",
      "119000 iterations done!\n",
      "Training loss =  19.968399047851562\n",
      "119200 iterations done!\n",
      "Training loss =  14.907818794250488\n",
      "119400 iterations done!\n",
      "Training loss =  23.69793701171875\n",
      "119600 iterations done!\n",
      "Training loss =  20.910320281982422\n",
      "119800 iterations done!\n",
      "Training loss =  29.31245994567871\n",
      "120000 iterations done!\n",
      "Training loss =  24.25200653076172\n",
      "120200 iterations done!\n",
      "Training loss =  21.193317413330078\n",
      "120400 iterations done!\n",
      "Training loss =  18.474489212036133\n",
      "120600 iterations done!\n",
      "Training loss =  22.73424530029297\n",
      "120800 iterations done!\n",
      "Training loss =  20.39773178100586\n",
      "121000 iterations done!\n",
      "Training loss =  24.048664093017578\n",
      "121200 iterations done!\n",
      "Training loss =  16.421682357788086\n",
      "121400 iterations done!\n",
      "Training loss =  20.940961837768555\n",
      "121600 iterations done!\n",
      "Training loss =  22.167736053466797\n",
      "121800 iterations done!\n",
      "Training loss =  19.469470977783203\n",
      "122000 iterations done!\n",
      "Training loss =  19.296672821044922\n",
      "122200 iterations done!\n",
      "Training loss =  20.343774795532227\n",
      "122400 iterations done!\n",
      "Training loss =  17.431888580322266\n",
      "122600 iterations done!\n",
      "Training loss =  18.410045623779297\n",
      "122800 iterations done!\n",
      "Training loss =  18.483240127563477\n",
      "123000 iterations done!\n",
      "Training loss =  19.09375762939453\n",
      "123200 iterations done!\n",
      "Training loss =  21.504541397094727\n",
      "123400 iterations done!\n",
      "Training loss =  22.648550033569336\n",
      "123600 iterations done!\n",
      "Training loss =  21.543272018432617\n",
      "123800 iterations done!\n",
      "Training loss =  17.87425422668457\n",
      "124000 iterations done!\n",
      "Training loss =  21.57984733581543\n",
      "124200 iterations done!\n",
      "Training loss =  17.458782196044922\n",
      "124400 iterations done!\n",
      "Training loss =  21.35837173461914\n",
      "124600 iterations done!\n",
      "Training loss =  19.084531784057617\n",
      "124800 iterations done!\n",
      "Training loss =  19.90717315673828\n",
      "125000 iterations done!\n",
      "Training loss =  26.343786239624023\n",
      "125200 iterations done!\n",
      "Training loss =  17.504600524902344\n",
      "125400 iterations done!\n",
      "Training loss =  20.431833267211914\n",
      "125600 iterations done!\n",
      "Training loss =  15.804720878601074\n",
      "125800 iterations done!\n",
      "Epoch 7 complete.\n",
      "Training loss =  23.668127059936523\n",
      "126000 iterations done!\n",
      "Training loss =  16.14274024963379\n",
      "126200 iterations done!\n",
      "Training loss =  22.517852783203125\n",
      "126400 iterations done!\n",
      "Training loss =  19.6523380279541\n",
      "126600 iterations done!\n",
      "Training loss =  24.374162673950195\n",
      "126800 iterations done!\n",
      "Training loss =  24.1805362701416\n",
      "127000 iterations done!\n",
      "Training loss =  21.526939392089844\n",
      "127200 iterations done!\n",
      "Training loss =  17.68343162536621\n",
      "127400 iterations done!\n",
      "Training loss =  17.430543899536133\n",
      "127600 iterations done!\n",
      "Training loss =  23.549015045166016\n",
      "127800 iterations done!\n",
      "Training loss =  28.277542114257812\n",
      "128000 iterations done!\n",
      "Training loss =  19.375934600830078\n",
      "128200 iterations done!\n",
      "Training loss =  13.067144393920898\n",
      "128400 iterations done!\n",
      "Training loss =  18.030115127563477\n",
      "128600 iterations done!\n",
      "Training loss =  17.29094696044922\n",
      "128800 iterations done!\n",
      "Training loss =  23.982341766357422\n",
      "129000 iterations done!\n",
      "Training loss =  20.404611587524414\n",
      "129200 iterations done!\n",
      "Training loss =  15.82727336883545\n",
      "129400 iterations done!\n",
      "Training loss =  23.144439697265625\n",
      "129600 iterations done!\n",
      "Training loss =  17.896820068359375\n",
      "129800 iterations done!\n",
      "Training loss =  24.663013458251953\n",
      "130000 iterations done!\n",
      "Training loss =  16.000869750976562\n",
      "130200 iterations done!\n",
      "Training loss =  21.216472625732422\n",
      "130400 iterations done!\n",
      "Training loss =  19.4230899810791\n",
      "130600 iterations done!\n",
      "Training loss =  20.18935775756836\n",
      "130800 iterations done!\n",
      "Training loss =  22.944046020507812\n",
      "131000 iterations done!\n",
      "Training loss =  20.473665237426758\n",
      "131200 iterations done!\n",
      "Training loss =  22.74675178527832\n",
      "131400 iterations done!\n",
      "Training loss =  18.76956558227539\n",
      "131600 iterations done!\n",
      "Training loss =  15.69186782836914\n",
      "131800 iterations done!\n",
      "Training loss =  26.290054321289062\n",
      "132000 iterations done!\n",
      "Training loss =  15.4569730758667\n",
      "132200 iterations done!\n",
      "Training loss =  25.79170799255371\n",
      "132400 iterations done!\n",
      "Training loss =  22.224334716796875\n",
      "132600 iterations done!\n",
      "Training loss =  23.39026641845703\n",
      "132800 iterations done!\n",
      "Training loss =  17.74321174621582\n",
      "133000 iterations done!\n",
      "Training loss =  22.54814338684082\n",
      "133200 iterations done!\n",
      "Training loss =  22.48617172241211\n",
      "133400 iterations done!\n",
      "Training loss =  28.027080535888672\n",
      "133600 iterations done!\n",
      "Training loss =  16.74758529663086\n",
      "133800 iterations done!\n",
      "Training loss =  25.013704299926758\n",
      "134000 iterations done!\n",
      "Training loss =  17.843416213989258\n",
      "134200 iterations done!\n",
      "Training loss =  18.93621063232422\n",
      "134400 iterations done!\n",
      "Training loss =  17.201452255249023\n",
      "134600 iterations done!\n",
      "Training loss =  18.966012954711914\n",
      "134800 iterations done!\n",
      "Training loss =  22.259830474853516\n",
      "135000 iterations done!\n",
      "Training loss =  19.20147132873535\n",
      "135200 iterations done!\n",
      "Training loss =  23.721878051757812\n",
      "135400 iterations done!\n",
      "Training loss =  22.02269744873047\n",
      "135600 iterations done!\n",
      "Training loss =  20.4022216796875\n",
      "135800 iterations done!\n",
      "Training loss =  19.657855987548828\n",
      "136000 iterations done!\n",
      "Training loss =  15.085861206054688\n",
      "136200 iterations done!\n",
      "Training loss =  17.685991287231445\n",
      "136400 iterations done!\n",
      "Training loss =  20.07144546508789\n",
      "136600 iterations done!\n",
      "Training loss =  17.787057876586914\n",
      "136800 iterations done!\n",
      "Training loss =  17.0710391998291\n",
      "137000 iterations done!\n",
      "Training loss =  20.34465980529785\n",
      "137200 iterations done!\n",
      "Training loss =  18.70880889892578\n",
      "137400 iterations done!\n",
      "Training loss =  20.630685806274414\n",
      "137600 iterations done!\n",
      "Training loss =  15.4803466796875\n",
      "137800 iterations done!\n",
      "Training loss =  22.87923812866211\n",
      "138000 iterations done!\n",
      "Training loss =  19.266706466674805\n",
      "138200 iterations done!\n",
      "Training loss =  16.57624053955078\n",
      "138400 iterations done!\n",
      "Training loss =  16.644304275512695\n",
      "138600 iterations done!\n",
      "Training loss =  25.206132888793945\n",
      "138800 iterations done!\n",
      "Training loss =  17.75708770751953\n",
      "139000 iterations done!\n",
      "Training loss =  19.457468032836914\n",
      "139200 iterations done!\n",
      "Training loss =  13.83342170715332\n",
      "139400 iterations done!\n",
      "Training loss =  18.4094295501709\n",
      "139600 iterations done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss =  16.473554611206055\n",
      "139800 iterations done!\n",
      "Training loss =  23.50560760498047\n",
      "140000 iterations done!\n",
      "Training loss =  19.71564292907715\n",
      "140200 iterations done!\n",
      "Training loss =  15.387641906738281\n",
      "140400 iterations done!\n",
      "Training loss =  23.544933319091797\n",
      "140600 iterations done!\n",
      "Training loss =  18.41555404663086\n",
      "140800 iterations done!\n",
      "Training loss =  24.560396194458008\n",
      "141000 iterations done!\n",
      "Training loss =  18.75901222229004\n",
      "141200 iterations done!\n",
      "Training loss =  19.806293487548828\n",
      "141400 iterations done!\n",
      "Training loss =  19.12457847595215\n",
      "141600 iterations done!\n",
      "Training loss =  21.00269317626953\n",
      "141800 iterations done!\n",
      "Training loss =  25.398561477661133\n",
      "142000 iterations done!\n",
      "Training loss =  16.16298484802246\n",
      "142200 iterations done!\n",
      "Training loss =  21.880939483642578\n",
      "142400 iterations done!\n",
      "Training loss =  18.491661071777344\n",
      "142600 iterations done!\n",
      "Training loss =  21.06307601928711\n",
      "142800 iterations done!\n",
      "Training loss =  24.525606155395508\n",
      "143000 iterations done!\n",
      "Training loss =  20.682056427001953\n",
      "143200 iterations done!\n",
      "Training loss =  15.964008331298828\n",
      "143400 iterations done!\n",
      "Training loss =  18.36742401123047\n",
      "143600 iterations done!\n",
      "Training loss =  18.054140090942383\n",
      "143800 iterations done!\n",
      "Epoch 8 complete.\n",
      "Training loss =  19.670713424682617\n",
      "144000 iterations done!\n",
      "Training loss =  19.602846145629883\n",
      "144200 iterations done!\n",
      "Training loss =  18.291309356689453\n",
      "144400 iterations done!\n",
      "Training loss =  23.988574981689453\n",
      "144600 iterations done!\n",
      "Training loss =  20.39267921447754\n",
      "144800 iterations done!\n",
      "Training loss =  18.527673721313477\n",
      "145000 iterations done!\n",
      "Training loss =  16.454652786254883\n",
      "145200 iterations done!\n",
      "Training loss =  21.54214859008789\n",
      "145400 iterations done!\n",
      "Training loss =  16.459779739379883\n",
      "145600 iterations done!\n",
      "Training loss =  17.164974212646484\n",
      "145800 iterations done!\n",
      "Training loss =  23.87158203125\n",
      "146000 iterations done!\n",
      "Training loss =  17.977575302124023\n",
      "146200 iterations done!\n",
      "Training loss =  23.064931869506836\n",
      "146400 iterations done!\n",
      "Training loss =  27.712154388427734\n",
      "146600 iterations done!\n",
      "Training loss =  18.267196655273438\n",
      "146800 iterations done!\n",
      "Training loss =  13.627716064453125\n",
      "147000 iterations done!\n",
      "Training loss =  20.154008865356445\n",
      "147200 iterations done!\n",
      "Training loss =  26.01125144958496\n",
      "147400 iterations done!\n",
      "Training loss =  19.64676856994629\n",
      "147600 iterations done!\n",
      "Training loss =  14.992929458618164\n",
      "147800 iterations done!\n",
      "Training loss =  19.941675186157227\n",
      "148000 iterations done!\n",
      "Training loss =  18.34564971923828\n",
      "148200 iterations done!\n",
      "Training loss =  21.685867309570312\n",
      "148400 iterations done!\n",
      "Training loss =  21.335002899169922\n",
      "148600 iterations done!\n",
      "Training loss =  16.78002166748047\n",
      "148800 iterations done!\n",
      "Training loss =  46.629032135009766\n",
      "149000 iterations done!\n",
      "Training loss =  20.586448669433594\n",
      "149200 iterations done!\n",
      "Training loss =  18.74397850036621\n",
      "149400 iterations done!\n",
      "Training loss =  23.820560455322266\n",
      "149600 iterations done!\n",
      "Training loss =  17.445350646972656\n",
      "149800 iterations done!\n",
      "Training loss =  16.88226318359375\n",
      "150000 iterations done!\n",
      "Training loss =  20.517642974853516\n",
      "150200 iterations done!\n",
      "Training loss =  18.944528579711914\n",
      "150400 iterations done!\n",
      "Training loss =  24.796091079711914\n",
      "150600 iterations done!\n",
      "Training loss =  15.027362823486328\n",
      "150800 iterations done!\n",
      "Training loss =  16.186933517456055\n",
      "151000 iterations done!\n",
      "Training loss =  18.556949615478516\n",
      "151200 iterations done!\n",
      "Training loss =  18.397258758544922\n",
      "151400 iterations done!\n",
      "Training loss =  20.290428161621094\n",
      "151600 iterations done!\n",
      "Training loss =  16.71454620361328\n",
      "151800 iterations done!\n",
      "Training loss =  18.468469619750977\n",
      "152000 iterations done!\n",
      "Training loss =  22.331146240234375\n",
      "152200 iterations done!\n",
      "Training loss =  15.445091247558594\n",
      "152400 iterations done!\n",
      "Training loss =  27.29535675048828\n",
      "152600 iterations done!\n",
      "Training loss =  24.587194442749023\n",
      "152800 iterations done!\n",
      "Training loss =  19.446640014648438\n",
      "153000 iterations done!\n",
      "Training loss =  17.206985473632812\n",
      "153200 iterations done!\n",
      "Training loss =  18.40199089050293\n",
      "153400 iterations done!\n",
      "Training loss =  30.717365264892578\n",
      "153600 iterations done!\n",
      "Training loss =  21.42079734802246\n",
      "153800 iterations done!\n",
      "Training loss =  20.07122230529785\n",
      "154000 iterations done!\n",
      "Training loss =  16.22392463684082\n",
      "154200 iterations done!\n",
      "Training loss =  22.830873489379883\n",
      "154400 iterations done!\n",
      "Training loss =  19.98963737487793\n",
      "154600 iterations done!\n",
      "Training loss =  16.900602340698242\n",
      "154800 iterations done!\n",
      "Training loss =  20.759817123413086\n",
      "155000 iterations done!\n",
      "Training loss =  19.384273529052734\n",
      "155200 iterations done!\n",
      "Training loss =  19.48113441467285\n",
      "155400 iterations done!\n",
      "Training loss =  17.755630493164062\n",
      "155600 iterations done!\n",
      "Training loss =  19.65310287475586\n",
      "155800 iterations done!\n",
      "Training loss =  19.749414443969727\n",
      "156000 iterations done!\n",
      "Training loss =  21.238636016845703\n",
      "156200 iterations done!\n",
      "Training loss =  15.922874450683594\n",
      "156400 iterations done!\n",
      "Training loss =  18.2777042388916\n",
      "156600 iterations done!\n",
      "Training loss =  21.030412673950195\n",
      "156800 iterations done!\n",
      "Training loss =  23.926939010620117\n",
      "157000 iterations done!\n",
      "Training loss =  18.75286293029785\n",
      "157200 iterations done!\n",
      "Training loss =  15.355965614318848\n",
      "157400 iterations done!\n",
      "Training loss =  20.53268051147461\n",
      "157600 iterations done!\n",
      "Training loss =  21.430614471435547\n",
      "157800 iterations done!\n",
      "Training loss =  17.5183162689209\n",
      "158000 iterations done!\n",
      "Training loss =  16.571725845336914\n",
      "158200 iterations done!\n",
      "Training loss =  21.137216567993164\n",
      "158400 iterations done!\n",
      "Training loss =  18.734506607055664\n",
      "158600 iterations done!\n",
      "Training loss =  22.080589294433594\n",
      "158800 iterations done!\n",
      "Training loss =  23.312448501586914\n",
      "159000 iterations done!\n",
      "Training loss =  19.24688720703125\n",
      "159200 iterations done!\n",
      "Training loss =  9.645644187927246\n",
      "159400 iterations done!\n",
      "Training loss =  29.129148483276367\n",
      "159600 iterations done!\n",
      "Training loss =  25.069730758666992\n",
      "159800 iterations done!\n",
      "Training loss =  21.945810317993164\n",
      "160000 iterations done!\n",
      "Training loss =  17.88410186767578\n",
      "160200 iterations done!\n",
      "Training loss =  18.38092803955078\n",
      "160400 iterations done!\n",
      "Training loss =  23.199880599975586\n",
      "160600 iterations done!\n",
      "Training loss =  26.24298667907715\n",
      "160800 iterations done!\n",
      "Training loss =  19.893918991088867\n",
      "161000 iterations done!\n",
      "Training loss =  20.683353424072266\n",
      "161200 iterations done!\n",
      "Training loss =  20.287796020507812\n",
      "161400 iterations done!\n",
      "Training loss =  17.822147369384766\n",
      "161600 iterations done!\n",
      "Training loss =  17.220932006835938\n",
      "161800 iterations done!\n",
      "Epoch 9 complete.\n",
      "Training loss =  20.275474548339844\n",
      "162000 iterations done!\n",
      "Training loss =  20.481374740600586\n",
      "162200 iterations done!\n",
      "Training loss =  18.67426872253418\n",
      "162400 iterations done!\n",
      "Training loss =  16.68003273010254\n",
      "162600 iterations done!\n",
      "Training loss =  22.55970573425293\n",
      "162800 iterations done!\n",
      "Training loss =  22.73723030090332\n",
      "163000 iterations done!\n",
      "Training loss =  19.580156326293945\n",
      "163200 iterations done!\n",
      "Training loss =  16.565689086914062\n",
      "163400 iterations done!\n",
      "Training loss =  24.782968521118164\n",
      "163600 iterations done!\n",
      "Training loss =  17.098201751708984\n",
      "163800 iterations done!\n",
      "Training loss =  22.204763412475586\n",
      "164000 iterations done!\n",
      "Training loss =  19.1291561126709\n",
      "164200 iterations done!\n",
      "Training loss =  17.657194137573242\n",
      "164400 iterations done!\n",
      "Training loss =  20.446914672851562\n",
      "164600 iterations done!\n",
      "Training loss =  20.055164337158203\n",
      "164800 iterations done!\n",
      "Training loss =  26.36241340637207\n",
      "165000 iterations done!\n",
      "Training loss =  21.227802276611328\n",
      "165200 iterations done!\n",
      "Training loss =  18.971763610839844\n",
      "165400 iterations done!\n",
      "Training loss =  18.147724151611328\n",
      "165600 iterations done!\n",
      "Training loss =  15.671138763427734\n",
      "165800 iterations done!\n",
      "Training loss =  26.577350616455078\n",
      "166000 iterations done!\n",
      "Training loss =  18.17290687561035\n",
      "166200 iterations done!\n",
      "Training loss =  18.212078094482422\n",
      "166400 iterations done!\n",
      "Training loss =  21.338274002075195\n",
      "166600 iterations done!\n",
      "Training loss =  19.60013198852539\n",
      "166800 iterations done!\n",
      "Training loss =  18.53253746032715\n",
      "167000 iterations done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss =  16.306127548217773\n",
      "167200 iterations done!\n",
      "Training loss =  17.710655212402344\n",
      "167400 iterations done!\n",
      "Training loss =  22.22171974182129\n",
      "167600 iterations done!\n",
      "Training loss =  22.140480041503906\n",
      "167800 iterations done!\n",
      "Training loss =  21.18450927734375\n",
      "168000 iterations done!\n",
      "Training loss =  21.20291519165039\n",
      "168200 iterations done!\n",
      "Training loss =  15.681964874267578\n",
      "168400 iterations done!\n",
      "Training loss =  21.07587432861328\n",
      "168600 iterations done!\n",
      "Training loss =  18.35218620300293\n",
      "168800 iterations done!\n",
      "Training loss =  14.812780380249023\n",
      "169000 iterations done!\n",
      "Training loss =  19.23590660095215\n",
      "169200 iterations done!\n",
      "Training loss =  15.249451637268066\n",
      "169400 iterations done!\n",
      "Training loss =  22.496631622314453\n",
      "169600 iterations done!\n",
      "Training loss =  19.73723793029785\n",
      "169800 iterations done!\n",
      "Training loss =  14.493307113647461\n",
      "170000 iterations done!\n",
      "Training loss =  21.401527404785156\n",
      "170200 iterations done!\n",
      "Training loss =  17.07150650024414\n",
      "170400 iterations done!\n",
      "Training loss =  21.515981674194336\n",
      "170600 iterations done!\n",
      "Training loss =  13.244962692260742\n",
      "170800 iterations done!\n",
      "Training loss =  16.433090209960938\n",
      "171000 iterations done!\n",
      "Training loss =  17.479145050048828\n",
      "171200 iterations done!\n",
      "Training loss =  18.663793563842773\n",
      "171400 iterations done!\n",
      "Training loss =  19.441648483276367\n",
      "171600 iterations done!\n",
      "Training loss =  23.017057418823242\n",
      "171800 iterations done!\n",
      "Training loss =  24.43035888671875\n",
      "172000 iterations done!\n",
      "Training loss =  24.725997924804688\n",
      "172200 iterations done!\n",
      "Training loss =  18.275747299194336\n",
      "172400 iterations done!\n",
      "Training loss =  20.5206241607666\n",
      "172600 iterations done!\n",
      "Training loss =  21.059412002563477\n",
      "172800 iterations done!\n",
      "Training loss =  22.3607234954834\n",
      "173000 iterations done!\n",
      "Training loss =  13.519243240356445\n",
      "173200 iterations done!\n",
      "Training loss =  20.97951316833496\n",
      "173400 iterations done!\n",
      "Training loss =  27.06618881225586\n",
      "173600 iterations done!\n",
      "Training loss =  21.362892150878906\n",
      "173800 iterations done!\n",
      "Training loss =  17.317668914794922\n",
      "174000 iterations done!\n",
      "Training loss =  17.950345993041992\n",
      "174200 iterations done!\n",
      "Training loss =  28.689546585083008\n",
      "174400 iterations done!\n",
      "Training loss =  25.82965850830078\n",
      "174600 iterations done!\n",
      "Training loss =  15.480332374572754\n",
      "174800 iterations done!\n",
      "Training loss =  18.509864807128906\n",
      "175000 iterations done!\n",
      "Training loss =  20.509441375732422\n",
      "175200 iterations done!\n",
      "Training loss =  18.311477661132812\n",
      "175400 iterations done!\n",
      "Training loss =  20.437049865722656\n",
      "175600 iterations done!\n",
      "Training loss =  19.4350528717041\n",
      "175800 iterations done!\n",
      "Training loss =  21.56362533569336\n",
      "176000 iterations done!\n",
      "Training loss =  22.992584228515625\n",
      "176200 iterations done!\n",
      "Training loss =  22.491397857666016\n",
      "176400 iterations done!\n",
      "Training loss =  21.91945457458496\n",
      "176600 iterations done!\n",
      "Training loss =  23.1932373046875\n",
      "176800 iterations done!\n",
      "Training loss =  17.555522918701172\n",
      "177000 iterations done!\n",
      "Training loss =  19.897886276245117\n",
      "177200 iterations done!\n",
      "Training loss =  13.297487258911133\n",
      "177400 iterations done!\n",
      "Training loss =  18.801008224487305\n",
      "177600 iterations done!\n",
      "Training loss =  17.872953414916992\n",
      "177800 iterations done!\n",
      "Training loss =  20.82969856262207\n",
      "178000 iterations done!\n",
      "Training loss =  24.610862731933594\n",
      "178200 iterations done!\n",
      "Training loss =  15.506288528442383\n",
      "178400 iterations done!\n",
      "Training loss =  16.85171127319336\n",
      "178600 iterations done!\n",
      "Training loss =  22.27654457092285\n",
      "178800 iterations done!\n",
      "Training loss =  21.317289352416992\n",
      "179000 iterations done!\n",
      "Training loss =  23.378931045532227\n",
      "179200 iterations done!\n",
      "Training loss =  18.886058807373047\n",
      "179400 iterations done!\n",
      "Training loss =  50.21086120605469\n",
      "179600 iterations done!\n",
      "Training loss =  20.228967666625977\n",
      "179800 iterations done!\n",
      "Epoch 10 complete.\n"
     ]
    }
   ],
   "source": [
    "if not is_continuous:\n",
    "    t0 = time.time()\n",
    "    global_counter = 0\n",
    "else:\n",
    "    with open(os.path.join(ckpt_dir, 'log.out')) as f:\n",
    "        records = f.readlines()\n",
    "        if records[-1] != 'Training finished\\n':\n",
    "            final_record = records[-1]\n",
    "        else:\n",
    "            final_record = records[-2]\n",
    "    count, t_final = int(final_record.split('\\t')[0]), float(final_record.split('\\t')[1])\n",
    "    t0 = time.time() - t_final * 60\n",
    "    global_counter = count\n",
    "\n",
    "epoch_no = 1\n",
    "break_train_loop = False\n",
    "early_stopping = EarlyStopping(patience=patience, delta=1e-2, less_is_better=True)\n",
    "\n",
    "with open(os.path.join(ckpt_dir, 'log.out'),\n",
    "          mode='w' if not is_continuous else 'a') as f:\n",
    "    if not is_continuous:\n",
    "        f.write('Training started...\\n')\n",
    "        f.write('step\\ttime(m)\\tloss\\tlr\\n')\n",
    "    while True:\n",
    "        global_counter += 1\n",
    "        eval_now = False\n",
    "        try:\n",
    "            inputs = next(it_train)\n",
    "        except StopIteration:\n",
    "            eval_now = True\n",
    "            print('Epoch {} complete.'.format(epoch_no))\n",
    "            f.write('\\nEpoch {} complete.\\n'.format(epoch_no))\n",
    "            epoch_no += 1\n",
    "            it_train = iter(loader_train)\n",
    "            inputs = next(it_train)\n",
    "\n",
    "        # move to gpu\n",
    "        inputs = CMolRNNLoader.from_numpy_to_tensor(inputs)\n",
    "\n",
    "        with autograd.record():\n",
    "            loss = [(model(*inputs)).as_in_context(mx.gpu())]\n",
    "            loss = sum(loss)\n",
    "            loss.backward()\n",
    "\n",
    "        nd.waitall()\n",
    "        gc.collect()\n",
    "\n",
    "        trainer.step(batch_size=1, ignore_stale_grad=True)\n",
    "\n",
    "        if global_counter % decay_step == 0:\n",
    "            trainer.set_learning_rate(trainer.learning_rate * (1.0 - decay))\n",
    "\n",
    "        if global_counter % summary_step == 0:\n",
    "            model.save_parameters(os.path.join(ckpt_dir, 'ckpt.params'))\n",
    "            trainer.save_states(os.path.join(ckpt_dir, 'trainer.status'))\n",
    "            \n",
    "            print('Training loss = ', (sum(loss)).asnumpy().item())\n",
    "            f.write('{}\\t{:.2f}\\t{:.4f}\\t{:.8f}\\n'.format(global_counter, float(time.time() - t0) / 60, (sum(loss)).asnumpy().item(),\n",
    "                                              trainer.learning_rate))\n",
    "            f.flush()\n",
    "\n",
    "            print('{} iterations done!'.format(global_counter))\n",
    "            \n",
    "        if eval_now:\n",
    "            if train_only:\n",
    "                loss = (sum(loss)).asnumpy().item()\n",
    "            else:\n",
    "                del loss, inputs\n",
    "                gc.collect()\n",
    "\n",
    "                loss = []\n",
    "                while True:\n",
    "                    try:\n",
    "                        inputs = next(it_test)\n",
    "                        with autograd.predict_mode():\n",
    "                            # move to gpu\n",
    "                            inputs = CMolRNNLoader.from_numpy_to_tensor(inputs)\n",
    "                            loss.append(sum([(model(*inputs)).as_in_context(mx.gpu())]))\n",
    "                    except StopIteration:\n",
    "                        loss = (sum(loss)/len(loss)).asnumpy().item()\n",
    "                        print('Validation loss = ', loss)\n",
    "                        f.write('\\nValidation loss: {}\\n\\n'.format(loss))\n",
    "                        stop = early_stopping.step(loss)\n",
    "                        if stop:\n",
    "                            best_score = early_stopping.get_best_score()\n",
    "                            print(f'Early stopping! Best validation loss: {best_score}')\n",
    "                            f.write(f'\\n\\nEarly stopping! Best validation loss: {best_score}\\n\\n')\n",
    "                            break_train_loop = True\n",
    "                        if not break_train_loop:\n",
    "                            f.write('\\nstep\\ttime(h)\\tloss\\tlr\\n')\n",
    "                        it_test = iter(loader_test)\n",
    "                        inputs = next(it_test)\n",
    "                        break\n",
    "\n",
    "        if break_train_loop:\n",
    "            break\n",
    "        \n",
    "        if train_only and epoch_no > max_epochs:\n",
    "            break\n",
    "\n",
    "    # save before exit\n",
    "    model.save_parameters(os.path.join(ckpt_dir, 'ckpt.params'))\n",
    "    trainer.save_states(os.path.join(ckpt_dir, 'trainer.status'))\n",
    "\n",
    "    f.write('Training finished\\n')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyN7lEvr9ZvHj02o6K2PAX84",
   "collapsed_sections": [],
   "name": "Multi_Obj_CGGM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
